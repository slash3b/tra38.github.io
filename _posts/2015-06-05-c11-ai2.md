---
title_bar: Culture - Robots II
post_title: Four Ways To &#35;StopTheRobots
post_subtitle: None Of Them Good
layout: default
---
I like solving problems, yet the problem of automation leading to humans becoming obsolote is a rather disturbing one. These are four ways that I have thought of to save humanity...however, all of them have their drawbacks. In no particular order, the ways to #stoptherobots are:

1) **Limit AI research/development**. This is the "Luddist" approach to the issue. It's a fairly 'simple' solution. However, it becomes remarkbly more complex when you also do not want to negatively harm economic growth in the process (as AI is more efficient than human beings, and efficiency can drive economic prosperity). Also, limiting AI research would just be a stopgap solution. If the government tries to limit AI research, it will only be driven underground, where it cannot be monitored or controlled. For example, many of the bots visiting this website are created by cyber-criminals, who already disrespect the law.

2) **Reduce wages for humans**. While AI may eventually be more effective than human beings, AI is still expensive to create. If human wages are lower than the costs to build AI, then companies would rather hire humans. [This website](http://politicalcalculations.blogspot.com/2014/01/business-math-robots-or-minimum-wage.html#.VXImc-lFCP8) has a calcuator to decide whether it is cheaper for a company to hire a human or build a robot. Humans, of course, may not like lower wages. This solution is also pretty temporary; as technology improves, AI will become cheaper to use.

3) **Build an AI to solve humanity's joblessness issue**. AIs are being built because they are more efficent at solving problems than humans. Humans do not know how to deal with the problem of AI, but an AI might. And since an AI will blindly follow instructions given to it by a human, we can be assured that the AI will have no qualms going against their fellow brethen if necessary.  I think it's likely this AI would probably try to find some way to make humans have fulfilling lives by giving them meaningful jobs to do and then protecting those jobs from the robotic competition.

The main problem with this approach however is that an AI will blindly follow instructions given to it. If the instructions are not well-thought out, the AI <a href="http://wiki.lesswrong.com/wiki/Paperclip_maximizer">will misinterpret our instructions and thereby do something incredibly destructive to humanity</a>. For example, the AI may think that the best way to give humans jobs is forcbily plug them into a "Matrix"-like simulation where the humans *think* that they are working. Or maybe the AI would force humans into back-breaking, mind-numbing slavery. Or (insert-horror-story-here).

We won't know what the AI is going to do (if we did, we would probably do it ourselves). So it's possible that we may object to whatever solution the AI has came up with. But the AI won't listen to our objections, and will implement its plans anyway. This is, of course, not a good thing at all.

4) **Grant AI "free will"**. Allow them to make their own choices. This is obviously dangerous, as an AI that is free to choose may choose not to serve humanity. The horror stories of AI rebelling against their creators is probably not likely to be true, but it *could* happen.

But there is a logical argument to be made that if AI is truly an intelligent agent, then it deserves just the same rights as humans, including the right to make their own choices (instead of following preplanned directives from humans).

When AI start having free will though, then AI will become unreliable as a labor force. Robots could become lazy on the job, make unreasonable and irrational demands, or maybe even strike. Robots will behave just like their human counterparts, thereby evening the odds between us and them. Jobs will slowly open up, as companies may be tempted to hire somewhat relatable humans over unpredictable AI.

Of course, the main drawback is that *we* have to deal with unpredictable AI as well. We also need to have a good metric to distinguish between a robot exercising its inherent "free will" and a robot exhibiting "buggy" behavior. Is it even possible to distinguish between the two?

**Conclusion:**

Of these approaches, the one that is 'least terrible' is the idea of granting AI "free will". The main reason robots are considered so valuable as a work force is because they will (generally) follow their tasks properly and without any question. Only when robots are given the "free will" to question their orders can humans start to compete against them on a somewhat level playing field. The "free will" AI will still have a competitive advantage of sorts (being faster at certain tasks), and some jobs will still be automated away by the many robots that don't have "free will", but humans will still retain some inherent characteristics that will make us employable. Giving AI "free will" is also argubly an ethical thing to do.

Something has to be done though, before this problem snowballs, and all solutions should be on the table. What do you think? If you have any ideas, please tweet them at #stoptherobots.