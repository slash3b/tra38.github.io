<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tariq Ali&#39;s Blog</title>
    <description>A blog dedicated to showcasing my talents</description>
    <link>tra38.github.io/blog/</link>
    <atom:link href="tra38.github.io/blog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 18 Jan 2016 07:49:26 -0600</pubDate>
    <lastBuildDate>Mon, 18 Jan 2016 07:49:26 -0600</lastBuildDate>
    <generator>Jekyll v3.0.1</generator>
    
      <item>
        <title>C20 Robojournalism 2</title>
        <description>![CDATA[Though we are able to teach robots how to write as well as a human, we may have difficulty teaching them how to view the world as a human.
]</description>
        <content>&lt;p&gt;Though we are able to teach robots how to write as well as a human, we may have difficulty teaching them how to view the world as a human.&lt;/p&gt;

&lt;p&gt;Computers are learning how to write. It&#39;s not considered weird or bizarre anymore to see companies like &lt;a href=&quot;http://narrativescience.com&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;Narrative Science&lt;/a&gt; and &lt;a href=&quot;https://automatedinsights.com&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;Automated Insights&lt;/a&gt; be able to generate news reports covering topics as broad as sports and finance.  Automated Insights have also made their &quot;Wordsmith&quot; platform publicly available, meaning that anyone have the potential to command their own robotic writers, without any programming knowledge.&lt;/p&gt;

&lt;p&gt;Human journalists have been able to accept their robo-journalistic brethren under the mistaken impression that robo-journalists will be regulated to writing &quot;quantitative&quot; articles while humans will have more time to write &quot;qualitative&quot; analyses. But you can indeed turn human experiences into quantitative data that a robot can then write. For example, &quot;sentiment analysis&quot; algorithms are able to determine whether a certain article is happy or sad, based on analyzing what words were used. The output would be a qualitative judgment, based solely on quantitative data. Narrative Science has already explored the possibility of moving onto &quot;qualitative&quot; analyses by creating &quot;Quill Connect&quot;, a program that is able to write &lt;a href=&quot;https://quillconnect.narrativescience.com&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;qualitative analyses of Twitter profiles&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Algorithms are not limited to writing nonfiction. Every November (starting from 2013), programmers participate in a competition called &lt;a href=&quot;https://github.com/dariusk/NaNoGenMo-2015&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;National Novel Generation Month&lt;/a&gt;; the goal is to write a fictional novel of 50,000 words or more. Some of these generated novels are generally dull, but readable (examples: &quot;&lt;a href=&quot;https://github.com/dariusk/NaNoGenMo-2015/issues/142&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;Around the World in X Wikipedia Articles&lt;/a&gt;&quot;, &quot;&lt;a href=&quot;https://github.com/dariusk/NaNoGenMo-2015/issues/11&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;A Time of Destiny&lt;/a&gt;&quot;, &quot;&lt;a href=&quot;https://github.com/dariusk/NaNoGenMo-2015/issues/40&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;Simulationist Fantasy Novel&lt;/a&gt;&quot;). They have the plot, characterization, and imagery that you would normally associate with a human-written work. Programmers will have to put in more effort for computer-generated novels to be on-par with human-produced literature, but there does not seem to be any inherent limit to algorithmic creativity.&lt;/p&gt;

&lt;p&gt;Of course, one could argue that robots will never be able to replace humans at all. Robots are reliant on &quot;templates&quot; to help them organize their stories properly, and humans are the ones in charge of designing the templates that the robots will end up using to write. In this viewpoint, humans would willingly give up the ability to write since they can find it a much more rewarding task to simply instruct the computer how to write a certain story.&lt;/p&gt;

&lt;p&gt;But I would argue that even if humans would want to outsource all writing to their robotic slaves, humans will still write out some of their ideas out by hand...because of an inherent limitations of bots. Bots lack the &quot;worldview&quot; of humans.&lt;/p&gt;

&lt;p&gt;Humans take for granted their ability to perceive the world. Their five senses gives a continual stream of data that humans are able to quickly process. Bots, on other hand, are only limited to the &quot;raw data&quot; that we give them to process. They will not &quot;see&quot; anything that is not in the dataset. As a result, how the bots understand our world will be very foreign to our own (human) understanding.&lt;/p&gt;

&lt;p&gt;For some people, this is actually a benefit that bots bring to writing. Bots will not have the same biases as human beings. They will therefore discover new insights and meanings that humans may have overlooked. However, bots will instead bring their own unique &#39;biases&#39; and issues into their work, and humans may not tolerate the biases of algorithms as much as they would tolerate the biases of other humans. Humans will, of course, still happily read what the bots have to say. But they also want to read what humans have to say too.&lt;/p&gt;

&lt;p&gt;Humans will likely tolerate the rise of automation in literature, and accept it. Bots may even write the majority of all literature by 2100. But there will still be some marginal demand for human writers, simply because humans can relate more to the &quot;worldview&quot; of other humans. These human writers must learn how to coexist with their robotic brethren though.&lt;/p&gt;

&lt;p&gt;This article was originally published by me on LinkedIn as &lt;a href=&quot;https://www.linkedin.com/pulse/why-robots-fully-replace-human-writers-tariq-ali?trk=pulse_spock-articles&quot; rel=&quot;no-follow&quot;&gt;&quot;Why Robots Will Not (Fully) Replace Human Writers&quot;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Thank you for reading this blog post written by Tariq Ali, an aspiring web developer. If you are interested in reading more blog posts, please go to tra38.github.io/blog today.&lt;/p&gt;</content>
        <pubDate>Thu, 07 Jan 2016 00:00:00 -0600</pubDate>
        <link>tra38.github.io/blog/c20-robojournalism-2.html</link>
        <guid isPermaLink="true">tra38.github.io/blog/c20-robojournalism-2.html</guid>
        
        
      </item>
    
      <item>
        <title>T13 Sql Ordinals And Aliases</title>
        <description>![CDATA[Writing SQL can be a painful experience. But there are two shortcuts that can be used to make your code easier to type. Both shortcuts are cool to understand, but only one is actually advised by the broader "SQL community".
]</description>
        <content>&lt;p&gt;Writing SQL can be a painful experience. But there are two shortcuts that can be used to make your code easier to type. Both shortcuts are cool to understand, but only one is actually advised by the broader &quot;SQL community&quot;.&lt;/p&gt;

&lt;p&gt;I was inspired to write this blog post while completing Codecademy&#39;s tutorial &quot;SQL: Analyzing Business Metrics&quot;. In one exercise in the tutorial, I had to write an SQL query to find out how many people are playing a game per day. This was the query I wrote:&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-psql&quot; data-lang=&quot;psql&quot;&gt;SELECT date(created_at), count(DISTINCT user_id)
from gameplays
GROUP BY date(created_at)
ORDER BY date(created_at);&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;And this is the resulting table...&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;date(created_at)  count(distinct user_id)
2015-08-04          99
2015-08-05          117
2015-08-06          106
...&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;But the SQL query is too verbose and unclear. Could there be a simpler way to refer back to &#39;date(created_at)&#39;? Codecademy suggest using &quot;ordinals&quot;. Ordinals are numbers that are used to refer to the columns you are &#39;selecting&#39; in an SQL query.&lt;/p&gt;

&lt;p&gt;Here&#39;s an easy diagram to determine the ordinal number of a SELECT query...&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-psql&quot; data-lang=&quot;psql&quot;&gt;SELECT date(created_at), count(DISTINCT user_id)
       ^^^^              ^^^
       1                 2&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;By using ordinals, we can simplify our original SQL query as follows:&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-psql&quot; data-lang=&quot;psql&quot;&gt;SELECT date(created_at), count(DISTINCT user_id)
FROM gameplays
GROUP BY 1
ORDER BY 1;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;This approach &lt;em&gt;seems&lt;/em&gt; less verbose, but it is actually much more unclear. At first glance, it is impossible to know what &quot;1&quot; is supposed to mean. And if someone decides to switch the order of the SELECT query (putting &quot;count(DISTINCT user_id)&quot; first), the resulting table will be messed up.&lt;/p&gt;

&lt;p&gt;There has to be a better way.&lt;/p&gt;

&lt;p&gt;And there is. SQL Aliases. Aliases work the same as variables in other programming languages, and to define an alias in SQL, you simply write &quot;AS [alias_name]&quot;. Aliases ensure that your SQL code will be self-documenting, while also ensuring that your code would be less verbose.&lt;/p&gt;

&lt;p&gt;Here&#39;s an example of me defining aliases in an SELECT query, and then using them later on:&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-psql&quot; data-lang=&quot;psql&quot;&gt;SELECT date(created_at) AS time, count(DISTINCT user_id) AS daily_users
FROM gameplays
GROUP BY time
ORDER BY time;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;An interesting side note is that by defining variables in the SELECT query, you also change the name of the columns in the resulting table...&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;time              daily_users
2015-08-04          99
2015-08-05          117
2015-08-06          106
...&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;The main reason Codecademy teaches the use of ordinals is because it was traditionally used during the early days of SQL programming. Thus, knowing ordinals will allow you to understand and debug any legacy SQL queries you encounter.&lt;/p&gt;

&lt;p&gt;However, the broader SQL community strongly discourages the use of ordinals because of the confusion and problems that they may cause. Instead, the SQL community suggest using aliases to make your code clearer and easier to understand. Following these recommendations would make sure that when you do deal with SQL, your pain would be minimal.&lt;/p&gt;
&lt;p&gt;Thank you for reading this blog post written by Tariq Ali, an aspiring web developer. If you are interested in reading more blog posts, please go to tra38.github.io/blog today.&lt;/p&gt;</content>
        <pubDate>Thu, 31 Dec 2015 00:00:00 -0600</pubDate>
        <link>tra38.github.io/blog/t13-sql-ordinals-and-aliases.html</link>
        <guid isPermaLink="true">tra38.github.io/blog/t13-sql-ordinals-and-aliases.html</guid>
        
        
      </item>
    
      <item>
        <title>E6 Fads 2</title>
        <description>![CDATA[In the past, I wrote about Yali Pali's paper about the lifecycle of intellectual fads. I am curious about fads because the programming field seems prone to them, and it's important to avoid fads at all costs. Today, I learned two new ways to explain fads, after reading When a Fad Ends: An Agent-Based Model of Imitative Behavior by Margo Bergman.
]</description>
        <content>&lt;p&gt;In the past, I wrote about &lt;a href=&quot;http://tra38.github.io/blog/e2-fads.html&quot;&gt;Yali Pali&#39;s paper about the lifecycle of intellectual fads&lt;/a&gt;. I am curious about fads because the programming field seems prone to them, and it&#39;s important to avoid fads at all costs. Today, I learned two new ways to explain fads, after reading &lt;a href=&quot;http://uh.edu/margo/paper.pdf&quot;&gt;When a Fad Ends: An Agent-Based Model of Imitative Behavior&lt;/a&gt; by Margo Bergman.&lt;/p&gt;

&lt;p&gt;Pali focused his studies on academica and claimed that &quot;intellectual fads&quot; followed this pattern:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A new field is developed.&lt;/li&gt;
&lt;li&gt;People see that the field has no experts yet (since it is a new field).&lt;/li&gt;
&lt;li&gt;People &lt;strong&gt;rush&lt;/strong&gt; to be a part of the hot new thing, with the goal of becoming prestigious experts.&lt;/li&gt;
&lt;li&gt;A few people ends up becoming experts.&lt;/li&gt;
&lt;li&gt;Everyone else gets bored and leave. The fad ends.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;This approach seems to make sense for explaining fads in programming, since it implies why certain languages, frameworks, and even methodologies may rise and fall in popularity. But what if programming is not structured like academia? What if languages/frameworks/methodologies are not specialized fields of interest but instead tools that can be switched in and out? Then it may be important to view fads in programming as being about &#39;products&#39;, not about &#39;intellectual pursuits&#39;. Bergman&#39;s paper only discusses product fads (and I have taken the liberty of using term &quot;product&quot; with &quot;tool&quot; interchangeably).&lt;/p&gt;

&lt;p&gt;When Bergman wrote her paper, she argued against &quot;information cascades&quot;, a theory developed in economics in the 1990s that also purports to provide a lifecycle for fads. An &quot;information cascade&quot; has these stages:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;People are given a choice to accept or reject a tool. Since they lack complete knowledge about whether that tool is good to use, they rely on other signals from previous &quot;adopters&quot; of the tool.&lt;/li&gt;
&lt;li&gt;The signals from previous &quot;adopters&quot; show that the tool is indeed good. Therefore, people end up adapting the tool.&lt;/li&gt;
&lt;li&gt;The tool actually turns out to be terrible.&lt;/li&gt;
&lt;li&gt;When people learn that the tool is terrible, they stop using the tool. The fad ends.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Bergman does not like this theory though because it implies that fads are always around &#39;bad&#39; tools. For example, Hula Hoops, miniskirts, and Rubik&#39;s Cubes are not &quot;bad&quot; products by any means, yet they were all examples of fads.&lt;/p&gt;

&lt;p&gt;Bergman instead argued that people can be divided into two groups:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Fad Setters&lt;/em&gt; are people who are always looking out for the next best thing. According to Bergman, they &quot;specialize in the discovery of new products&quot; and &quot;are known to be the people who others wish to emulate&quot;. Fad Setters do not care about what other Fad Setters do and are independent.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Fad Followers&lt;/em&gt; are people who want to use the next best thing, but do not know how to find it. So they instead rely on the preferences of Fad Setters. Whatever a Fad Setter wants, the Fad Followers will want too.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Fad Followers do not need to know the exact identity of a Fad Setter to decide whether to buy a product. Instead they look at the behavior of other people, some of whom may potentially have access to the knowledge of a Fad Setter. They can then copy the behavior of those other people, and thereby indirectly emulate the Fad Setter.&lt;/p&gt;

&lt;p&gt;Bergman&#39;s fad theory has these stages:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A Fad Setter finds a product, finding it novel and new. He starts using the product.&lt;/li&gt;
&lt;li&gt;Fad Followers end up emulating the Fad Setter by using that product too.&lt;/li&gt;
&lt;li&gt;When too many Fad Followers use the product, the Fad Setter gets disgusted. The product is no longer novel or new enough for him to use. The Fad Setter leaves the product.&lt;/li&gt;
&lt;li&gt;Fad Followers also leave the product, thereby emulating the Fad Setter.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Bergman also discussed ways to prevent fads from occuring.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;First, there may be multiple Fad Setters using different products and competing with each other. No one product would become popular enough to inspire &quot;disgust&quot; in the Fad Setters (as all the Fad Followers would end up following different Fad Setters). These products thereby continue to stay popular, and avoid having to face the downturn that a fad normally faces.&lt;/li&gt;
&lt;li&gt;Second, a Fad Setter may get disgusted easily and leave a product as soon as Fad Followers starts getting interested. This stops a fad from rising.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I tend to not like Bergman&#39;s theory all that much, because it seems to imply that most people are lemmings, who only follow Fad Setters without any regard to whether the product is actually good. It also implies that Fad Setters are themselves not interested in whether the product is actually good either, and only choose products based on whether other people are &lt;em&gt;not&lt;/em&gt; using them. Yet whether I like it has no bearing on whether it is true or not, and it does seem plausible that programmers are tempted to &quot;follow the crowd&quot; and listen to gurus. I&#39;m not sure whether the gurus themselves are as flaky as the term &quot;Fad Setters&quot; imply.&lt;/p&gt;

&lt;p&gt;Surely, more work has to be done to determine whether Pali&#39;s theory, &quot;Information Cascade&quot; theory, or Bergman&#39;s theory is the correct way to explain the presences of fads in programming. Or maybe all three approaches are correct. I just don&#39;t know yet. I do know that the &#39;life cycle&#39; of a fad is an important topic that I have to keep in mind when evaluating the &quot;next hot thing&quot; in programming. As I wrote in a previous blog post:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;&quot;As beginning programmers, we do not have the time or the money to be chasing fads. We want to specialize in the fields that (a) we like and/or (b) will be popular for a long time. Understanding these fads is important, simply so that we can spot them and avoid them.&quot;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Thank you for reading this blog post written by Tariq Ali, an aspiring web developer. If you are interested in reading more blog posts, please go to tra38.github.io/blog today.&lt;/p&gt;</content>
        <pubDate>Sat, 14 Nov 2015 00:00:00 -0600</pubDate>
        <link>tra38.github.io/blog/e6-fads-2.html</link>
        <guid isPermaLink="true">tra38.github.io/blog/e6-fads-2.html</guid>
        
        
      </item>
    
      <item>
        <title>T12 Dumb Ai</title>
        <description>![CDATA[For a recent coding challenge, I had to implement an AI for a Tic-Tac-Toe game. This AI should not be able to lose a match. A quick Google search reveals that the most efficent way is to use a minimax algorithm to determine what moves are likely to cause the AI to either tie or win against another player. However, nobody I found seemed to analyze what decisions the AI actually makes.
]</description>
        <content>&lt;p&gt;For a recent coding challenge, I had to implement an AI for a Tic-Tac-Toe game. This AI should not be able to lose a match. A quick Google search reveals that the most efficent way is to use &lt;a href=&quot;http://neverstopbuilding.com/minimax&quot;&gt;a minimax algorithm&lt;/a&gt; to determine what moves are likely to cause the AI to either tie or win against another player. However, nobody I found seemed to analyze what decisions the AI actually makes.&lt;/p&gt;

&lt;p&gt;The minimax algorimth works by having the computer play Tic-Tac-Toe against itself, figuring out the best moves to use against itself...and then figuring out the best counters to those moves, and so on and so forth. It will then rank all the possible moves it can take:&lt;/p&gt;

&lt;table&gt;
&lt;tr&gt;&lt;td&gt;-10&lt;/td&gt;&lt;td&gt;- The AI will lose if it chooses this move. Avoid it.&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; 0&lt;/td&gt;&lt;td&gt;- It&#39;s a tie.&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;- The AI will win.&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;


&lt;p&gt;The AI will pick the move that has the highest score.&lt;/p&gt;

&lt;p&gt;Therefore, the computer will play perfectly and never lose. It may never win either though, if the human it is playing against is also playing perfectly.&lt;/p&gt;

&lt;p&gt;But there is an extreme amount of trust in how the Minimax Algorithm works. There is a lot of literature examining how to program the algorithm, and very little literature on what choices the algorithm ends up making. I assume that most people assume that since the algorithm will choose the best possible move for Tic-Tac-Toe, examining what moves it actually picks would be a waste of time.&lt;/p&gt;

&lt;p&gt;But the problem is that the computer has to play through every possible game of Tic-Tac-Toe before it can make its choice. This is not an instantaneous process. For the worst case scenario (where the AI goes first, and thus must play through all Tic-Tac-Toe games), I had to wait 114 seconds for a response from the AI. This is absurd.&lt;/p&gt;

&lt;p&gt;There are ways to reduce the number of games a computer has to play through (since many Tic-Tac-Toe games are really just duplicates of each other, with the boards being rotated)...but there had to be a better way to make the game go faster.&lt;/p&gt;

&lt;p&gt;I decided to look &quot;under the hood&quot; of the AI, because I was curious in seeing how the AI thought for itself. I soon realize that if I know that the AI is thinking, I could &#39;hardcode&#39; in the AI&#39;s choices. The AI will just end up doing its move rather than spending 114 seconds learning to do that same move.&lt;/p&gt;

&lt;p&gt;In the following terminal outputs, O is human and X is the computer. I also output a Hash showing the computer&#39;s thought process (&quot;MOVE&quot;=&gt;SCORE).&lt;/p&gt;

&lt;p&gt;So here&#39;s three starting scenarios:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;O goes first and pick a corner space.&lt;/p&gt;

&lt;p&gt;|&lt;em&gt;0&lt;/em&gt;|&lt;em&gt;1&lt;/em&gt;|&lt;em&gt;2&lt;/em&gt;|&lt;/p&gt;

&lt;p&gt;|&lt;em&gt;3&lt;/em&gt;|&lt;em&gt;4&lt;/em&gt;|&lt;em&gt;5&lt;/em&gt;|&lt;/p&gt;

&lt;p&gt;|&lt;em&gt;6&lt;/em&gt;|&lt;em&gt;7&lt;/em&gt;|&lt;em&gt;O&lt;/em&gt;|&lt;/p&gt;

&lt;p&gt;It is now X&#39;s Turn.&lt;/p&gt;

&lt;p&gt;{&quot;0&quot;=&gt;-10, &quot;1&quot;=&gt;-10, &quot;2&quot;=&gt;-10, &quot;3&quot;=&gt;-10, &quot;4&quot;=&gt;0, &quot;5&quot;=&gt;-10, &quot;6&quot;=&gt;-10, &quot;7&quot;=&gt;-10}&lt;/p&gt;

&lt;p&gt;X has picked Spot 4.
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The computer will choose the space that has the highest score. In this case, &lt;em&gt;every space&lt;/em&gt; other than 4 has a score of -10. The only space that allows the computer to tie the human player is 4, the spot right in the middle. So X picks that.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
O goes first and pick a edge space.&lt;/p&gt;

&lt;p&gt;|&lt;em&gt;0&lt;/em&gt;|&lt;em&gt;1&lt;/em&gt;|&lt;em&gt;2&lt;/em&gt;|&lt;/p&gt;

&lt;p&gt;|&lt;em&gt;3&lt;/em&gt;|&lt;em&gt;4&lt;/em&gt;|&lt;em&gt;5&lt;/em&gt;|&lt;/p&gt;

&lt;p&gt;|&lt;em&gt;6&lt;/em&gt;|&lt;em&gt;O&lt;/em&gt;|&lt;em&gt;8&lt;/em&gt;|&lt;/p&gt;

&lt;p&gt;It is now X&#39;s Turn.&lt;/p&gt;

&lt;p&gt;{&quot;0&quot;=&gt;-10, &quot;1&quot;=&gt;0, &quot;2&quot;=&gt;-10, &quot;3&quot;=&gt;-10, &quot;4&quot;=&gt;0, &quot;5&quot;=&gt;-10, &quot;6&quot;=&gt;0, &quot;8&quot;=&gt;0}&lt;/p&gt;

&lt;p&gt;X has picked Spot 1.
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;In this case, now, there are four moves that may lead to a tie: 1, 4, 6, and 8. Any other move will lead to the computer&#39;s loss.&lt;/p&gt;

&lt;p&gt;But it does not matter how the computer gets to that tie. A tie is a tie. So the computer can easily pick any move that leads to a tie, and the outcome will always be the same. In this case, the computer just choses Spot 1.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
What happens if I let the computer go first?&lt;/p&gt;

&lt;p&gt;|&lt;em&gt;0&lt;/em&gt;|&lt;em&gt;1&lt;/em&gt;|&lt;em&gt;2&lt;/em&gt;|&lt;/p&gt;

&lt;p&gt;|&lt;em&gt;3&lt;/em&gt;|&lt;em&gt;4&lt;/em&gt;|&lt;em&gt;5&lt;/em&gt;|&lt;/p&gt;

&lt;p&gt;|&lt;em&gt;6&lt;/em&gt;|&lt;em&gt;7&lt;/em&gt;|&lt;em&gt;8&lt;/em&gt;|&lt;/p&gt;

&lt;p&gt;{&quot;0&quot;=&gt;0, &quot;1&quot;=&gt;0, &quot;2&quot;=&gt;0, &quot;3&quot;=&gt;0, &quot;4&quot;=&gt;0, &quot;5&quot;=&gt;0, &quot;6&quot;=&gt;0, &quot;7&quot;=&gt;0, &quot;8&quot;=&gt;0}&lt;/p&gt;

&lt;p&gt;X has picked Spot 0.
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;None&lt;/em&gt; of the spaces are any more advantageous than the others. No matter what move the computer picks, a tie will result. So the computer could pick literally any move and be ensured that the other player won&#39;t win. In this case, the computer choses Spot 0.&lt;/p&gt;

&lt;p&gt;I now know that the computer sees the space &quot;4&quot; as an excellent move to make, no matter whether O goes first or second. (Other moves can be just as excellent as &quot;4&quot;, but &quot;4&quot; will always be excellent.)&lt;/p&gt;

&lt;p&gt;I then programmed the computer to always pick &quot;4&quot; as its first move. The minimax algorithm would still be used after the first move though. After the first few moves, the number of possible Tic-Tac-Toe games decreases, and so the minimax algorithm is speedy enough for the user to not be annoyed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Result&lt;/strong&gt;: The AI is still unbeatable, but now the game is faster to play! It takes less than a second for the computer to make its first move, and its second move against the player is still just as fast.&lt;/p&gt;

&lt;p&gt;It is cool to have the AI think how to beat its opponent. But just because something is cool does not mean that it is necessary or worthwhile in the real world. Thinking takes time, and a user is not going to be happy waiting for 114 seconds while the computer figure out what it should do next. Hardcoding in the AI choice ensure that the game runs faster. And since the hardcoded choice is actually based on the results of the minimax algorithm, we are not risking the chance of the AI losing.&lt;/p&gt;
&lt;p&gt;Thank you for reading this blog post written by Tariq Ali, an aspiring web developer. If you are interested in reading more blog posts, please go to tra38.github.io/blog today.&lt;/p&gt;</content>
        <pubDate>Sun, 27 Sep 2015 00:00:00 -0500</pubDate>
        <link>tra38.github.io/blog/t12-dumb-ai.html</link>
        <guid isPermaLink="true">tra38.github.io/blog/t12-dumb-ai.html</guid>
        
        
      </item>
    
      <item>
        <title>C19 Robojournalism</title>
        <description>![CDATA[Automated Insights and Narrative Science are two companies that specialize in producing software that can write news articles. But how do these news articles stack up to the human-written competition? A German researcher attempts to find out.
]</description>
        <content>&lt;p&gt;Automated Insights and Narrative Science are two companies that specialize in producing software that can write news articles. But how do these news articles stack up to the human-written competition? A German researcher attempts to find out.&lt;/p&gt;

&lt;p&gt;In the peer-reviewed article &lt;a href=&quot;http://kau.diva-portal.org/smash/get/diva2:699641/FULLTEXT01.pdf&quot;&gt;&quot;Enter the robot journalist: users’ perceptions of automated content&quot;&lt;/a&gt;, researchers randomly gave German undergraduate students one of two articles. One article was written by a human, and another was written by a program called &quot;Statsheet&quot; (which was created by Automated Insights). The German students were to read the article, rate it, and then say whether the article was written by a human being or a bot.&lt;/p&gt;

&lt;p&gt;The researchers asked two questions:&lt;/p&gt;

&lt;p&gt;1) Can the research students consistently identify whether an article is written by a human or a bot? - The answer is a resounding &quot;NO&quot;. While a majority of students correctly identified the bot-written article as being written by a bot, a majority of students also identified the &lt;strong&gt;human&lt;/strong&gt;-written article as being written by a bot. There is no statistically significant difference between how the students &#39;evaluated&#39; whether the article was written by a human or a bot.&lt;/p&gt;

&lt;p&gt;2) Are bots able to write prose of equal quality and crediblity as that of humans? - The answer is &quot;Yes, with one exception&quot;. There was no statistically significant difference between the quality of the journalism of the human and the bot...&lt;em&gt;except&lt;/em&gt; for the fact that the human&#39;s article was seen as more pleasant to read.&lt;/p&gt;

&lt;p&gt;(As a tangentical side note, the students rated the bot&#39;s article was being more informative, accurate, trustworthy, and objective...though the bot&#39;s article was also rated as more boring to read too. The students also rated the human&#39;s article as being well-written and coherent.&lt;/p&gt;

&lt;p&gt;The reason this note is tangentical is that these results are not &quot;statistically significant&quot;; if you were to run this same experiment with a different population, you would likely get different results.)&lt;/p&gt;

&lt;p&gt;Conclusion: The &quot;robojournalists&quot; are able to produce journalism that is able to equal that of the human competition. There are limits to the robojournalists though. They are unable to write prose that is &#39;pleasent to read&#39;. This may be a sign of limits to robojournalists&#39; creativity, which would give humans the edge. For now.&lt;/p&gt;
&lt;p&gt;Thank you for reading this blog post written by Tariq Ali, an aspiring web developer. If you are interested in reading more blog posts, please go to tra38.github.io/blog today.&lt;/p&gt;</content>
        <pubDate>Mon, 07 Sep 2015 00:00:00 -0500</pubDate>
        <link>tra38.github.io/blog/c19-robojournalism.html</link>
        <guid isPermaLink="true">tra38.github.io/blog/c19-robojournalism.html</guid>
        
        
      </item>
    
      <item>
        <title>C18 Narrative Science</title>
        <description>![CDATA["Practical Artifical Intelligence for Dummies: Narrative Science Edition" is a free e-book that provides a quick introduction into the AI field. It is also a treatise that outlined Narrative Science's approach towards dealing with AI.
]</description>
        <content>&lt;p&gt;&lt;a href=&quot;http://www.narrativescience.com/practical-ai&quot;&gt;&quot;Practical Artifical Intelligence for Dummies: Narrative Science Edition&quot;&lt;/a&gt; is a free e-book that provides a quick introduction into the AI field. It is also a treatise that outlined Narrative Science&#39;s approach towards dealing with AI.&lt;/p&gt;

&lt;p&gt;One of the taglines of this e-book is that it will &quot;demystify the hype, fear, and confusion about AI&quot;. Though the book primarly focused on explaining AI, it did pay some attention to the &#39;fears&#39; within popular culture, and attempted to address them in a &quot;roundabout&quot; way, without necessarily rebutting any specific fear.&lt;/p&gt;

&lt;p&gt;Narrative Science believed that AI only need to display intelligent behavior, and not necessarily need to think like human beings. As a result, Narrative Science concluded that technologies that humans take for granted are already AI. Voice recognition, recommendation engines, autocompletes, etc. are all commonly accepted within society, and yet all of these algorithms display intelligent behaviors.&lt;/p&gt;

&lt;p&gt;But these AI programs do not seek to take over the world or render people unemployed. Instead, these programs are just tools, there to help humans accomplish their day-to-day tasks. People are still in control and still receiving regular paychecks; all the AI did just made their lives easier. Narrative Science concluded that this trend would continue, and future AI programs will simply help humans instead of displacing them.&lt;/p&gt;

&lt;p&gt;If that was the crux of Narrative Science&#39;s argument, then I don&#39;t think this book&#39;s philosophy would have been interesting enough to blog about. But Narrative Science also surprisingly expressed some concern about AI proliferation. Narrative Science fears &lt;strong&gt;black boxes&lt;/strong&gt;: AI programs that are able to give answers but &lt;em&gt;fail&lt;/em&gt; to provide explainations for why it came up with those answers.&lt;/p&gt;

&lt;p&gt;Black boxes are not hypothetical concerns. Self-learning entities are being built and used already, such as &quot;evidence engines&quot; (IBM&#39;s Watson) and deep-learning networks (&#39;neural networks&#39;). These entities are able to learn about the world, but they cannot tell other people how they learn the world. You ask them a question, and all they give you an answer...with no context or reasoning behind the answer.&lt;/p&gt;

&lt;p&gt;Black boxes ruin trust. If you do not know how the AI came up with the answer, you cannot trust  whether the answer is actually correct[1]. Without trust, AI loses their potential as useful tools for humanity. A calcuator that gives the wrong answer every 30 minutes is not a very useful calcuator.&lt;/p&gt;

&lt;p&gt;Narrative Science claims that the best way to preserve trust in AI (and to keep their status as useful tools) is to enable the AI to communicate its internal thought process to human beings. That way, we can evaluate the AI&#39;s thought process and decide whether it is correct or incorrect. Narrative Science has already implemented a &quot;logic trace&quot; within its own AI program, &quot;Quill&quot;, allowing people to understand why the AI program had made the choices that it did. Quill is currently being used to write newspaper articles and financial reports.&lt;/p&gt;

&lt;p&gt;As black boxes are used in more and more critical industries, Narrative Science&#39;s apprehension about them will only grow. Narrative Science recommends that other companies also implement &quot;logic traces&quot; in their own AI programs as a way to counteract the possibility of &#39;black boxes&#39;. Already, Google has tried visualizing how its own black boxes works through its &lt;a href=&quot;googleresearch.blogspot.com/2015/06/inceptionism-going-deeper-into-neural.html&quot;&gt;&quot;Google Dreams&quot;&lt;/a&gt;. More work will have to be done to deal with the dangers of of black boxes..&lt;/p&gt;

&lt;p&gt;[1]The alternative is to implicilty trust the AI&#39;s thought process...but when the AI inevitably make mistakes, you will not know how to prevent it from making future mistakes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Correction&lt;/strong&gt; - On Janurary 17, 2016, this article was updated to correct mistaken impressions about Narrative Science.&lt;/p&gt;
&lt;p&gt;Thank you for reading this blog post written by Tariq Ali, an aspiring web developer. If you are interested in reading more blog posts, please go to tra38.github.io/blog today.&lt;/p&gt;</content>
        <pubDate>Sat, 22 Aug 2015 00:00:00 -0500</pubDate>
        <link>tra38.github.io/blog/c18-narrative-science.html</link>
        <guid isPermaLink="true">tra38.github.io/blog/c18-narrative-science.html</guid>
        
        
      </item>
    
      <item>
        <title>C17 Postmorterm</title>
        <description>![CDATA[Devbootcamp does not have a conventional 'final exam' like most schools. Instead, to graduate from the program, you must work with a team of like-minded individuals to build a full-blown website within 7 days. This was my experience in trying to build Hashography, a website that would display tweets about a certain word onto a Google Map.
]</description>
        <content>&lt;p&gt;Devbootcamp does not have a conventional &#39;final exam&#39; like most schools. Instead, to graduate from the program, you must work with a team of like-minded individuals to build a full-blown website within 7 days. This was my experience in trying to build &lt;a href=&quot;http://hashography.herokuapp.com/&quot;&gt;Hashography&lt;/a&gt;, a website that would display tweets about a certain word onto a Google Map.&lt;/p&gt;

&lt;p&gt;Our project lead, Evangeline Garreau, was inspired by a news article about a professor who was able to trace the usage of certain words on Twitter and develop a heat map displaying his results. Evangeline found the system cool and wanted to replicate that with a website that would display the history of word usage throughout the entire world.&lt;/p&gt;

&lt;p&gt;At first, the whole team was excited about the project. We wanted to determine the history of a word and be able to trace it from the very beginning. We also wanted to know where the words were being used, not just in the US, but globally. We had big dreams and aspirations for how to interpret the data and display it on a &quot;heat map&quot;. All we had to do is just to grab those Tweets from Twitter.&lt;/p&gt;

&lt;p&gt;But then those big ideas faced &quot;technical limitations&quot;, which would be a euphemism for &quot;cold hard reality&quot;. The best way to grab data from Twitter is to use an API. An API is a tool that enables websites (like Twitter) to communicate with other programs and websites (like &quot;Hashography&quot;). Twitter has two such APIs: a &quot;REST API&quot; that allows us to access past tweets, and a &quot;Streaming API&quot; that allows us to view present &#39;real-time&#39; tweets.&lt;/p&gt;

&lt;p&gt;It turns out that they were inherent limits to the Twitter REST API...such as the fact that Twitter would only allow us access to the last 7-10 days of Tweets. It also turns out that the professor that inspired Evangeline likely used the &quot;Streaming API&quot; for an entire year in order to grab the tweets necessary for him to discover the history. This option was incredibly impractical for our week-long project.&lt;/p&gt;

&lt;p&gt;Our passion for the project turned into panic. We tried searching for alternative ways of meeting our project. We looked at Google Trends, which gave us all the data we need to trace the history of the word usage...except that if we end up using it, we would be pretty much copy Google Trends. We tried switching to a multimiedia approach that would use Instagram&#39;s photographs to present a history of the word, except Instagram&#39;s own API gave us trouble too. At some point, some members of our team even half-jokingly supported violating Twitter&#39;s Terms of Service agreement by building robots that would &quot;scrape&quot; tweets.&lt;/p&gt;

&lt;p&gt;We floundered horribly.&lt;/p&gt;

&lt;p&gt;By Friday afternoon, it was agreed that we had to pivot away from the original idea behind the project. The group overall came to an agreement that it was more important to see where tweets are located rather than see the &quot;history&quot; of the word (after all, Google Trends already gave us a good history of the word). We decided to use the Twitter Streaming API to grab current tweets of a word and then display them onto a map. We also decided that we never really liked seeing &quot;heat maps&quot;, and switched to displaying data as points on a Google Map.&lt;/p&gt;

&lt;p&gt;At the time, it seemed that we had &quot;lost time&quot;, as we spent two days without any &quot;progress&quot;. But we have learned a lot more about our tools and our limitations, enabling us to catch up quickly. We also were more willing to try everything possible to resolve any disaster that would confront us, including preparing backup plans in case our current &quot;course-of-action&quot; was not working. We ended up producing a workable website by Sunday, and started polishing and improving on it.&lt;/p&gt;

&lt;p&gt;True, we still had problems after Sunday. We did not have enough automated tests and had to waste valuable time manually testing changes. We needed to learn the &quot;best practices&quot; for the new tools we were using. We needed to refactor our code constantly. We had trouble with designing our website to be user-friendly. But those were problems that we could &lt;strong&gt;fix&lt;/strong&gt; as part of standard maintenance, and they were much easier problems than trying to build the website in the first place. The very act of having a physical web presence gave the team a big morale boost.&lt;/p&gt;

&lt;p&gt;User feedback was excellent, and people saw our program as being &#39;beautiful and meaningful&#39;. Our &quot;pivot&quot; was a success.&lt;/p&gt;

&lt;p&gt;Why was it a success? For me, I think it was just due to the fact that we &lt;s&gt;panicked&lt;/s&gt; realized the extent of our situation early, and thus was more willing to do &quot;whatever it takes&quot; to accomplish our goals, including changing our goals to something more feasible. Our project lead, Evangeline Garreau, would instead likely credit the fact that we had good group dynamics and that we respected each other&#39;s ideas. (Of course, both thoughts may be true.) And, of course, we may have just been lucky.&lt;/p&gt;

&lt;p&gt;Overall, we did a good job with this website, and it showed us ultimately that we can still produce beautiful and meaningful work without necessarily staying true to the original idea. We were humbled by the experience, and only by being humble can we begin to learn from our surroundings. We had fun, and we learned a lot.&lt;/p&gt;
&lt;p&gt;Thank you for reading this blog post written by Tariq Ali, an aspiring web developer. If you are interested in reading more blog posts, please go to tra38.github.io/blog today.&lt;/p&gt;</content>
        <pubDate>Thu, 13 Aug 2015 00:00:00 -0500</pubDate>
        <link>tra38.github.io/blog/c17-postmorterm.html</link>
        <guid isPermaLink="true">tra38.github.io/blog/c17-postmorterm.html</guid>
        
        
      </item>
    
      <item>
        <title>C16 Lovelace Test 2</title>
        <description>![CDATA[In my previous blog post, I wrote about two tests used to determine whether AI is intelligent. However, it turns out that I did not fully understand one of those tests: the Lovelace Test.
]</description>
        <content>&lt;p&gt;In my previous blog post, I wrote about &lt;a href=&quot;http://tra38.github.io/blog/c15-lovelace-test.html&quot;&gt;two tests used to determine whether AI is intelligent&lt;/a&gt;. However, it turns out that I did not fully understand one of those tests: the Lovelace Test.&lt;/p&gt;

&lt;p&gt;The Lovelace Test was named after Ada Lovelace (the first computer programmer) who argued famously that computers will only follow instructions that programmers give it. According to my summary of this test, a program must match the following criteria:&lt;/p&gt;

&lt;p&gt;&quot;1. The program must be able to design something &#39;original&#39; and &#39;creative&#39; (such as a story, music, idea, or even another computer program).
2. The program itself is a result of processes that can be &#39;reproduced&#39;. (In other words, it does not rely on some bug in the &#39;hardware&#39; that the program is running on.)
3. The programmer must not know how the program actually works.&quot;&quot;&lt;/p&gt;

&lt;p&gt;I thought that this test is flawed because it excludes the possibility of bugs or programs being so overly complex that a programmer would not be able to understand them.&lt;/p&gt;

&lt;p&gt;But it turns out that my summary of this test was based on &lt;a href=&quot;http://motherboard.vice.com/read/forget-turing-the-lovelace-test-has-a-better-shot-at-spotting-ai&quot;&gt;a Vice article&lt;/a&gt;, which neglected one additional criteria: the program must not be a result of bugs. In the original paper &lt;a href=&quot;http://kryten.mm.rpi.edu/lovelace.pdf&quot;&gt;&quot;Creativity, the Turing Test, and the (Better) Lovelace Test&quot;&lt;/a&gt;, the authors specifically addressed the idea of bugs, and why their presence does not mean intelligence.&lt;/p&gt;

&lt;p&gt;“Sure, we all know that computers do things we don’t intend for them to do. But that’s because we’re not smart and careful enough, or — if we’re talking about rare hardware errors — because sometimes microscopic events unfold in unforeseen ways. The unpredictability in question does not result from the fact that the computer system has taken it upon itself to originate something. To see the point, consider the assembling of your Toyota Camry. Suppose that while assembling a bumper, a robot accidentally attaches a spare tire to the bumper instead of leaving it to be placed in its designated spot in the trunk. The cause of the error, assume, is either a ﬂuke low—level hardware error or a bug inadvertently introduced by some programmers. And suppose for the sake of argument that as serendipity would have it, the new position for the tire strikes some designers as the ﬁrst glorious step toward an automobile that is half conventional sedan and half sport utility vehicle. Would we want to credit the malfunctioning robot with having originated a new auto? Of course not.”&lt;/p&gt;

&lt;p&gt;Under this idea, the Lovelace Test does indeed have meaning. It may be impossible to actually pass (as originally designed), as I actually do support Ada Lovelace&#39;s contentions that computer programs only do stuff we tell it to do. But I do not think that the ability to tell programs what to do automatically renders programs non-intelligent. Even intelligent species like humans have to receive instructions and learn from other intelligent beings.&lt;/p&gt;

&lt;p&gt;But at least the test has rational and logical meaning, and places a higher barrier than the Turing Test. So this blog post is an apology of sorts for misrepresenting the Lovelace Test in my previous post. If you do agree with the premises of the test, then it would serve as a valid way of determining intelligence. That being said, this passage on bugs raises three questions about the Lovelace Test:&lt;/p&gt;

&lt;p&gt;1) Who do we credit then for making the new car, if not the robot? We can’t credit the designers...they did not come up with the idea or build the prototype. We can’t credit the programmers or the low-level hardware error: they made mistakes and were not doing their job. The only entity that actually built the new auto was the malfunctioning robot, and is not creation a type of origination?&lt;/p&gt;

&lt;p&gt;2) If machines do something new and unexpected, it is not a sign of machine intelligence, but human stupidity. This seems somewhat more disturbing to me, especially as we may soon easily build machines so complex that we cannot even begin to understand and comprehend how they work. How would the &quot;stupid&quot; humans be able to handle dealing with these mechanical brutes (especially in terms of debugging)?&lt;/p&gt;

&lt;p&gt;3) These &quot;complex&quot; machines may, of course, accomplish their assigned tasks efficiently than a human can. Does that make the machines&#39; non-intelligence &quot;better&quot; than human intelligence? Is &quot;intelligence&quot;, then, an overrated concept?&lt;/p&gt;
&lt;p&gt;Thank you for reading this blog post written by Tariq Ali, an aspiring web developer. If you are interested in reading more blog posts, please go to tra38.github.io/blog today.&lt;/p&gt;</content>
        <pubDate>Wed, 12 Aug 2015 00:00:00 -0500</pubDate>
        <link>tra38.github.io/blog/c16-lovelace-test-2.html</link>
        <guid isPermaLink="true">tra38.github.io/blog/c16-lovelace-test-2.html</guid>
        
        
      </item>
    
      <item>
        <title>C15 Lovelace Test</title>
        <description>![CDATA[Today, we are making great strides in producing bots that can automate tasks efficently. This can be a major problem for us, as AI automation may endanger jobs. But are these bots that are replacing humans actually 'intelligent'? Two tests exist to determine whether an AI is intelligent. In my opinion, both tests are flawed.
]</description>
        <content>&lt;p&gt;Today, we are making great strides in producing bots that can automate tasks efficently. This can be a major problem for us, as &lt;a href=&quot;http://tra38.github.io/blog/c10-ai.html&quot;&gt;AI automation may endanger jobs&lt;/a&gt;. But are these bots that are replacing humans actually &#39;intelligent&#39;? Two tests exist to determine whether an AI is intelligent. In my opinion, both tests are flawed.&lt;/p&gt;

&lt;p&gt;The Turing Test has traditionally been used as a way to determine whether an AI is intelligent. If a machine is able to convince a human through conversation that the machine is human, then the machine can be said to have intelligence. The test sounded like a good idea at the time...but it turns out that it&#39;s too easy to pass. You just have to figure out how to string sentences together well-enough to fool humans. In 2011, an AI named &lt;a href=&quot;https://en.wikipedia.org/wiki/Cleverbot&quot;&gt;Cleverbot&lt;/a&gt; was able to convince 59.3% of humans at a festival that it was a human, while in 2014, &lt;a href=&quot;motherboard.vice.com/read/how-a-computer-beat-the-turing-test-by-pretending-to-be-a-13-year-old-boy&quot;&gt;&quot;Eugene Goodman&quot;&lt;/a&gt; pretended to be a 13-year-old Ukrainian and convinced over 30% of judges. In 2015, &lt;a href=&quot;motherboard.vice.com/read/the-poem-that-passed-the-turing-test&quot;&gt;a PhD student&lt;/a&gt; submitted several computer-generated poems to literary maganizes and succesfully got one of them published.&lt;/p&gt;

&lt;p&gt;Deceit is a poor subsitute for intelligence.&lt;/p&gt;

&lt;p&gt;Some computer scientists have designed an alternative to the Turing Test, called the &lt;a href=&quot;http://motherboard.vice.com/read/forget-turing-the-lovelace-test-has-a-better-shot-at-spotting-ai&quot;&gt;&quot;Lovelace Test&quot;&lt;/a&gt;, named after Ada Lovelace, the first computer programmer. Ada Lovelace argued that computers could never be intelligent, simply because computers will always do what programmers tell it to do.&lt;/p&gt;

&lt;p&gt;To pass the Lovelace Test:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The program must be able to design something &#39;original&#39; and &#39;creative&#39; (such as a story, music, idea, or even another computer program).&lt;/li&gt;
&lt;li&gt;The program itself is a result of processes that can be &#39;reproduced&#39;. (In other words, it does not rely on some bug in the &#39;hardware&#39; that the program is running on.)&lt;/li&gt;
&lt;li&gt;The programmer must not know how the program actually works.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;This test also seemed like a good idea, except when it isn&#39;t. This test is very easy to pass, even easier than the Turing Test. If a programmer writes up a program that is overly complex, then the programmer would not know how the program works. Therefore, the program would pass the Lovelace Test easily. (Any output by this complex program would could then be defined as being &#39;original&#39; and &#39;creative&#39; by at least one other person.)&lt;/p&gt;

&lt;p&gt;What would be more interesting is thinking about would this hapless programmer do next. If the programmer then studied the code carefully, made educated guesses, and slowly refactored the code, then did the program loses the intelligence it previously gained from its complexity? If that is true, then we should look at scientists who are trying to understand the human mind. If we start getting a good sense about how our brain works, do we lose our own &#39;intelligence&#39; in the process?&lt;/p&gt;

&lt;p&gt;The main problem I have with both the Lovelace Test and the Turing Test is that it assumes that intelligence is a binary trait. Either you have it or you do not. But it seems intelligence would be better modeled as a continuum. AI, of course, are not as intelligent as human beings (at least, not yet). But AI still have some intelligence. We can even bring in the idea of &quot;multiple intelligences&quot;: humans are good at creativity while AI are good at solving math. Just because humans are &#39;top dogs&#39; does not mean that we can disrespect &#39;lesser dogs&#39;.&lt;/p&gt;

&lt;p&gt;Of course, I do not think that Duke Greene, my instructor at DevBootCamp, would even like the idea of measuring intelligence in the first place. He quoted a philosopher who (and I am paraphrasing here) that if bunnies thought like humans, then bunnies would consider themselves to be the most intelligent beings on Earth and the second-intelligent beings would be those beings who take orders from the bunnies. Prehaps intelligence itself merely a codeword for &quot;thinking like us&quot;, and we should instead respect AI as what it is, rather than hope for it to turn into something it is not.&lt;/p&gt;

&lt;h6&gt;#&lt;/h6&gt;

&lt;p&gt;As a side-note: I do like the idea of having code so complex humans would not understand how it actually works. Such a thing would make the code unpredictable, and unpredictable code is code that cannot be trusted to do work as reliably as a human can. (In fact, in a blog post about how to stop AI automation from destorying jobs, I mused about giving AI &lt;a href=&quot;http://tra38.github.io/blog/c11-ai2.html&quot;&gt;&#39;free will&#39;&lt;/a&gt; to make them less appealing as job candidates. Unpredictablity can also work to discourage people from hiring bots.)&lt;/p&gt;

&lt;p&gt;The problem is that unpredictable code is &lt;strong&gt;bad&lt;/strong&gt;. Such code will be dismissed as being buggy and unmaintainable, especially by the programmer who wrote the code in the first place. Programmers will invest time in trying to simplify the program&#39;s complexity and refactor it so that the code can end up being predictable and simple to understand. If a program passes the Lovelace Test, it is not a sign of an AI renaissance. It is a sign that something has gone horribly wrong.&lt;/p&gt;
&lt;p&gt;Thank you for reading this blog post written by Tariq Ali, an aspiring web developer. If you are interested in reading more blog posts, please go to tra38.github.io/blog today.&lt;/p&gt;</content>
        <pubDate>Sun, 02 Aug 2015 00:00:00 -0500</pubDate>
        <link>tra38.github.io/blog/c15-lovelace-test.html</link>
        <guid isPermaLink="true">tra38.github.io/blog/c15-lovelace-test.html</guid>
        
        
      </item>
    
      <item>
        <title>C14 Surviving Dbc</title>
        <description>![CDATA[DevBootCamp has been a rough journey, but I know that it will give me the skill and confidence necessary for me to survive in the tech world.
]</description>
        <content>&lt;p&gt;DevBootCamp has been a rough journey, but I know that it will give me the skill and confidence necessary for me to survive in the tech world.&lt;/p&gt;

&lt;p&gt;Today, I passed a major assessment in DevBootCamp, thereby ensuring that I will move forward in the program. The feedback I got from my assessor was excellent, telling me my weaknesses (failing to use prototypes in Javascript, accidentally destroying the DOM element instead of simply hiding it, having complicated code when simple code would do). But now that I know these weaknesses, I can now move to correct them. Progress is being made.&lt;/p&gt;

&lt;p&gt;I would not be where I was today without asking for help. Indeed, I asked for help a lot during the assessment, perhaps more so than necessary. There has been instances where I asked for help only to be able to solve the problem by myself, with the guide just sitting there watching me explain the problem and figure out what&#39;s wrong. The lesson I learnt from today is that I need to be more self-confident of my own abilities. At the same time, I still need to know when to ask for help...but only when I know when I need that help.&lt;/p&gt;

&lt;p&gt;After the assessment, DevBootCamp gives us 1.5 days to work on any project we want, so long as we use an API (Application Programming Interface). I decided to work on Friend-Computer, a program that will automatically generate blog posts. I had thought that the program would take me 2 hours to complete, but instead it took me all day. However, I did manage to &lt;a href=&quot;http://friendcomputer.herokuapp.com/&quot;&gt;successfully create it&lt;/a&gt;, and most people are sastified with the results, though there is some critique about the &quot;grammar&quot; of the posts. The lesson I learnt here is that I need to prepare for the fact that I &lt;strong&gt;don&#39;t&lt;/strong&gt; know how long a project would take, and prepare myself accordingly. I must avoid overconfidence, lest I get burned.&lt;/p&gt;

&lt;p&gt;Indeed, when I entered into DevBootCamp, I had the intention of updating my blog every week. This, of course, did not happen, as I was more focused on preparing for the assessment than producing content. But if DevBootCamp helps me be a better programmer, nay, a better person, then the experience will all be worth it.&lt;/p&gt;
&lt;p&gt;Thank you for reading this blog post written by Tariq Ali, an aspiring web developer. If you are interested in reading more blog posts, please go to tra38.github.io/blog today.&lt;/p&gt;</content>
        <pubDate>Thu, 23 Jul 2015 00:00:00 -0500</pubDate>
        <link>tra38.github.io/blog/c14-surviving-dbc.html</link>
        <guid isPermaLink="true">tra38.github.io/blog/c14-surviving-dbc.html</guid>
        
        
      </item>
    
  </channel>
</rss>