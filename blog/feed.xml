<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tariq Ali&#39;s Blog</title>
    <description>A blog dedicated to showcasing my talents</description>
    <link>tra38.github.io/blog/</link>
    <atom:link href="tra38.github.io/blog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sat, 23 Jul 2016 19:56:53 -0500</pubDate>
    <lastBuildDate>Sat, 23 Jul 2016 19:56:53 -0500</lastBuildDate>
    <generator>Jekyll v3.0.1</generator>
    
      <item>
        <title>C23 One Year Later</title>
        <description>![CDATA[Most people think spies are afraid of guns, or KGB guards, or barbed wire, but in point of fact the most dangerous thing they face is paper. Papers carry secrets. Papers can carry death warrants. Papers like this one, this folio with its blurry eighteen year old faked missile photographs and estimates of time/survivor curves and pervasive psychosis ratios, can give you nightmares, dragging you awake screaming in the middle of the night.---A Colder War
]</description>
        <content>&lt;blockquote&gt;&lt;p&gt;Most people think spies are afraid of guns, or KGB guards, or barbed wire, but in point of fact the most dangerous thing they face is paper. Papers carry secrets. Papers can carry death warrants. Papers like this one, this folio with its blurry eighteen year old faked missile photographs and estimates of time/survivor curves and pervasive psychosis ratios, can give you nightmares, dragging you awake screaming in the middle of the night.---&lt;a href=&quot;http://www.infinityplus.co.uk/stories/colderwar.htm&quot;&gt;A Colder War&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Humans have written papers all their lives. Writing was once considered a praiseworthy activity that required intelligence.  But technology has now advanced to the point that computers are able to imitate the very process of writing successfully, and we are forced to decide between two unappeasing statements:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Computers must be intelligent.&lt;/li&gt;
&lt;li&gt;Writing doesn&#39;t require intelligence.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This is, to me, pretty scary for a variety of reasons: loss of purpose and ego issues, possible rise in unemployment, human dependency on technology to do all the work for them, etc. And I wanted to do something to counter the rising power of AI.&lt;/p&gt;

&lt;p&gt;It has been 15 months since I first entered DBC, almost one year since I graduated from Dev Bootcamp. During that time, I attempted to research the emerging field of computer-generated literature...in the hopes of using this technology against them, as a form of advocacy against uncontrolled research into AI.&lt;/p&gt;

&lt;p&gt;During the on-site portion of the DBC curriculum in Chicago, many people learned about this plan, especially after I produced &lt;a href=&quot;https://github.com/tra38/FriendComputer&quot;&gt;Friend Computer&lt;/a&gt;, a blog written by a computer, as a standalone DBC project. However, since then, I have not been in much contact with the people on DBC ever since my graduation...and so I decide to write a blog post to explain what happened.&lt;/p&gt;

&lt;p&gt;I&#39;ll start by quoting an email that I sent to a random stranger:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Let me introduce myself fairly carefully. I like to write. When I was a teenager, I served as a volunteer journalist. In college, I published a research paper. Even today, I write nonfictional blog posts and some (unpublished) fictional stories. So I still write, although I do not identify myself as a &quot;writer&quot;. My day-job is a computer programmer...where I write computer code (although it may not be beautiful or elegant to read).&lt;/p&gt;

&lt;p&gt;I had very some uncomfortable feelings about artificial intelligence and technology...especially our overt dependence on it and its potential impact on humanity. After reading an article about Narrative Science and its robojournalist articles, I wanted to take a stand against technological progress causing us to &quot;shoot ourselves in the foot&quot; [by writing a dystopian novel about an AI supercomputer that ruled over human society with an iron fist].&lt;/p&gt;

&lt;p&gt;And I immediately realized that this novel had to be computer-generated. It had to be that way. If I &quot;handwrote&quot; a sci-fi novel about the dangers of technology, then it would just be my opinions, my thoughts. Not reality. Nobody would listen to the thoughts of a random person, and would simply dismiss my fear as &quot;Luddism&quot;. If I am able to build proof of the dangers of technology (computer-generated literature), then that would make my arguments against technology more persuasive. &quot;If technology can do X now, what about the future?&quot; Etc., etc.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The &quot;Ozymandias Gambit&quot; is the term that Duke Greene (a DBC teacher) named my scheme. It is named after the character from the &quot;Watchmen&quot; comic book series. Ozymandias, the main villian of the comic book series, wanted to end the Cold War by creating a Lovecraftian monster to attack both the United States and the Soviet Union. His hope is that this monster would scare the superpowers long enough to unify together, thereby forestalling the possibility of nuclear armageddon.&lt;/p&gt;

&lt;p&gt;I built &lt;a href=&quot;https://github.com/tra38/Skynet&quot;&gt;&quot;Skynet&quot;&lt;/a&gt;, a command-line program that can be used to generate stories based on a &quot;Mad-Lib&quot;-style template. Someone even built &lt;a href=&quot;https://github.com/enkiv2/Skynet/blob/master/txt2sky.py&quot;&gt;a Python script&lt;/a&gt; that can turn existing novels &lt;em&gt;into&lt;/em&gt; templates that can then be used to generate stories...and used it and &lt;a href=&quot;https://github.com/enkiv2/Skynet/blob/master/odyssey.dict&quot;&gt;a configuration file&lt;/a&gt; to generate a Sky template of &lt;a href=&quot;https://github.com/enkiv2/Skynet/blob/master/odyssey.sky&quot;&gt;The Odyssey&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now, certainly, I could have actually produced the anti-technological novel with this tool I had. But I would have to &lt;em&gt;write&lt;/em&gt; out all the templates in question, which almost defeats the whole purpose of building the program. The templates would have done most of the work, while the computer in question really only randomly select words to plug into the templates. So I gave up on that line...and tried other approaches to procedural generation.&lt;/p&gt;

&lt;p&gt;As for expressing my ire about technology, I did find other outlets. I wrote algorithms to generate &lt;a href=&quot;https://medium.com/laughter-in-the-singularity&quot;&gt;two short stories against technology&lt;/a&gt; and &lt;a href=&quot;tra38.github.io/blog/ai3.html&quot;&gt;a blog post condemning artificial intelligence&lt;/a&gt;. There was still some human involvement involved. I wrote all the words in the short stories and the blog posts. But the algorithm also played a role in deciding the order of my paragraphs within the story. And the order matters just as much as the text. I even released my algorithm as an open-source Ruby gem called &lt;a href=&quot;https://github.com/tra38/Prolefeed&quot;&gt;Prolefeed&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So, yeah, I carried out a lesser version of the &quot;Ozymandias Gambit&quot;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Most people didn&#39;t cared.&lt;/em&gt; Those that did can be placed into two camps, the Futurists and the Traditionalists:&lt;/p&gt;

&lt;h2&gt;The Futurists&lt;/h2&gt;

&lt;p&gt;There are those who really liked the technology that I was working on. They saw it as cool and exciting, and thought about improving on my methods. Which, of course, &lt;em&gt;misses the point.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Which is why I started my blog post by quoting from &quot;A Colder War&quot; instead of from the &quot;Watchmen&quot;. &quot;A Colder War&quot; is a novella that had a premise: What if the superpowers discovered Lovecraftian technologies? The answer was simple...they started using it to fight their Cold War! The &#39;paper&#39; that the quote was referring to is a CIA report about Soviet Union&#39;s &quot;Project Koschei&quot; (a plan to use Cthulhu as a superweapon against the United States).&lt;/p&gt;

&lt;p&gt;So Ozymandias may have been too optimistic. Instead of seeing Ozymandias&#39; monster as a wakeup call, the superpowers could have simply started building their own monsters instead. I mean, Ozy already did it. How hard could it be?&lt;/p&gt;

&lt;p&gt;Ozymandias could, of course, take a perverse pride in contributing to the very fear that he opposed. In fact, he&#39;d probably make a killing selling his monster-building techniques to the superpowers. But the irony is there. It cannot be concealed or denied.&lt;/p&gt;

&lt;h2&gt;The Traditionalists&lt;/h2&gt;

&lt;p&gt;These people did view technology as a threat, and thus would sympathize with my anti-tech rants. They were mostly creeped out by the possibility of text generation. The computer-generated text may not be &#39;on par&#39; with human-generated work, but it could improve with time..and besides, computer-generated work doesn&#39;t have to be &quot;good&quot;. It just have to be &quot;good enough&quot;. This audience was receptive to my message.&lt;/p&gt;

&lt;p&gt;But that&#39;s it. Having felt chills down their spine, they haven&#39;t moved a single inch to stop this trend. It&#39;s almost as if they couldn&#39;t &lt;em&gt;do anything&lt;/em&gt; to stop it. There is almost a sense of resignation...that you can&#39;t fight progress. I guess the best you can do is write against the future...but then again, I already wrote an algorithm to write against the future.&lt;/p&gt;

&lt;p&gt;In this scenario, people saw Ozymandias&#39; evil monster, but resigned themselves to the fact that it existed and moved on with their lives. &quot;Yeah, a monster destroyed New York. What a shame. So whose going to win the Super Bowl this year?&quot; It&#39;s not like they hate the future...they certainly do. But they realize that they can&#39;t change it. All they can do is to accept it and prepare for it.&lt;/p&gt;

&lt;h2&gt;The Failure of the Ozymandias Gambit&lt;/h2&gt;

&lt;p&gt;Today, I sometimes &quot;write&quot; computer-generated blog posts about computer-generated literature and &lt;a href=&quot;https://dev.to/tra&quot;&gt;post them on dev.to&lt;/a&gt;, with plans to eventually port them over to this site. I like to inform people about this field and its technical merits, while also conveying my distaste and disgust. at the field as well.&lt;/p&gt;

&lt;p&gt;This blog post, however, was not computer-generated. I could have wrote an algorithm to write this blog post. I have to break this blog post up into smaller chunks, and then use &lt;a href=&quot;https://github.com/tra38/Prolefeed&quot;&gt;Prolefeed&lt;/a&gt; to shuffle the chunks. It is fairly easy (although time-consuming) to pull off. But today, I just wanted to express &lt;em&gt;my&lt;/em&gt; own feelings about my experience. Not what the computer wanted to write about. Maybe the computer might be a better writer than me. But I still want to vent my thoughts anyway. (Duke Greene, my DBC teacher, have also said the same thing about his own creative works...he likes doing them manually because he wants to express his own thoughts, without any such mechanical filters.)&lt;/p&gt;

&lt;p&gt;I&#39;ll probably keep writing against the future, warning people about the possible disaster of artificial intelligence and how we need societal regulation against it to limit its damages. But now I am under no illusion that my posts will actually change anything. At least I tried.&lt;/p&gt;
&lt;p&gt;Thank you for reading this blog post written by Tariq Ali, an aspiring web developer. If you are interested in reading more blog posts, please go to tra38.github.io/blog today.&lt;/p&gt;</content>
        <pubDate>Sat, 23 Jul 2016 00:00:00 -0500</pubDate>
        <link>tra38.github.io/blog/c23-one-year-later.html</link>
        <guid isPermaLink="true">tra38.github.io/blog/c23-one-year-later.html</guid>
        
        
      </item>
    
      <item>
        <title>T17 Markov</title>
        <description>![CDATA[According to Victor Powell, "Markov chains, named after Andrey Markov, are mathematical systems that hop from one "state" (a situation or set of values) to another." Many people have used Markov chains to generate literature based on a pre-existing corpus. This blog post explains some experiments I recently did with Markov chains, with the goal of generating readable text.
]</description>
        <content>&lt;p&gt;According to &lt;a href=&quot;http://setosa.io/ev/markov-chains/&quot;&gt;Victor Powell&lt;/a&gt;, &quot;Markov chains, named after Andrey Markov, are mathematical systems that hop from one &quot;state&quot; (a situation or set of values) to another.&quot; Many people have used Markov chains to generate literature based on a pre-existing corpus. This blog post explains some experiments I recently did with Markov chains, with the goal of generating readable text.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zolrath/marky_markov&quot;&gt;Marky_Markov&lt;/a&gt; is a Ruby gem that can be used to generate Markov chains. I used it to quickly set up a Markov chain and start fooling around with it.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;ruby&quot;&gt;require &#39;marky_markov&#39;

markov = MarkyMarkov::TemporaryDictionary.new
markov.parse_string &quot;The cat is very friendly.&quot;
markov.parse_string &quot;The dog is slightly friendly.&quot;
markov.parse_string &quot;The sky is very unfriendly.&quot;
markov.parse_string &quot;The window is slightly unfriendly.&quot;

markov.generate_10_sentences
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;=&gt;&quot;The window is slightly unfriendly. The window is slightly friendly. The sky is very unfriendly. The sky is very unfriendly. The window is slightly friendly. The sky is very friendly. The cat is very unfriendly. The cat is very unfriendly. The window is slightly unfriendly. The cat is very friendly.&quot;&lt;/p&gt;

&lt;p&gt;Some of the sentences are copies of the original corpus, but other sentences are original! The algorithm had learned how to write original sentences!&lt;/p&gt;

&lt;p&gt;These sentences are fairly dull though. The generator also repeats itself sometimes.&lt;/p&gt;

&lt;p&gt;At this point, the budding Markov Chainer will start searching for a pre-made corpus that can then be fed into the Markov Chain. But I&#39;m not a budding Markov Chainer...if anything, I&#39;m slightly jaded. The benefit of feeding a pre-made corpus into a Markov Chain is that the text start sounding more interesting. The problem is that the text also becomes less &lt;em&gt;coherent&lt;/em&gt;. Here&#39;s &lt;a href=&quot;https://brandonbyars.com/2007/05/06/tdding-a-markov-chain/&quot;&gt;some sample text from a Markov chain trained on &quot;The Raven&quot;&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Once upon a bust of Pallas just above my chamber door; –
This it is, and this mystery explore; –
‘Tis the wind and nothing more.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I can understand why some people may be happy with text that seems meaningful so long as you don&#39;t read it too deeply. In fact, this text could plausibly be used as poetry. Any oddities in the text itself can be explained away as stylistic choices and metaphors.&lt;/p&gt;

&lt;p&gt;But if we decide to use Markov chains outside of poetry, well...um....&lt;/p&gt;

&lt;p&gt;Here&#39;s &lt;a href=&quot;http://www.r-bloggers.com/is-deep-learning-a-markov-chain-in-disguise/&quot;&gt;some sample text from a Markov chain trained on the &#39;tinyshakespeare&#39; corpus&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;DUKE VINCENTIO:&lt;/p&gt;

&lt;p&gt;Well, your wit is in the care of side and that.&lt;/p&gt;

&lt;p&gt;FRIAR LAURENCE:&lt;/p&gt;

&lt;p&gt;Or walk liest;&lt;/p&gt;

&lt;p&gt;And the ears.&lt;/p&gt;

&lt;p&gt;And hell!&lt;/p&gt;

&lt;p&gt;In self.&lt;/p&gt;

&lt;p&gt;PETRUCHIO:&lt;/p&gt;

&lt;p&gt;Persuading to our the enemy, even woman, I&#39;ll afford show&#39;d and speaking of&lt;/p&gt;

&lt;p&gt;England all out what least. Be satisfied! Now, sir.&lt;/p&gt;

&lt;p&gt;Second Lord:&lt;/p&gt;

&lt;p&gt;They would be ruled after this chamber, and&lt;/p&gt;

&lt;p&gt;my fair nues begun out of the fact, to be conveyed,&lt;/p&gt;

&lt;p&gt;Whose noble souls I&#39;ll have the heart of the wars.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;...yeah, no. The blogger is surprised about how his Markov chain learned Old English, but I absolutely hate this output. This is why I had been reluctant to use Markov chains ever...because what they actually produce is utter nonsense.&lt;/p&gt;

&lt;p&gt;When I conduct experimentation in text generation, my goal is to produce text that reaches human quality. This bias may have been caused by my previous experience as a journalist, where I was expected to write words that people are able to read...not necessarily text that &lt;em&gt;appears&lt;/em&gt; to be readable. This causes me to prefer more &#39;conservative&#39; techniques in text generation (shuffling paragraphs, using pre-written templates, etc.)&lt;/p&gt;

&lt;p&gt;Recently, I started &lt;a href=&quot;https://medium.com/laughter-in-the-singularity&quot;&gt;a Medium blog&lt;/a&gt; where I showed off some fictional work that was computer-generated. This blog was somewhat controversial on Reddit, as can be seen in &lt;a href=&quot;https://www.reddit.com/r/proceduralgeneration/comments/4ken1d/laughter_in_the_singularity_computergenerated/?&quot;&gt;the comment thread&lt;/a&gt;. Many people raised the valid point that the algorithms used were just too simplistic for their tastes (as I was too reliant on my &#39;conservative&#39; techniques). While I was happy with my output (it was readable), I also realized that I needed to do slightly more experimentation. One reddit poster (kleer001) made a rather interesting comment:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;let&#39;s see some markov chains maybe?
here, have som fun with this: http://projects.haykranen.nl/markov/demo/
sorry I noped you, you sound like a cool dude, but still, you can do hella better. i believe in you.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hence, why I was fooling around with marky_markov.&lt;/p&gt;

&lt;p&gt;Looking at the output of my toy Markov chain (&quot;The window is slightly friendly&quot;) makes me realize that the main problem I had with Markov chains isn&#39;t necessarily the chain itself, but the corpus that people use to train on it. If you provide complicated prose to a Markov chain (such as Shakespeare&#39;s plays), you are not going to get good results out of it. What I&#39;m going to have to do is to simplify the prose to a level that a Markov chain will be able to grasp and imitate properly.&lt;/p&gt;

&lt;p&gt;I need to generate a corpus that follow these two rules:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Each sentence should have very simplistic and repetitive grammar.&lt;/li&gt;
&lt;li&gt;Each sentence should be context-less; there should be no connection to previous and future sentences (the Markov chain will not have the ability to understand the context of words).&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Now, writing out all these sentences for a corpus is rather dull...which can explain why some people reach for a pre-made corpus instead. But I remembered that I still had prior experience in text-generation. Why don&#39;t I write a program that can generate text...and then use its &lt;em&gt;output&lt;/em&gt; as a corpus? I used the Ruby gem &lt;a href=&quot;https://github.com/maetl/calyx&quot;&gt;&#39;Calyx&#39;&lt;/a&gt; to quickly set up a template-based text generator.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;require &#39;marky_markov&#39;
require &#39;calyx&#39;

markov = MarkyMarkov::TemporaryDictionary.new

class CorpusGenerator &amp;lt; Calyx::Grammar
  start &#39;{dialogue}, {gender} {speak} {mood}.&#39;
  rule :dialogue, &#39;Do you think she is out there&#39;, &#39;Do you think she cares&#39;, &#39;Will she ever care&#39;,
  &#39;We have a job to do&#39;, &#39;She has a job to do&#39;, &#39;I did the right thing&#39;,
  &#39;She did what was right&#39;, &#39;Do not kill me&#39;, &#39;I must kill you&#39;,
  &#39;You have a job to do&#39;, &#39;I am sorry&#39;, &#39;She is sorry&#39;,
  &#39;I hate you&#39;, &#39;I am sorry I met you&#39;
  rule :gender, &#39;he&#39;, &#39;she&#39;, &#39;the computer&#39;, &#39;the monster&#39;
  rule :speak, &quot;says&quot;, &quot;giggles&quot;, &quot;laughs&quot;, &quot;cries&quot;, &quot;sings&quot;
  rule :mood, &quot;sadly&quot;, &quot;madly&quot;
end

generator = CorpusGenerator.new

corpus = []

10.times do
  sentence = generator.generate
  corpus &amp;lt;&amp;lt; sentence
  markov.parse_string sentence
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, the problem is that the sentences are so basic that it&#39;s likely a Markov chain could just re-generate those same, basic sentences. That&#39;s no fun. But, the Markov chain does have the possibility of generating unique sentences too. I may need to measure how many &#39;duplicated&#39; sentences it generates, to determine how much &quot;uniqueness&quot; the Markov chain can produce.&lt;/p&gt;

&lt;p&gt;I decide to run my program with 10 corpus sentences, and then generate 10 sentences using my Markov chain, and then compare the uniqueness (the number of unique sentences divided by the number of total sentences generated).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SAMPLE = 10

SAMPLE.times do |sentence|
  generated_sentences &amp;lt;&amp;lt; markov.generate_1_sentence
end

duplicated_sentences = generated_sentences.select {|sentence| corpus.include?(sentence) }

def calculate_odds(duplicated_sentences)
  duplicates = (duplicated_sentences.length.to_f)
  percent_of_duplicate = (duplicates/(SAMPLE.to_f))
  1 - percent_of_duplicate
end

puts &quot;The number of duplicated sentences is #{duplicated_sentences.length}.&quot;
puts &quot;The uniqueness of the text is #{calculate_odds(duplicated_sentences)}.&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I also added some print statements to this code to view the original corpus and the generated sentences. I highlighted any unique sentences by using dashes.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#Input Corpus (Generated By Templates):
Do you think she cares, the monster says madly.
Do you think she cares, the monster sings madly.
She has a job to do, she giggles madly.
I am sorry, she sings sadly.
I am sorry I met you, the computer cries sadly.
I hate you, he says sadly.
I did the right thing, she laughs madly.
I am sorry I met you, she sings madly.
She has a job to do, the monster says sadly.
Will she ever care, the monster says madly.

#Ouput Sentences (Generated by Markov Chains):
I hate you, he says sadly.
I did the right thing, she laughs madly.
Will she ever care, the monster says madly.
She has a job to do, she giggles madly.
I hate you, he says sadly.
- I am sorry I met you, she sings sadly. -
She has a job to do, she giggles madly.
- Will she ever care, the monster says sadly. -
- Will she ever care, the monster sings madly. -
Will she ever care, the monster says madly.

The number of duplicated sentences is 7.
The uniqueness of the text is 0.19999999999999996.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, about 20% of the generated text is unique. That&#39;s...actually kinda justifiable. Surely, the corpus generator is doing most of the work, but the Markov chain is able to add some additional &quot;variation&quot; to the output.&lt;/p&gt;

&lt;p&gt;Usually, Markov chains work well when you give it a large corpus. But luckily, since we already have a corpus text generator, we can just tell it to produce as many sentences as we want. So, I generate a corpus of 1000 sentences, use that corpus to train a Markov chain, and then use the Markov chain to generate 1000 new sentences.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;The number of duplicated sentences is 712.
The uniqueness of the text is 0.28800000000000003.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Excellent. 288 unique sentences! I wonder what they read like...&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;I met you, the monster giggles madly.
a job to do, she sings madly.
I met you, she cries madly.
must kill you, she sings madly.
do, the monster laughs sadly.
You have a job to do, the computer laughs sadly.
he cries madly.
job to do, the computer cries madly.
a job to do, she sings madly.
me, he giggles madly.
I met you, the monster cries madly.
you think she is out there, she says madly.
he says madly.
She is sorry, the monster cries sadly.
she cries madly.
I met you, the monster laughs sadly.
right, she laughs sadly.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Oh.&lt;/p&gt;

&lt;p&gt;Clearly, there&#39;s still work to be done, especially with the initial text generator. The sentence fragments are annoying (though I presume I can write some more code to scan the generated text and delete any sentence that aren&#39;t complete). The bigger problem is that this wall of text is...fairly dull and boring. I don&#39;t really care why all these characters are speaking.&lt;/p&gt;

&lt;p&gt;I need to try to create a somewhat sensible theme, at least one well enough that to justify having all these random people talking to each other. Or maybe I need to have different types of sentences that could be generated (possible descriptions and actions) that makes the generated text slightly more engaging.&lt;/p&gt;

&lt;p&gt;The Markov chain interestingly was able to generate a phrase that was not specifically hardocded into the generator...&quot;I met you&quot; (obviously an extract from &quot;I am sorry I met you&quot;). And yes, it can repeat that phrase a lot.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;I met you, the monster says sadly.
I met you, the computer giggles madly.
I met you, the monster says sadly.
I met you, the computer laughs madly.
I met you, the monster sings madly.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I am unsure whether this experiment was a success. On the one hand, the Markov chain did add &#39;value&#39; to the process, by generating fairly unique sentences. On the other hand, much of this additional variation could have been easily accomplished by simply running the text generator again. The variations that the Markov chain does produce are fairly trivial and may not be interesting on its own.&lt;/p&gt;

&lt;p&gt;I am also dubious on whether more uniqueness would actually be a worthwhile goal. &lt;a href=&quot;http://procedural-generation.tumblr.com/post/137167846082/infinity-and-procedural-generation-i-was-asked-on&quot;&gt;Infinite text generation itself may not be very interesting&lt;/a&gt;. You&#39;re usually only going to read a small fraction of whatever the program generates, before you get bored and ready to move onto the next content to consume. If that&#39;s the case, then why try to increase the number of possible variations? The reader is still going to get bored anyway. It may be smarter to instead focus on making each individual variation interesting.&lt;/p&gt;

&lt;p&gt;The one bright spot in this research is its possible application to machine learning. The blogger who trained Markov chains on the &#39;tinyshakespeare&#39; corpus asked &lt;a href=&quot;http://www.r-bloggers.com/is-deep-learning-a-markov-chain-in-disguise&quot;&gt;&quot;Is Deep Learning a Markov Chain In Disguise?&quot;&lt;/a&gt; The output of a machine learning algorithm (when trained on text) is only slightly better than that of a Markov chain, after all. If you can write a program that can generate a corpus that then can train a Markov chain to produce meaningful text, then you could reuse that same program on a machine learning algorithm. Maybe their output may be more interesting to read.&lt;/p&gt;
&lt;p&gt;Thank you for reading this blog post written by Tariq Ali, an aspiring web developer. If you are interested in reading more blog posts, please go to tra38.github.io/blog today.&lt;/p&gt;</content>
        <pubDate>Sat, 18 Jun 2016 00:00:00 -0500</pubDate>
        <link>tra38.github.io/blog/t17-markov.html</link>
        <guid isPermaLink="true">tra38.github.io/blog/t17-markov.html</guid>
        
        
      </item>
    
      <item>
        <title>T16 Resume Driven Development</title>
        <description>![CDATA[Disclaimer: I wrote the book "Essential Copying and Pasting From Stack Overflow", which was inspired by a cover designed by ThePracticalDev. This blog post is also inspired by a cover designed by ThePracticalDev. Just like "Essential Copying and Pasting From Stack Overflow", "Expert Resumé Driven Development" is also written in a deadpan manner.
]</description>
        <content>&lt;p&gt;Disclaimer: I wrote the book &lt;a href=&quot;https://tra38.gitbooks.io/essential-copying-and-pasting-from-stack-overflow/content/&quot;&gt;&quot;Essential Copying and Pasting From Stack Overflow&quot;&lt;/a&gt;, which was inspired by a cover designed by &lt;a href=&quot;https://twitter.com/thepracticaldev?lang=en&quot;&gt;ThePracticalDev&lt;/a&gt;. This blog post is also inspired by a cover designed by ThePracticalDev. Just like &quot;Essential Copying and Pasting From Stack Overflow&quot;, &quot;Expert Resumé Driven Development&quot; is also written in a deadpan manner.&lt;/p&gt;

&lt;p&gt;&lt;img style=&quot;float: left; height: 30%; width: 30%&quot; src=&quot;http://i.imgur.com/DQcks8u.jpg&quot; alt=&quot;Cover for Resumé Driven Development, &#39;The Passionate, Functional, Micro-Serviced Approach&quot;&gt;&lt;h3&gt;What is &quot;Resumé Driven Development&quot;?&lt;/h3&gt;&lt;/p&gt;

&lt;p&gt;Resumé Driven Development refers to the practice of choosing hot new technologies for your projects to make your resumé more impressive. For example, you want to write a program in that hot new Javascript micro-generator framework &lt;a href=&quot;https://github.com/amwmedia/plop&quot;&gt;&quot;Plop.js&quot;&lt;/a&gt;...because it&#39;s a hot new JavaScript framework. Sure, you can use the technologies that you already know and are probably better suited for the job, but that wouldn&#39;t be as impressive as riding the hot bandwagon. You don&#39;t even know what &quot;Plop.js&quot; is, but you&#39;ll find out soon enough.&lt;/p&gt;

&lt;p&gt;There is some controversy on whether Resumé Driven Development is actually good. &lt;a href=&quot;http://willcode4beer.com/opinion.jsp?set=in_favor_of_rdd&quot;&gt;Paul E. Davis defended the practice&lt;/a&gt; as it is an indictator that the developer is willing to learn new technologes instead of sticking with potentially obsolete solutions. Sure, &quot;Plop.js&quot; may not actually be suited for your use case, but at least you showed that you &lt;em&gt;can&lt;/em&gt; learn it and use it quickly, and that you could later on find the bleeding-edge technology that might actually be useful.&lt;/p&gt;

&lt;p&gt;&quot;In the end, if developers are not concerned with their own careers, it&#39;s not likely they&#39;ll be concered with your business.&quot;---Paul E. Davis&lt;/p&gt;

&lt;p&gt;Most developers seem to detest RDD though (as can be seen by &lt;a href=&quot;http://rdd.io&quot;&gt;this parody website&lt;/a&gt;). There is a general belief that developers are not using using the hot new technologies in their spare time, but instead &lt;a href=&quot;http://www.healthcareguy.com/2007/01/19/resume-driven-development-rdd/&quot;&gt;using them on company projects&lt;/a&gt;. This is a colossal waste of company funds, as the technology may be ill-suited for the task at hand and can lead to long-term maintanance headaches.&lt;/p&gt;

&lt;p&gt;Developers are not the only people that engage in Resumé Driven Development. Management may also engage in RDD &lt;a href=&quot;http://radar.oreilly.com/2014/10/resume-driven-development.html&quot;&gt;during the hiring process as well&lt;/a&gt;. If a job vacancy exists, there is a desire to hire a new programmer with the same skillset of the old programmer. So the old programmer was an expert in Plop.js, then the job posting will read &quot;6 months  experience with Plop.js&quot;. This may be a very bad thing because you are optimizing for knowledge of certain technologies, instead of choosing the &quot;right tool&quot; for the job. If you only hire Plop.js developers, you will only get Plop.js websites.&lt;/p&gt;

&lt;p&gt;RDD is probably a subset of a larger &lt;a href=&quot;https://en.wikipedia.org/wiki/Principal%E2%80%93agent_problem&quot;&gt;Principal-Agent Problem&lt;/a&gt;. The Principal (management) hires an Agent (a developer) to build a program and allow the Agent to choose the tech stack. But the Agent&#39;s interest (making his resumé more impressive) can be orthogonal to the Principal&#39;s interest (producing a great product by using the &quot;right tool&quot; for the job). If the Principal allows the Agent to do as he wish, then the Agent &lt;strong&gt;will&lt;/strong&gt; do as he wishes, thereby leading to the Agent to prosper and the Principal to suffer.&lt;/p&gt;

&lt;p&gt;RDD is composed of two sections: the Resumé and the Technology. Let&#39;s look at them both briefly.&lt;/p&gt;

&lt;h3&gt;The Role of the &quot;Resumé&quot;&lt;/h3&gt;


&lt;p&gt;A resumé serves to let people know what you have done, in the hopes of getting people impressed. According to &lt;a href=&quot;http://cdn.theladders.net/static/images/basicSite/pdfs/TheLadders-EyeTracking-StudyC2.pdf&quot;&gt;a study by TheLadders, a resume re-writing company&lt;/a&gt;, recruiters can spend 6 seconds per  resumé, scanning for keywords and cursory background information about your education and job history before deciding whether you are &quot;fit&quot; or &quot;no-fit&quot;. You can see why some developers want to maximize those 6 seconds by practicing RDD.&lt;/p&gt;

&lt;p&gt;Now, there is not that much in-depth examination of what&#39;s on the resumé. If someone says that they used Plop.js when building a Uber-For-Dogs application, then you assume that they did use Plop.js and that there actually is a Uber-For-Dogs application...that he really spent 6 months on it...and that he got to pet a unicorn while on the job. It says so on the resumé. Just keep scanning for more keywords then.&lt;/p&gt;

&lt;p&gt;You can see where I&#39;m going here. Resumés can lie. You can claim to have worked on sixty Plop.js projects in closed-source companies, and provide fake phone numbers and references as &#39;proof&#39; that these closed-source projects exist. Lying is horribly unethical, and I would not recommend it for anyone to do. But it can be done, and is probably more efficent than a pure RDD approach...at least in getting the foot in the door. However, at the interview stage, interviewers will attempt to weed out those that did lie on their resumés - such as asking basic questions like &quot;What is Plop.js?&quot;.&lt;/p&gt;

&lt;p&gt;So the better option is to actually use the technologies on real projects then, so that when you get to the interview stage, you can actually prove your expertise in it and not be exposed as a fraud. That (sadly) means you do have to build a Uber-For-Dogs application just so you can use the Plop.js keyword. You may even need to provide a link to the application and show the source code, in case someone looks at your resumé for longer than six seconds. However, once you do this, you do not have to worry about proving anything else. If you can demonstrate that you can use Plop.js, then most people will assume that you know how to use it.&lt;/p&gt;

&lt;h3&gt;Choosing the Hot Technology&lt;/h3&gt;


&lt;p&gt;The biggest problem with choosing hot new technologies isn&#39;t actually learning them. It&#39;s difficult, and you&#39;ll have to deal with bad documentation and difficult solutions, but given enough time and persistence, you will eventually succeed.&lt;/p&gt;

&lt;p&gt;No, the biggest problem with choosing hot new technologies is that you don&#39;t &lt;strong&gt;know&lt;/strong&gt; what is actually hot.&lt;/p&gt;

&lt;p&gt;You can guess. You can listen to a community, and if the community talks a lot about Plop.js, then that suggest that Plop.js is hot. If the community stops talking about Plop.js, then Plop.js must not be hot. You can bookmark &lt;a href=&quot;https://www.google.com/trends/&quot;&gt;Google Trends&lt;/a&gt; and pay attention to what words people are searching. You can look at job postings and see what keywords the recruiters are using. And so on and so forth.&lt;/p&gt;

&lt;p&gt;But they&#39;re all trailing indictators. It doesn&#39;t help you predict whether that tech will stay hot in the future.&lt;/p&gt;

&lt;p&gt;It is here that Gartner Hype Cycle can be most useful for an RDDer. Gartner argues that all new technologies go through a cycle. I described the Hype Cycle in &lt;a href=&quot;http://tra38.github.io/blog/ai3.html&quot;&gt;my blog post against Artifical Intelligence&lt;/a&gt;...and I reprint my comments on this cycle here:&lt;/p&gt;

&lt;p&gt;&lt;img style=&quot;float: right;&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/9/94/Gartner_Hype_Cycle.svg/559px-Gartner_Hype_Cycle.svg.png&quot; &gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;At first, people become very interested in a brand new technology (a &quot;Technology Trigger&quot;). Companies start to over-invest in researching this technology, leading to a &quot;Peak of Inflated Expectations&quot; (i.e, a bubble). But the technology turns out to have major limitations. As a result, investment in the technology dries up (&quot;Trough of Disillusionment&quot;). Most companies either begin laying people off or closing down outright.&lt;/p&gt;

&lt;p&gt;Eventually, the survivors soon realize how to use the technology properly (&quot;Slope of Enlightenment&quot;), and we can finally use the technology in our day-to-day life (&quot;Plateau of Productivity&quot;). But as this picture from Wikipedia shows, the visibility of the technology in the Plateau of Productivity is much less than the visibility of that same technology in the Peak of Inflated Expectations. The brand new technology has done great things for us. It&#39;s just not as great as we hoped it to be. And does it justify the extreme waste seen in the &quot;Peak of Inflated Expectations&quot;?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Since the technology reaches the maximum visibility/hype in the Peak of Inflated Expectations, this implies that, by the time you hear of a tech, &lt;em&gt;it&#39;s already too late&lt;/em&gt;. The only people who would be able to take advantage of the hype are the &#39;early adopters&#39;, and they are the ones who will likely reap most of the benefits (though they also accept most of the costs as well, since not all new technologies are interesting or useful enough to go through this Hype Cycle).&lt;/p&gt;

&lt;p&gt;Another message this charts tells us is that the best time to learn a new technology is either during the Technology Trigger or the Trough of Disillusionment.&lt;/p&gt;

&lt;p&gt;During the Technology Trigger stage, there are no experts. There are only a few people who know your keyword. Specializing now as an &#39;early adopter&#39; (while the field is still young) would be much more impressive than being an expert during the Peak of Expectation, when you have thousands of &#39;experts&#39; in your field. Building a bot that can draw paintings now seems neat. Building a paint-bot five years from now, when every programmer have already built their own personal paint-bot? &lt;strong&gt;Boring.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;During the Trough of Disillusionment, you are able to take advantage of previous research done during the Technology Trigger and the Peak of Inflated Expectations, learning from past successes and failures with the tech. You also will have less competition during the Trough, as most programmers have already left to move onto the next &quot;hot&quot; technology. Eventually, interest in the tech will revive, and your lovely keyword will regain some allure.&lt;/p&gt;

&lt;p&gt;Any other time to study a technology would carry either too much competition (the Peak of Inflated Expectations) or too little reward (the Plateau of Productivity) to justify the expense. The only reason you would want to study the technology in that case is because you think it might actually help you grow as a developer.&lt;/p&gt;

&lt;!-- Now, if you are not dealing with recruiters but instead cold-calling or networking with companies directly, then it is possible that a company will probably do more in-depth research on you before deciding &quot;fit&quot;/&quot;no-fit&quot;. This may seem good (more than 6 seconds&#39; attention) but it also means that your resumé itself might play a lesser role in the deciding process (lessening the need for RDD). Instead, your Side Projects and GitHub Open Source contributions might play a larger role (though you can also highlight that on your resumé). --&gt;

&lt;p&gt;Thank you for reading this blog post written by Tariq Ali, an aspiring web developer. If you are interested in reading more blog posts, please go to tra38.github.io/blog today.&lt;/p&gt;</content>
        <pubDate>Thu, 12 May 2016 00:00:00 -0500</pubDate>
        <link>tra38.github.io/blog/t16-resume-driven-development.html</link>
        <guid isPermaLink="true">tra38.github.io/blog/t16-resume-driven-development.html</guid>
        
        
      </item>
    
      <item>
        <title>T15 Architect</title>
        <description>![CDATA[Can humans build a story-telling machine that can trick other human beings (a la the 'Turing Test')? Dartmouth College's Neukom Institute for Computational Science is hosting a competition ("DigiLit") to see if that can be done. I submitted a program (Architect) to this program. I think it has a good shot of winning (~40%).
]</description>
        <content>&lt;p&gt;Can humans build a story-telling machine that can trick other human beings (a la the &#39;Turing Test&#39;)? Dartmouth College&#39;s Neukom Institute for Computational Science is hosting a competition (&quot;DigiLit&quot;) to see if that can be done. I submitted a program (Architect) to this program. I think it has a good shot of winning (~40%).&lt;/p&gt;

&lt;p&gt;The idea behind &lt;a href=&quot;http://bregman.dartmouth.edu/turingtests/digilit&quot;&gt;DigiLit&lt;/a&gt; is simple. You build a machine that can accept an arbitrary noun prompt (&quot;wedding&quot;, &quot;sorrow&quot;, &quot;car keys&quot;, etc.). The machine then uses this noun prompt to generate a short story that is 7000 words or less. Then this short story is then presented to two panels (each with 3 judges). If you can convince a majority of one panel that your story is human-generated, then you win.&lt;/p&gt;

&lt;p&gt;This does seem like a difficult task. You must:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Have the computer accept arbitrary input&lt;/li&gt;
&lt;li&gt;Generate a story using the input&lt;/li&gt;
&lt;li&gt;Make sure the story is coherent enough to be readable&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Yet, I think I completed this task. After I finished writing my program (&lt;strong&gt;Architect&lt;/strong&gt;, named after the program in the Matrix trilogy), I decided to test it out. I chose a random noun prompt, and wrote a story based on it. I also had my program generate a story as well (using that same noun prompt). I then gave those two stories to 7 people. Story A was written by me, and Story B was written by the program.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;3 people correctly identified Story A as being written by a human.&lt;/li&gt;
&lt;li&gt;3 people wrongly identified Story B as being written by a human.&lt;/li&gt;
&lt;li&gt;1 person admitted he didn&#39;t know.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This means that 43% of the humans were fooled. That&#39;s pretty good. And it&#39;s good enough for me to submit it over to DigiLit.&lt;/p&gt;

&lt;p&gt;That doesn&#39;t mean that victory is assured, after all. 43% isn&#39;t exactly a majority. But consider that I really need to convince 2 out of 6 judges to have a good chance of &#39;convincing&#39; a whole panel of 3 judges (assuming that both judges sit on the same panel). That means I only need to convince 33% of all judges.&lt;/p&gt;

&lt;p&gt;To be 100% certain of convincing a panel though, I would need to convince 3 out of 6 judges (so that no matter how the judges are distributed, at least a majority of those I convinced would sit on a single panel). That does require me to be able to convince 50% of all judges. That doesn&#39;t seem too likely.&lt;/p&gt;

&lt;p&gt;One main problem with my study is that the format of my test and the DigiLit test is slightly different. In my test, each person is reading both stories individually and then making a choice. In DigiLit, the judges sit on a panel, have access to multiple stories (some human-generated and some machine-generated) so they can detect patterns and &#39;tells&#39;, and can talk to one another and share their insights. The mob will likely find more faults in a story than a single human can. So my study may likely overestimate how good my program actually is.&lt;/p&gt;

&lt;p&gt;But enough boring stats. Here&#39;s &lt;a href=&quot;https://github.com/tra38/Architect&quot;&gt;the source code&lt;/a&gt; of Architect and a brief description of how it works:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
  Have the computer accept input:
  &lt;ul&gt;
  &lt;li&gt;Plug the input into a madlib template: {INPUT} {LOCATION_TYPE}. For example:
    &lt;ul&gt;
      &lt;li&gt;Sorrow Academy.&lt;/li&gt;
      &lt;li&gt;Wedding Plaza.&lt;/li&gt;
      &lt;li&gt;Car Keys Muesum&lt;/li&gt;
    &lt;/ul&gt;
  The story will therefore be about a generated LOCATION, and not directly about the noun prompt. Here, I&#39;m assuming that reader will assume that when the program is writing about &quot;Sorrow Academy&quot;, they assume it to be symbolic of the actual noun in question (&#39;sorrow&#39;).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;br&gt;
&lt;li&gt;Generate a story using that input:
  &lt;br&gt;
  &lt;ul&gt;
    &lt;li&gt;Randomly pick an introductory sentence from a corpus of introductory sentences.&lt;/li&gt;
    &lt;li&gt;Load the program with several pre-written passages.&lt;/li&gt;
    &lt;li&gt;Randomly pick a few of those passages. Insert in the LOCATION into the passages (so that the passages would therefore be about the input in question).&lt;/li&gt;
    &lt;li&gt;Between each passage, insert a &quot;transition passage&quot; to link the two passages together (providing an illusion of continuity between the passages). The &quot;transition passage&quot; was also pre-written.&lt;/li&gt;
    &lt;li&gt;Randomly pick an conclusion sentence from a corpus of conclusion sentences.&lt;/li&gt;
  &lt;/ul&gt;
This isn&#39;t a story with any planned plot or coherence. The program has no idea what it&#39;s writing. But since there is an illusion of continuity between passages, humans are able to read a &#39;narrative&#39;.
&lt;/li&gt;
&lt;br&gt;
&lt;li&gt;
  Have the story be thematically coherent enough to be readable.
  &lt;br&gt;
  &lt;ul&gt;
    &lt;li&gt;Each sentence of the story must have certain repeated symbols, themes, and characters within them, so that readers are more likely to assume that there is some purpose behind the words. (This approach was originally used in the &lt;a href=&quot;https://web.archive.org/web/20061112014356/http://www.brown.edu/Courses/FR0133/Fairytale_Generator/gen.html&quot;&gt;Fairy Tale Generator&lt;/a&gt; to create interesting stories based on random paragraphs.)&lt;/li&gt;
    &lt;li&gt;Since I&#39;m too lazy to hand-craft most of these sentences myself, I decided to use a preexisting generator (Abulafia&#39;s &lt;a href=&quot;http://www.random-generator.com/index.php?title=Film_Noir_Monologue&quot;&gt;Film Noir Monologue&lt;/a&gt; generator) to generate most of the paragraphs, introductory sentences, and conclusion sentences. Since this generator already had a prebuilt theme in mind (a cynical detective trying to solve a mysterious case), the resulting output does have some sort of thematic coherence. I then edited most of the paragraphs and sentences to provide even more thematic coherence.&lt;/li&gt;
    &lt;li&gt;The &#39;transition passages&#39;, however, were handwritten by me. Again, their goal is to provide thematic coherence and to &quot;fit&quot; with the rest of the passages.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;This was the computer-generated story that I presented to my readers during the test:&lt;/p&gt;

&lt;blockquote&gt;&lt;blockquote&gt;&lt;p&gt;It was a dark day—maybe normal for this time of year, but today the big city felt even darker and more sinister.&lt;/p&gt;

&lt;p&gt;Miss Kitty Rider prowled through my door like a tigress slinks into a Burmese orphanage—a pinup blonde with legs as far as you could want ’em. No dame her age could afford a coat like that, my money was right where my mind was: the gutter. She was to bad news what apple pie is to America. The dame was all business—before I could even close the office door, she told me she wants Investigator Blake dead. Turns out Blake is masterminding the gang violence, and wants to seize the Envy Foundation for himself by killing off all his rivals. I laughed at her; Blake may be a devil, but he&#39;s a devil I can work with. I wasn&#39;t going to sell him out.&lt;/p&gt;

&lt;p&gt;I asked Miss Kitty Rider if there was another way to end the gang violence, without having to take out Investigator Blake. Miss Kitty Rider responded me that if I wanted to bring an end to the violence, then I needed someone who is outside of the system. She recommended a fresh recruit from the police academy who was &#39;on the ball&#39;. Mr. Simpson. I foolishly took her advice.&lt;/p&gt;

&lt;p&gt;“An inside job…?” Simpson gasped timidly. “Well … No… not an inside job,” I growled. I could barely contain myself with this new guy. “Here’s the deal,” I muttered, “Why don’t I handle this case, while you make like a magnet … and flux off?”&lt;/p&gt;

&lt;p&gt;Before I left, Mr. Simpson offered me the phone number to the office. He claimed that the office had helped him greatly and that it can help me too. Desperate for any clues, I took Mr. Simpson&#39;s advice. I spoke to the receptionist for an hour, and jotted down everything she said.&lt;/p&gt;

&lt;p&gt;I had little to go on this time … but the office did manage to dig something up. G-Men, fighting crime and resisting corruption, deciding that the best way to fight crime and resist corruption is to kill everyone who might be a criminal and might be corrupt. Their current goal is to &#39;purify&#39; the Envy Foundation before moving onto other tourist attractions. I always knew that G-Men were a little &#39;special&#39;.&lt;/p&gt;

&lt;p&gt;I shoulda got out when I had the chance.&lt;/p&gt;&lt;/blockquote&gt;&lt;/blockquote&gt;

&lt;p&gt;Now, this story does have its faults. According to one of my readers, there isn&#39;t really any coherent plot, and no actual attempt to explain what&#39;s going on. And I agree with that reader. The stories that my program spits out are really more like &#39;prose poetry&#39;: the plot is really just an excuse to show off different evocative scenes. But I know that some people may like reading these stories anyway. Evocative scenes are enjoyable in their own right.&lt;/p&gt;

&lt;p&gt;In addition, another reader praised the computer-generated story as having &quot;linear progression&quot;, and a &quot;[natural] level of detail and organization and flow&quot;. This suggest that the existence of some underlining structure may also help to make this story more appealing to read.&lt;/p&gt;

&lt;p&gt;What is undeniable is that my computer has written a story.&lt;/p&gt;

&lt;p&gt;My entry is based on a simple trick: Humans has a tendency to see patterns in everyday life, even when no patterns exist. The very act of me placing words right next to each other imply that the words must have some &lt;em&gt;relationship&lt;/em&gt; with each other. So the humans create this &lt;em&gt;relationship&lt;/em&gt; within their own mind. This tendency for humans to see patterns where none exist has a name: &quot;apophenia&quot;.&lt;/p&gt;

&lt;p&gt;All I have to do is to provide some context and themes to help trick the humans into assuming that there must be meaning in the words that the program generates. Once that happens, the humans will then fill in the details by interpreting the program&#39;s words. The computer can write total nonsense...and that&#39;s okay, because the humans will just happily figure out the meaning of that nonsense. While the humans figure out that meaning, they thereby construct the story out of that nonsense. A story, therefore, appears from a mere collection of words.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Appendix A: Is This Story Of High-Quality?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I don&#39;t know. It appears that if the judges think that a story is human-written, they will generally assume it to be better...&lt;/p&gt;

&lt;p&gt;This sounds silly, but according to my data, the rating of the quality of the story is dependent on whether the judge assume the story is written by a human.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Out of the 3 people who correctly identified Story A as being written by a human, 2 of them gave that story higher marks than Story B, while 1 person gave Story B higher marks.&lt;/li&gt;
&lt;li&gt;Out of the 3 people who wrongly identified Story B as being written by a human, all of them gave Story B higher marks.&lt;/li&gt;
&lt;li&gt;The one person who didn&#39;t know which story was human-written? He gave both stories &lt;em&gt;equal&lt;/em&gt; marks.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This is a pretty odd result and suggests to me that the &#39;origin&#39; of a story impacts how judges view its quality. This makes some sense: humans like reading what other humans have written, and if they assume that something is human-written, they are willing to think of it as good (even if it&#39;s not). But at the same time, I do have a small sample size, and it&#39;s possible that a different group of judges would determine the quality of a story more &quot;objectively&quot;.&lt;/p&gt;

&lt;p&gt;By the way, there have been several peer-reviewed studies about how humans&#39; perceptions of news articles can change based on whether they were told the news article was written by a &#39;human&#39; or by a &#39;robot&#39;, and I do plan on blogging on these articles later. I trust the results of those peer-reviewed studies over that of my unscientific study.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Appendix B: Can This Program &#39;Scale&#39;?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Using a single noun prompt, my story can generate 120 different short stories. Most people are not going to read all 120 stories. Instead, they&#39;ll only read a few computer-generated stories before moving onto the next content to consume. So I&#39;m fine with how the algorithm works currently. There&#39;s no reason to spend time writing more code than is necessary.&lt;/p&gt;

&lt;p&gt;That being said, the algorithm could &quot;scale&quot; upwards. If the program is given more passages, you could easily generate more short stories (or even &lt;em&gt;longer&lt;/em&gt; stories, possibly even novels). Finding those passages (and making sure they are all thematic coherent) may be somewhat difficult to do, and would most likely require using passages from Creative Commons or public domain works.&lt;/p&gt;

&lt;p&gt;Note that the &quot;transition phrases&quot; will have to be made more &#39;generic&#39; and reusable for different passages. Currently, I had to handwrite each transition phrase to handle specific situations (for example, I handwrote a the transition phrase to justify our unnamed detective leaving Mr. Simpson&#39;s place and calling the Office). Handwriting each transition phrase is a very laborious and tedious process. Having &quot;generic&quot; transition phrases, on the other hand, would have enabled me to focus on copying and pasting as many evocative scenes as possible into the generator.&lt;/p&gt;
&lt;p&gt;Thank you for reading this blog post written by Tariq Ali, an aspiring web developer. If you are interested in reading more blog posts, please go to tra38.github.io/blog today.&lt;/p&gt;</content>
        <pubDate>Thu, 21 Apr 2016 00:00:00 -0500</pubDate>
        <link>tra38.github.io/blog/t15-architect.html</link>
        <guid isPermaLink="true">tra38.github.io/blog/t15-architect.html</guid>
        
        
      </item>
    
      <item>
        <title>Ai3</title>
        <description>![CDATA[I was 'inspired' to write this article because I read the botifesto "How To Think About Bots". As I thought the 'botifesto' was too pro-bot, I wanted to write an article that takes the anti-bot approach. However, halfway through writing this blog post, I realized that the botifesto...wasn't written by a bot. In fact, most pro-bot articles have been hand-written by human beings. This is not at all a demonstration of the power of AI; after all, humans have written optimistic proclamations about the future since the dawn of time.
]</description>
        <content>&lt;p&gt;I was &#39;inspired&#39; to write this article because I read the botifesto &lt;a href=&quot;http://motherboard.vice.com/read/how-to-think-about-bots&quot;&gt;&quot;How To Think About Bots&quot;&lt;/a&gt;. As I thought the &#39;botifesto&#39; was too pro-bot, I wanted to write an article that takes the anti-bot approach. However, halfway through writing this blog post, I realized that the botifesto...wasn&#39;t written by a bot. In fact, most pro-bot articles have been hand-written by human beings. This is not at all a demonstration of the power of AI; after all, humans have written optimistic proclamations about the future since the dawn of time.&lt;/p&gt;

&lt;p&gt;If I am to demonstrate that AI is a threat, I have to also demonstrate that AI &lt;em&gt;can be&lt;/em&gt; a threat, and to do that, I have to show what machines are currently capable of doing (in the hopes of provoking a hostile reaction).&lt;/p&gt;

&lt;p&gt;So this blog post has been generated by a robot. I have provided all the content, but an algorithm (&quot;Prolefeed&quot;) is responsible for arranging the content in a manner that will please the reader. Here is the &lt;a href=&quot;https://gist.github.com/tra38/8a6bf3743cd89687151c&quot;&gt;source code&lt;/a&gt;. And as you browse through it, think of what else can be automated away with a little human creativity. And think whether said automation would be a good thing.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Unemployment&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In 2013, Oxford Professors Frey and Obsurne argued that robots will replace 70 million jobs in the next 20 years (or 47% of all jobs in the USA). J.P Gownder, an analyst at the Boston-based tech research firm &quot;Forrester&quot;, makes a more optimistic case for technology in 2015, by claiming that by the year 2025, robots will only cause a net job loss of 9.1 million. (Both studies came from &lt;a href=&quot;http://www.wired.com/2015/08/robots-will-steal-jobs-theyll-give-us-new-ones/&quot;&gt;Wired&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;J.P. Gownder argued his lower estimate for job loss is because technology will create new jobs. In a Forbes article, J.P. Gowdner &lt;a href=&quot;http://www.forbes.com/sites/forrester/2015/08/24/robots-wont-steal-all-the-jobs-but-theyll-transform-the-way-we-work/#1d0356cd29a3&quot;&gt;justified his viewpoint&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We forecast that 16% of jobs will disspear[sic] due to automation technologies between now and 2025, but that jobs equivalent to 9% of today’s jobs will be created. Physical robots require repair and maintenance professionals — one of several job categories that will grow up around a more automated world. That’s a net loss of 7%: far fewer than most forecasts, though still a significant job loss number.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The same &#39;adjustment&#39; for job gains was also done in a 2016 report by &lt;a href=&quot;http://reports.weforum.org/future-of-jobs-2016/employment-trends/&quot;&gt;World Economic Forum at Davos&lt;/a&gt;. &quot;[D]isruptive labour market changes&quot; (which includes not only AI, but other emerging tech such as 3D printing) could destroy 7.1 million jobs by 2020, while also creating 2 million jobs in smaller industry sectors. This means a net total of 5.1 million jobs lost.&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;&lt;td&gt;Study&lt;/td&gt;&lt;td&gt;Net Job Loss/Year&lt;/td&gt;
    &lt;tr&gt;&lt;td&gt;Frey and Obsurne (2013)&lt;/td&gt;&lt;td&gt;3.50 million&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td&gt;J.P. Gownder (2015)&lt;/td&gt;&lt;td&gt;0.91 million&lt;td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td&gt;World Economic Forum (2016)&lt;/td&gt;&lt;td&gt;1.02 million&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;


&lt;p&gt;There are some people who claim that we should not worry about robots because we&#39;ll just create brand new jobs out of thin air. To me, they are behaving like a complusive gambler boasting about how he can earn $2.82 by simply gambling away $10.&lt;/p&gt;

&lt;p&gt;In the long-term, perhaps, technology may finally erase the deficit in jobs and be seen as a net job producer. But that is exactly why we need to worry about this &quot;transition period&quot; to this &#39;long-term&#39;, whenever that may arrive.&lt;/p&gt;

&lt;p&gt;How will this new joblessness come into being? Personally, I do not foresee a bunch of people getting laid off immediately. Instead, companies will gradually reduce their hiring. Why hire a new [PROFESSION_NAME_HERE] when you can just get a robot to do the job for you? Existing employees may be deemed &#39;obsolete&#39;, but will be retrained with new skills that cannot be automated (yet).&lt;/p&gt;

&lt;p&gt;However, an academic paper entitled &lt;a href=&quot;http://miguelmorin.com/docs/Miguel_Morin_Great_Depression.pdf&quot;&gt;&quot;The Labor Market Consequences of Electricity Adoption: Concrete Evidence From The Great Depression&quot;&lt;/a&gt;, by Miguel Morin, does suggest that technological unemployment will indeed take the form of layoffs. During the Great Depression, the cost of electricity decreased for concrete plants. This increased the productivity of workers. Instead of increasing the production of concrete though, the concrete plants simply fired workers instead, thereby cutting their costs. &lt;a href=&quot;http://www.theatlantic.com/magazine/archive/2015/07/world-without-work/395294/&quot;&gt;The Atlantic&lt;/a&gt; also wrote several examples where technological unemployment occurred during times of recessions...when companies need to save money, humans get laid off and the cheaper bots come in instead.&lt;/p&gt;

&lt;p&gt;I hope I do not need to write out why unemployment is not good.&lt;/p&gt;

&lt;p&gt;&lt;img style=&quot;float: left;&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/9/94/Gartner_Hype_Cycle.svg/559px-Gartner_Hype_Cycle.svg.png&quot; &gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The &quot;AI Winter&quot;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Artificial Intelligence is, ultimately, just a technology. And technologies can often times go through a &lt;a href=&quot;https://en.wikipedia.org/wiki/Hype_cycle&quot;&gt;&#39;Hype Cycle&#39;&lt;/a&gt;, as coined by the research firm Gartner.&lt;/p&gt;

&lt;p&gt;At first, people become very interested in a brand new technology (a &quot;Technology Trigger&quot;). Companies start to over-invest in researching this technology, leading to a &quot;Peak of Inflated Expectations&quot; (i.e, a bubble). But the technology turns out to have major limitations. As a result, investment in the technology dries up (&quot;Trough of Disillusionment&quot;). Most companies either begin laying people off or closing down outright.&lt;/p&gt;

&lt;p&gt;Eventually, the survivors soon realize how to use the technology properly (&quot;Slope of Enlightenment&quot;), and we can finally use the technology in our day-to-day life (&quot;Plateau of Productivity&quot;). But as this picture from Wikipedia shows, the visibility of the technology in the Plateau of Productivity is much less than the visibility of that same technology in the Peak of Inflated Expectations. The brand new technology has done great things for us. It&#39;s just not as great as we hoped it to be. And does it justify the extreme waste seen in the &quot;Peak of Inflated Expectations&quot;?&lt;/p&gt;

&lt;p&gt;If this is some hypothetical graph, then it&#39;s not much to be worried about. But I have already lived through two tech bubbles: &lt;a href=&quot;https://en.wikipedia.org/wiki/Dot-com_bubble&quot;&gt;the dot-com bubble of 1997-2000&lt;/a&gt; and the &lt;a href=&quot;http://www.pbs.org/newshour/making-sense/unicorns-and-delusions-in-silicon-valleys-tech-bubble/&quot;&gt;current unicorn bubble&lt;/a&gt; (ending this year, at 2016). A cycle of &quot;irrational exuberance&quot; (&quot;Uber for [Plural Nouns]&quot;) followed by layoffs can be never a good thing. Especially if you have to live with the consequences. Any actual benefit caused by this overinvestment is only incidental.
&lt;br&gt;
&lt;br&gt;
I&#39;m afraid that this hype cycle can only get worse. The major reason the &lt;a href=&quot;https://en.wikipedia.org/wiki/United_States_housing_bubble&quot;&gt;American Real Estate Bubble of 2005-2007&lt;/a&gt; and the current unicorn bubble has grown as big as it did was due to a policy of &#39;low interest rates&#39; pursued by central banks. Investors are &#39;encouraged&#39; not to save money but instead to invest in risky ventures. The current interest in AI  suggest that investors may view this new technology as yet another chance to make money. These investors will probably pour way too much money into AI research (if they haven&#39;t started doing so already). And almost all of it will be exposed as wasteful in the &quot;Trough of Disillusionment&quot; stage.&lt;/p&gt;

&lt;p&gt;So why do I call the &quot;Trough of Disillusionment&quot; an &quot;AI Winter&quot;? Because I didn&#39;t come up with this name. It was invented in &lt;em&gt;1984&lt;/em&gt;. According to &lt;a href=&quot;https://en.wikipedia.org/wiki/AI_winter&quot;&gt;Wikipedia&lt;/a&gt;, the AI field had went through &lt;em&gt;two&lt;/em&gt; major AI winters (1974-1980, 1987-1993) and several &quot;smaller episodes&quot; as well.  Obviously our technology has improved. But human nature has not changed. If an AI bubble inflates, run.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Technological Dependence&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Some people believe that AI could serve as a tool that can perform routine, autonomous tasks. This frees up human to handle more creative tasks. Darrel West, the director for technology innovation at the Brookings Institution, embodies this sentiment of techno-optimism by saying:&lt;/p&gt;

&lt;blockquote&gt;&lt;blockquote&gt;&lt;p&gt;&quot;It&#39;s good that we&#39;re figuring out how to use robots to make our lives easier. There are tasks they can do very well and that free humans for more creative enterprises.&quot;&lt;/p&gt;&lt;/blockquote&gt;&lt;/blockquote&gt;

&lt;p&gt;For example, &lt;a href=&quot;http://articles.philly.com/2015-11-29/news/68626755_1_new-technology-textbook-robots&quot;&gt;robots are very good at writing 9-page textbooks&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now, I understand that some textbooks can be dry and boring. But it is hard to say that they are not &quot;creative enterprises&quot;. Yet, if you click on my above link, you will actually see Darrel West&#39;s quote, down to his very last words: &quot;more creative enterprises&quot;. Some human journalist told Darrel that a university is building a program that can write textbooks, and Darrel&#39;s only response boils down to: &quot;Oh, that&#39;s cool. More time for creativity then.&quot; (He also points to Associated Press&#39; own use of bots to write stories about sports to justify his viewpoint as well.)&lt;/p&gt;

&lt;p&gt;Does Darrel consider the act of writing &lt;em&gt;not creative&lt;/em&gt; then?&lt;/p&gt;

&lt;p&gt;Here&#39;s a dystopian idea. The term &quot;creative enterprise&quot; is a euphemism to refer to &quot;any activity that cannot be routinely automated away yet&quot;. Any task that we declare &#39;a creative expression of the human experience&#39; will be seen as &#39;dull busywork&#39; as soon as we invent a bot.  We delude themselves into thinking we are still the superior race, while slowly replacing all our usual activities with a bunch of silicon-based tools.&lt;/p&gt;

&lt;p&gt;This might be tolerable if your tools work 100% of the time. But &lt;a href=&quot;http://www.joelonsoftware.com/articles/LeakyAbstractions.html&quot;&gt;abstractions are leaky&lt;/a&gt;. If you rely on your tools too much, you open yourself up to terrible consequences if you ever lose access to the tools or your tools wind up malfunctioning. And the worst part is that your tools may fail at the very moment your (human) skills has decayed. After all, you didn&#39;t need to learn &quot;busywork&quot;. You focused all your efforts on mastering &quot;more creative enterprises&quot;.&lt;/p&gt;

&lt;p&gt;This skill decay has already happened to pilots. Thanks to the glory of automation on airplanes, &lt;a href=&quot;https://www.washingtonpost.com/local/trafficandcommuting/does-using-an-autopilot-dull-the-skills-of-us-commercial-pilots/2016/01/13/00e458fe-ba13-11e5-829c-26ffb874a18d_story.html&quot;&gt;the US Department of Transporation believe that many pilots are unable to fly airplanes in times of crises&lt;/a&gt;. When autopilot works, then all is well. When autopilot fails, then there&#39;s a real chance that the less capable human pilots make mistakes that winds up crashing the airplane.&lt;/p&gt;

&lt;p&gt;Far from AI freeing us to pursue worthier endeavours, it can only make us more dependent on technology and more vulnerable to disasters when that technology breaks. The only good news is that we can reduce our dependency. For example, the US Department of Transportation recommends that pilots should periodically fly their airplanes manually to keep their own skills fresh.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Angst&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;It is not enough to build robots to handle the tedious tasks of interviewing human beings and hiring them to do routine tasks, but instead, &lt;a href=&quot;http://www.govexec.com/excellence/promising-practices/2015/11/algorithms-make-better-hiring-decisions-humans/124035/&quot;&gt;&quot;Algorithms Make Better Hiring Decisions Than Humans&quot;&lt;/a&gt;. It is not enough to have a robot be able to cheerfully play board games and find creative strategies, but instead &lt;a href=&quot;http://www.theverge.com/2016/3/15/11213518/alphago-deepmind-go-match-5-result&quot;&gt;&quot;Google&#39;s AlphaGo AI beats Lee Se-dol again to win Go series, 4-1&quot;&lt;/a&gt;. It is not enough to give video game AI the ability to simulate emotional decision-making by keeping track of a bunch of variables and behaving differently based on those variables, but instead &lt;a href=&quot;http://time.com/3674972/mario-lives-artificial-intelligence/&quot;&gt;&quot;Researchers Make Super Mario Self-Aware&quot;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now, some people may argue that these algorithms are not examples of &quot;intelligence&quot;. The obvious conclusion must be that hiring people, beating people at Go, and playing Super Mario must also not be tasks that require intelligence.&lt;/p&gt;

&lt;p&gt;One of the problems with dealing with AI is the inherent vagueness of terms used to distinguish &quot;us&quot; (the humans) from &quot;them&quot; (the robots), leading to long and tedious arguments over whether this specific algorithm is an example of &quot;true AI&quot; without ever actually providing a decent definition of what is &quot;true AI&quot;. What  hurts matters even more is the &lt;a href=&quot;https://en.wikipedia.org/wiki/AI_effect&quot;&gt;AI Effect&lt;/a&gt;, where &lt;a href=&quot;http://www.dansdata.com/gz107.htm&quot;&gt;the goal posts are constantly shifting in response to new advances in technology&lt;/a&gt;. If you design a test to determine what is &quot;true AI&quot;, and then a machine passes the test, a new test will just get created instead.&lt;/p&gt;

&lt;blockquote&gt;&lt;blockquote&gt;&lt;p&gt;Apparently, you see, when they said &quot;a machine will never be able to spot-weld a car together&quot;, they meant to say &quot;a machine will never be &lt;em&gt;aware&lt;/em&gt; that it&#39;s welding a car together&quot;.&lt;/p&gt;&lt;/blockquote&gt;&lt;/blockquote&gt;

&lt;p&gt;Some of the goal post shifting is justified: if we build something, we know how it works, and if we know how it works, we can see that it is artificial. And yet, again, at some point, the goal post shifting starts being seen as utterly ridiculous. Please don&#39;t say the act of writing novels is not a sign of intelligence just because &lt;a href=&quot;https://github.com/dariusk/NaNoGenMo-2015&quot;&gt;NaNoGenMo&lt;/a&gt; exists. (In fact, it is very possible that as AI improves, that we may be forced to confront the possibility that &lt;a href=&quot;https://plus.google.com/100656786406473859284/posts/Yp83aFwFJEr&quot;&gt;intelligence itself may not exist&lt;/a&gt;, which seems like a far worse fate than merely accepting the existence of AI.)&lt;/p&gt;

&lt;p&gt;One way around this problem is to essentially refuse to mention the term AI at all, and instead use a more neutral term, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_learning&quot;&gt;machine learning&lt;/a&gt; or &lt;a href=&quot;https://en.wikipedia.org/wiki/Expert_system&quot;&gt;expert systems&lt;/a&gt;. &quot;Yeah that robot may not meet my arbitrary definition of intelligence, but it &lt;em&gt;is&lt;/em&gt; an expert in one specific domain area, and so I&#39;ll defer to its expertise.&quot; Yet the term of AI still continues to capture our imagination.&lt;/p&gt;

&lt;p&gt;Why?&lt;/p&gt;

&lt;p&gt;I think that our own ego is deeply invested in the idea that we are &#39;special&#39;, and that we are concerned when that &#39;specialness&#39; gets challenged. Michael Kearns, an AI researcher at the University of Pennsylvania, claimed that &quot;[p]eople subconsciously are trying to preserve for themselves some special role in the universe&quot;, &lt;a href=&quot;http://articles.philly.com/2004-01-15/news/25367871_1_new-robot-genes-yeast-cells&quot;&gt;in an article about an AI being built to conduct scientific experiments&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now Kearns doesn&#39;t care about protecting the human ego. But I do. I don&#39;t know what would happen to humanity when its self-image crumbles in the face of advanced machine capabilities. But I don&#39;t think it&#39;s something to look forward to.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Botcrime&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Consider the following quotes from the botifesto &lt;a href=&quot;http://motherboard.vice.com/read/how-to-think-about-bots&quot;&gt;&quot;How To Think About Bots&quot;&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;&lt;blockquote&gt;&lt;p&gt;The work and the legal response raise crucial questions. Who is responsible for the output and actions of bots, both ethically and legally? How does semi-autonomy create ethical constraints that limit the maker of a bot?&lt;/p&gt;

&lt;p&gt;If you make a bot, are you prepared to deal with the fallout when your tool does something that you yourself would not choose to do? How do you stem the spread of misinformation published by a bot? Automation and “big” data certainly afford innovative reporting techniques, but they also highlight a need for revamped journalistic ethics.&lt;/p&gt;

&lt;p&gt;Bots might be effective tools for guiding people toward healthier lifestyles or for spreading information about natural disasters. How can policies allow for civically “good” bots while stopping those that are repressive or manipulative?&lt;/p&gt;&lt;/blockquote&gt;&lt;/blockquote&gt;

&lt;p&gt;And so on and so forth. There are obviously legitimate fears about bots doing evil, either due to its interactions with the outside world or because it has been programmed to do evil by another entity.&lt;/p&gt;

&lt;p&gt;Raising questions about bot regulation is troubling though because they imply that these questions &lt;em&gt;must be answered&lt;/em&gt;. They do not have to be answered now, of course. But they do have to be answered fairly soon.&lt;/p&gt;

&lt;p&gt;Now only must they be answered, in the form of new government regulations, they must also be enforced. A law that is not enforced might as well not exist at all. Considering how successful we are currently in stopping preexisting spambots and social media manipulators, my hopes for effective enforcement of regulations is fairly low.&lt;/p&gt;

&lt;p&gt;What is worse is the fact that people will stand in the way of regulations. The authors (all creators of bots) strongly support regulation...except when said regulation might be used against them.&lt;/p&gt;

&lt;blockquote&gt;&lt;blockquote&gt;&lt;p&gt;Rumination on bots should also work to avoid policies or perspectives that simply blacklist all bots.  These automatons can and might be used for many positive efforts, from serving as a social scaffolding to pushing the bounds of art. We hope to provoke conversation about the design, implementation and regulation of bots in order to preserve these, and other as yet unimagined, possibilities.&lt;/p&gt;&lt;/blockquote&gt;&lt;/blockquote&gt;

&lt;p&gt;Again, a general blacklist of bots is a perfectly horrible idea (mostly because we cannot enforce it). But attempting to sort out &#39;good&#39; bots from &#39;bad&#39; bots seem like a rather dangerous and futile task. I can easily see the emergence of a &quot;pro-bot&quot; lobby that will stand against any but the most token of regulations, using doublespeak to claim that any use of technology has &quot;positive effects&quot;, while excusing away any problems the bots may cause.&lt;/p&gt;

&lt;p&gt;Alternatively, we can also see bot developers pitted against each other, decrying other people&#39;s uses of bots as being &quot;negative&quot; while championing their own use of bots as being &quot;positive&quot;. We need to have a legal system that can help evaluate whether bots are good or bad.&lt;/p&gt;

&lt;p&gt;According to Ryan Calo though, &lt;a href=&quot;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2737598&quot;&gt;the American legal system&#39;s views about robots are outdated and ill-suited to our brave new world&lt;/a&gt;. Judges generally see robots as &quot;programmable machine[s], by definition incapable of spontaneity&quot;, and ignore possible &#39;emergent properties&#39; that can exist when robots interact with society as a whole.&lt;/p&gt;

&lt;p&gt;Ryan Calo support the creation of &quot;a new technology commission, a kind of NASA-for-everything that can act as a repository of knowledge about robots to guide legal actors, including courts&quot;...but this commission seems very similar to that of an interest group, and one that may only have the interests of robot developers at heart, not that of society.&lt;/p&gt;

&lt;p&gt;It will take time to come up with the right balance between bot freedom and bot banning, if there really is any.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Conclusion&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In 2016, &lt;a href=&quot;http://www.evansdata.com/press/viewRelease.php?pressID=231&quot;&gt;Evans Data conducted a survey of 500 software engineers&lt;/a&gt; to find out what they feared the most. The largest plurality (29.1%) said that they feared AI taking their jobs. What&#39;s worse is that over 60% of software engineers thought that AI &quot;would be a disaster&quot;.&lt;/p&gt;

&lt;p&gt;This does not mean that these software engineers are Luddites. &quot;[O]ver three-quarters of the developers thought that robots and artificial intelligence would be a great benefit to mankind&quot;, claimed Janel Garvin (the CEO of Evans Data). &quot;Overlap between two groups was clear which shows the ambivalence that developers feel about the dawn of intelligent machines. There will be wonderful benefits, but there will also be some cataclysmic changes culturally and economically.&quot;&lt;/p&gt;

&lt;p&gt;There has been a lot of coverage about the rise of AI and its &#39;wonderful benefits&#39;. The goal of this post is to illustrate the &#39;cataclysmic changes&#39; and thereby make a implicit argument against AI.&lt;/p&gt;

&lt;p&gt;The dangers of AI are great, and we should not let the potentials of AI blind us to real risks. There are &lt;a href=&quot;tra38.github.io/blog/c11-ai2.html&quot;&gt;some solutions to help manage AI risk&lt;/a&gt; that I proposed in the past, but probably the most practical and sensible solution at the moment is to slow down AI development and think through these risks carefully. By slowing down progress, we can ensure that the changes won&#39;t be so &quot;cataclysmic&quot;, and that humanity can survive intact.&lt;/p&gt;
&lt;p&gt;Thank you for reading this blog post written by Tariq Ali, an aspiring web developer. If you are interested in reading more blog posts, please go to tra38.github.io/blog today.&lt;/p&gt;</content>
        <pubDate>Fri, 11 Mar 2016 00:00:00 -0600</pubDate>
        <link>tra38.github.io/blog/ai3.html</link>
        <guid isPermaLink="true">tra38.github.io/blog/ai3.html</guid>
        
        
      </item>
    
      <item>
        <title>C22 Sartre</title>
        <description>![CDATA[A few days ago, I read a blog post titled "Write code that was easy to delete, not easy to extend". At the top of this blog post was a quote...
]</description>
        <content>&lt;p&gt;A few days ago, I read a blog post titled &lt;a href=&quot;http://programmingisterrible.com/post/139222674273/write-code-that-is-easy-to-delete-not-easy-to&quot;&gt;&quot;Write code that was easy to delete, not easy to extend&quot;&lt;/a&gt;. At the top of this blog post was a quote...&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“Every line of code is written without reason, maintained out of weakness, and deleted by chance.”&lt;/em&gt; Jean-Paul Sartre’s Programming in ANSI C[1]&lt;/p&gt;

&lt;p&gt;I was so fasinciated by this profound quote that I didn&#39;t even bother reading the rest of the blog post and decided to find this obscure book and read it myself.&lt;/p&gt;

&lt;p&gt;I don&#39;t agree with all of what Sartre has written; his ideas about individualism reads a bit too utopian for my tastes. However, his ideas are still interesting to read. Here are a few choice quotes from the book.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;“Ah! How I hate the crimes of the new generation: they are dry and sterile as Assembly.”&lt;/li&gt;
&lt;li&gt;“A programmer is condemned to be free; because once thrown into the codebase, he is responsible for everything he does.”&lt;/li&gt;
&lt;li&gt;“Intellectuals cannot be good revolutionaries; they are just good enough to be programmers.”&lt;/li&gt;
&lt;li&gt;“If you want to deserve Hell, you need only stay in bed. The codebase is iniquity; if you accept it, you are an accomplice, if you change it you are an executioner.”&lt;/li&gt;
&lt;li&gt;“You must be afraid, my son. That is how one becomes an honest programmer.”&lt;/li&gt;
&lt;li&gt;“The more one is absorbed in fighting bad programming, the less one is tempted to write good programs.”&lt;/li&gt;
&lt;li&gt;“Do you think you can program innocently?”&lt;/li&gt;
&lt;li&gt;“As far as programmers go, it is not what they are that interests me, but what they can become.”&lt;/li&gt;
&lt;li&gt;“A programmer cannot will unless he has first understood that he must count on no one but himself; that he is alone, abandoned in the codebase in the midst of his infinite responsibilities, without help, with no other aim than the one he sets himself, with no other destiny than the one he forges for himself in his IDE.”&lt;/li&gt;
&lt;li&gt;“I do not give a damn about the previous programmers. They died for the codebase and the codebase can decide what it wants. I practice a live man’s politics, for the living.”&lt;/li&gt;
&lt;li&gt;“We programmers exist, that is all, and I find it nauseating.”&lt;/li&gt;
&lt;li&gt;“How can I, who was not able to retain my own past knowledge of the codebase, hope to teach another?”&lt;/li&gt;
&lt;li&gt;“I embraced Agile because its cause was just and I will leave Agile when it ceases to be just.”&lt;/li&gt;
&lt;li&gt;“Ah! Do not judge the gurus, young man, for they too have painful secrets.”&lt;/li&gt;
&lt;li&gt;“If a victory is told in detail, one can no longer distinguish it from a defeat.”&lt;/li&gt;
&lt;li&gt;“I respect orders but I respect myself too and I do not obey foolish design patterns made especially to humiliate me.”&lt;/li&gt;
&lt;li&gt;“What do you want to do with the language? Write Fizz-Buzz? What good is it to sharpen a knife every day if you never use it for slicing? Code is never more than a means. There is only one objective: power.”&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;[1]&quot;Jean-Paul Sartre’s Programming in ANSI C&quot; is a fictional book. The philosopher himself is real, as are the quotes, but almost all of them were modified to take into account the subject matter of programming.&lt;/p&gt;
&lt;p&gt;Thank you for reading this blog post written by Tariq Ali, an aspiring web developer. If you are interested in reading more blog posts, please go to tra38.github.io/blog today.&lt;/p&gt;</content>
        <pubDate>Wed, 17 Feb 2016 00:00:00 -0600</pubDate>
        <link>tra38.github.io/blog/c22-sartre.html</link>
        <guid isPermaLink="true">tra38.github.io/blog/c22-sartre.html</guid>
        
        
      </item>
    
      <item>
        <title>T14 Abstract</title>
        <description>![CDATA[Yesterday, while reading an old computer science textbook "Programming and Problem Solving with C++", I saw a chapter on "Abstract Data Types" (ADTs) and was instantly curious. So I read the chapter, hoping to learn about this strange and mysterious things. I was surprised to find out that ADTs are not strange and mysterious at all. In fact, I have been writing them in my programming career.
]</description>
        <content>&lt;p&gt;Yesterday, while reading an old computer science textbook &quot;Programming and Problem Solving with C++&quot;, I saw a chapter on &quot;Abstract Data Types&quot; (ADTs) and was instantly curious. So I read the chapter, hoping to learn about this strange and mysterious things. I was surprised to find out that ADTs are not strange and mysterious at all. In fact, I have been writing them in my programming career.&lt;/p&gt;

&lt;p&gt;An ADT is any entity that contains data and specified behavior (methods/functions). For example, let us define a simple Bank Account ADT. This is a &quot;specification&quot; (a detailed list of what we want our ADT to do):&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;We should store how much money is in the account.&lt;/li&gt;
&lt;li&gt;We can withdraw money from this account.&lt;/li&gt;
&lt;li&gt;We can deposit money from this account.&lt;/li&gt;
&lt;li&gt;We know how much money is in the account.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;And here&#39;s an example of the Bank Account ADT, coded in Ruby...&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
  class BankAccount
    attr_reader :current_funds&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def initialize(amount)
  @current_funds = amount
end

def withdraw(amount)
  @current_funds -= amount
end

def deposit(amount)
  @current_funds += amount
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;  end
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;That&#39;s it.&lt;/p&gt;

&lt;p&gt;Now, you may not like how I wrote this ADT. Perhaps you prefer not using classes and may wish to use a more &quot;functional&quot; approach to programming. Or maybe you just want to write this ADT in a different language.&lt;/p&gt;

&lt;p&gt;That&#39;s perfectly fine. Code the way that you wish. As long as you follow the specification listed above, the ADT is still valid. Your version of BankAccount is just as valid as mine. The implementation of an ADT is seperate from the specification of an ADT. That&#39;s why it is possible to refactor your code...so long as your code behaves the same way before and after the refactor.&lt;/p&gt;

&lt;p&gt;Now, the mind-blowing part is that you can build ADTs...&lt;strong&gt;using&lt;/strong&gt; ADTs. Suppose I want to create a Bank ADT that can store Bank Account ADTs, and I want to know how much money the Bank ADT has.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
  class Bank&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def initialize(array_of_bank_accounts)
  @accounts = array_of_bank_accounts
end

def total_funds
  sum = 0
  @accounts.each do |account|
    sum += account.current_funds
  end
  sum
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;  end
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&quot;Programming and Problem Solving with C++&quot; recommends the use of ADTs because it makes programs simpler and easier to understand, thereby making code easier to write. The book uses an example of a car to illustrate its point:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;&quot;With abstraction, we focus on the &lt;em&gt;what&lt;/em&gt;, not the &lt;em&gt;how&lt;/em&gt;. For example, our understanding of automobiles is largely based on abstraction. Most of us know &lt;em&gt;what&lt;/em&gt; the engine does (it propels the car) but fewer of us know-or wnt to know-precisely &lt;em&gt;how&lt;/em&gt; the engine works internally. Abstraction allows us to discuss, think about, and use automobiles without having to know everything about how they work.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;And programming makes use of abstractions heavily, with some languages being entirely built on other languages. Ruby, for example, is built on C. Abstraction serves as a great way to manage complex code bases...and ADTs are the living paragons of abstraction.&lt;/p&gt;

&lt;p&gt;But &lt;a href=&quot;http://www.joelonsoftware.com/articles/LeakyAbstractions.html&quot;&gt;abstractions are &#39;leaky&#39;&lt;/a&gt;. What if there is some bug in my BankAccount ADT? That bug would harm my Bank ADT as well...and could even harm any ADTs that were built on top of my Bank ADT.&lt;/p&gt;

&lt;p&gt;So while ADTs are useful, you cannot be totally dependent on them. You have to be willing to look at the internal code of an ADT you&#39;re using and be able to debug it.&lt;/p&gt;

&lt;p&gt;It is not enough to know how to program using abstractions. You must also know how to program the abstractions &lt;em&gt;themselves&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Thank you for reading this blog post written by Tariq Ali, an aspiring web developer. If you are interested in reading more blog posts, please go to tra38.github.io/blog today.&lt;/p&gt;</content>
        <pubDate>Fri, 12 Feb 2016 00:00:00 -0600</pubDate>
        <link>tra38.github.io/blog/t14-abstract.html</link>
        <guid isPermaLink="true">tra38.github.io/blog/t14-abstract.html</guid>
        
        
      </item>
    
      <item>
        <title>C21 Robot Rights</title>
        <description>![CDATA[As technology advances, so too does the capabilities of robots. As a result, some philosophers wondered whether robots may one day acquire 'rights' equal to that of their human brethren.
]</description>
        <content>&lt;p&gt;As technology advances, so too does the capabilities of robots. As a result, some philosophers wondered whether robots may one day acquire &#39;rights&#39; equal to that of their human brethren.&lt;/p&gt;

&lt;p&gt;In 2015, Time Maganize asked the question: &quot;Will Robots Need Rights?&quot; It also posted the answers of three people:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://time.com/4026684/david-gelernter-will-robots-need-rights/&quot;&gt;David Gelernter&lt;/a&gt; claim that robots are unlikely to gain rights because they lack consciousness. However, people should still treat robots humanely.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://time.com/4023496/ray-kurzweil-will-robots-need-rights/&quot;&gt;Ray Kurzweil&lt;/a&gt; states that consciousness is not scientifically testable, and thus would have to defined through philosophical arguments. Ray argued that robots will have rights once they are able to convince us that they deserves rights.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;(http://time.com/4023497/susan-n-herman-will-robots-need-rights/&quot;&gt;Susan N. Herman&lt;/a&gt;, President of the ACLU, suggest that the ACLU might defend robot rights if robots share the same sensibilities as human beings, but would first have to argue deeply over what types of rights robots should get.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;All three answers show a sympathetic outlook towards robots, treating the concept of &quot;rights&quot; as something to seriously consider as we deal with potential alien-sque entities. And while I support respecting robots as &quot;potential alien-sque entities&quot; instead of treating them as &quot;just tools&quot; to be used and abused, I believe that the case for robotic rights has been vastly overstated (and you don&#39;t even need to bring in the nebulous idea of consciousness into the discussion).&lt;/p&gt;

&lt;p&gt;In this blog post, I will lay out a hardline case against robotic rights. It&#39;s not a case I fully belive in myself, but it&#39;s a case that I believe that most people will end up using. Note that my argument is only against robot rights; it is silent on the question on whether humans have rights.&lt;/p&gt;

&lt;h3&gt;Premise&lt;/h3&gt;

&lt;p&gt;I rest my case on the premise that rights can only belong to entities that are able to exercise some control over their own actions and motives. I call these entities &quot;autonomous actors&quot;, to avoid any confusion with the term &lt;a href=&quot;https://en.wikipedia.org/wiki/Autonomous_agent&quot;&gt;&quot;autonomous agent&quot;&lt;/a&gt;.&lt;/p&gt;

&lt;h3&gt;The Lovelace Test&lt;/h3&gt;

&lt;p&gt;The original Lovelace Test, outlined in &lt;a href=&quot;http://kryten.mm.rpi.edu/lovelace.pdf&quot;&gt;&quot;Creativity, the Turing Test, and the (Better) Lovelace Test&quot;&lt;/a&gt;, was meant as a way to determine whether programs are intelligent. A program is intelligent if and only if it did all these things:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The program must be able to design something &#39;original&#39; and &#39;creative&#39; (such as a story, music, idea, or even another computer program).&lt;/li&gt;
&lt;li&gt;The program itself is a result of processes that can be &#39;reproduced&#39;. (In other words, it does not rely on some bug in the &#39;hardware&#39; that the program is running on.)&lt;/li&gt;
&lt;li&gt;The program&#39;s creative output must not be a result of bugs within the program itself.&lt;/li&gt;
&lt;li&gt;The programmer must not know how the program actually works.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Criteria #4 is the kicker though. It is very possible for programs to design &#39;original&#39; and &#39;creative&#39; work. After all, &lt;a href=&quot;http://tra38.github.io/blog/c20-robojournalism-2.html&quot;&gt;robots are already writing fiction and nonfiction stories&lt;/a&gt;. But the programmers themselves &lt;em&gt;know&lt;/em&gt; how the robots are able to produce creative outputs. Therefore, all of these programs fail the original Lovelace Test.&lt;/p&gt;

&lt;p&gt;Even an algorithm that uses &quot;machine learning&quot; is unable to pass Criteria #4, as it can be argued that the programmers are able to control the robot by controlling the dataset the robot uses to understand the world. Thus, you can explain the robot&#39;s creative output by deducing the dataset that it used.&lt;/p&gt;

&lt;p&gt;In fact, the whole point of this original Lovelace Test is to argue against the idea that robots could ever be intelligent, by arguing that robots ultimately need programmers. Mark O. Riedl &lt;a href=&quot;http://arxiv.org/pdf/1410.6142v3.pdf&quot;&gt;wrote&lt;/a&gt; that &quot;any [programmer] with resources to build [the program] in the first place ... also has the ability to explain [how the program generates the creative output]&quot; [1].&lt;/p&gt;

&lt;p&gt;I disagree with the original Lovelace Test, because it claims that intelligence is a trait that either exists or doesn&#39;t exist. I prefer to think of intelligence either in terms of a continuum or through the idea of multiple intelligences.&lt;/p&gt;

&lt;p&gt;But I think the original version of the Lovelace Test is useful when thinking about robotic rights. Robots do what programmers tell them to do. They do not have any independent &#39;will&#39; of their own. Robots are essentially puppets. It&#39;s hard to consider them autonomous actors, because there is always someone behind them (either a human or a dataset) pulling the strings. And we can see those strings.&lt;/p&gt;

&lt;h3&gt;The Parable of the Paperclip Maximizer&lt;/h3&gt;

&lt;p&gt;That doesn&#39;t mean programmers has &lt;em&gt;total control&lt;/em&gt; over these robots. Sloppy code, bad planning and codebase complexity can lead to unexpected outcomes.&lt;/p&gt;

&lt;p&gt;Given enough time and patience, a programmer should be able to figure out why his code lead to the outcome that it did. If a problem is big enough though, practically speaking, many programmers will not have the time or patience. Shortcuts are taken. Rationalization sets in. Ignorance is accepted as best practice.&lt;/p&gt;

&lt;p&gt;For example, I can easily imagine a lazy programmer write the following code before going to bed...&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Produce as many paperclips as physically possible.&lt;/li&gt;
&lt;li&gt;Increase your processing power so that you can more effectively produce paperclips.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;...and thereby destroy humanity because his &lt;a href=&quot;https://wiki.lesswrong.com/wiki/Paperclip_maximizer&quot;&gt;&#39;Paperclip Maximizer&#39;&lt;/a&gt; has decided to turn the entire solar system into paperclips and computer mainframes.&lt;/p&gt;

&lt;p&gt;But this is not an &quot;artificial intelligence&quot; problem. It&#39;s a &quot;human stupidity&quot; problem. The programmer wanted to produce paperclips and did not think through the consequences of his code. The &#39;Paperclip Maximizer&#39; simply followed orders.&lt;/p&gt;

&lt;p&gt;The programmer, if he did think through his code carefully, would have likely noticed problems. But, of course, he had other priorities. He had to go to sleep so that he can be well-rested the next day to watch Netflix movies and contribute to open source projects. And his code was so elegant, and it passed all his automated tests. He had nothing to worry about.&lt;/p&gt;

&lt;p&gt;So he goes to sleep and never wakes up.&lt;/p&gt;

&lt;h3&gt;Robots Follow Orders, Only We May Not Know What Those Orders Mean&lt;/h3&gt;

&lt;p&gt;A programmer may not know exactly why a program is doing what it is doing...but he has the &lt;em&gt;theoretical capability&lt;/em&gt; to find out for himself (since, you know, the programmer wrote the program). And he should at least attempt to do that, so that you can reduce the chances of scenarios such as the above parable.&lt;/p&gt;

&lt;p&gt;But what if a programmer is unable (or unwilling) to do this? Does the robot deserves rights then?&lt;/p&gt;

&lt;p&gt;No. The programmer had the capability to understand what a robot is doing. He just decided not to use it. But the fact that the programmer could have found out suggest that the robot is not an autonomous actor.&lt;/p&gt;

&lt;p&gt;The robot simply follow orders...only in this specific case, they are orders that we do not quite fully understand ourselves.&lt;/p&gt;

&lt;p&gt;For debugging purposes, we should hire another programmer who will be more willing to figure out why the robot is acting the way it is. After all, if one human has the theoretical capability to find out what a robot is doing...then it is likely that another human will eventually gain that same theoretical capability.&lt;/p&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;If we want robots to actually think for themselves (instead of just being puppets of datasets and human programmers), we have to turn robots into autonomous actors. As the original Lovelace Test suggest, this is an impossible goal. If we are able to write a program, then we should be able to also know how that program works. There is no autonomy to be found anywhere.&lt;/p&gt;

&lt;p&gt;If robots can never be free, then they can never deserve rights.&lt;/p&gt;

&lt;h2&gt;Footnotes&lt;/h2&gt;

&lt;p&gt;[1] Incidentally, Mark O. Riedl proposed a less strict version of the Lovelace Test, the &lt;a href=&quot;http://arxiv.org/pdf/1410.6142v3.pdf&quot;&gt;Lovelace 2.0 Test&lt;/a&gt;, as a test that can actually be beaten. Instead of mandating that the programmer remain ignorant of the inner workings of his program, the program&#39;s creative output must meet certain constraints as determined by an independent judge.&lt;/p&gt;
&lt;p&gt;Thank you for reading this blog post written by Tariq Ali, an aspiring web developer. If you are interested in reading more blog posts, please go to tra38.github.io/blog today.&lt;/p&gt;</content>
        <pubDate>Tue, 02 Feb 2016 00:00:00 -0600</pubDate>
        <link>tra38.github.io/blog/c21-robot-rights.html</link>
        <guid isPermaLink="true">tra38.github.io/blog/c21-robot-rights.html</guid>
        
        
      </item>
    
      <item>
        <title>C20 Robojournalism 2</title>
        <description>![CDATA[Though we are able to teach robots how to write as well as a human, we may have difficulty teaching them how to view the world as a human.
]</description>
        <content>&lt;p&gt;Though we are able to teach robots how to write as well as a human, we may have difficulty teaching them how to view the world as a human.&lt;/p&gt;

&lt;p&gt;Computers are learning how to write. It&#39;s not considered weird or bizarre anymore to see companies like &lt;a href=&quot;http://narrativescience.com&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;Narrative Science&lt;/a&gt; and &lt;a href=&quot;https://automatedinsights.com&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;Automated Insights&lt;/a&gt; be able to generate news reports covering topics as broad as sports and finance.  Automated Insights have also made their &quot;Wordsmith&quot; platform publicly available, meaning that anyone have the potential to command their own robotic writers, without any programming knowledge.&lt;/p&gt;

&lt;p&gt;Human journalists have been able to accept their robo-journalistic brethren under the mistaken impression that robo-journalists will be regulated to writing &quot;quantitative&quot; articles while humans will have more time to write &quot;qualitative&quot; analyses. But you can indeed turn human experiences into quantitative data that a robot can then write. For example, &quot;sentiment analysis&quot; algorithms are able to determine whether a certain article is happy or sad, based on analyzing what words were used. The output would be a qualitative judgment, based solely on quantitative data. Narrative Science has already explored the possibility of moving onto &quot;qualitative&quot; analyses by creating &quot;Quill Connect&quot;, a program that is able to write &lt;a href=&quot;https://quillconnect.narrativescience.com&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;qualitative analyses of Twitter profiles&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Algorithms are not limited to writing nonfiction. Every November (starting from 2013), programmers participate in a competition called &lt;a href=&quot;https://github.com/dariusk/NaNoGenMo-2015&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;National Novel Generation Month&lt;/a&gt;; the goal is to write a fictional novel of 50,000 words or more. Some of these generated novels are generally dull, but readable (examples: &quot;&lt;a href=&quot;https://github.com/dariusk/NaNoGenMo-2015/issues/142&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;Around the World in X Wikipedia Articles&lt;/a&gt;&quot;, &quot;&lt;a href=&quot;https://github.com/dariusk/NaNoGenMo-2015/issues/11&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;A Time of Destiny&lt;/a&gt;&quot;, &quot;&lt;a href=&quot;https://github.com/dariusk/NaNoGenMo-2015/issues/40&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;Simulationist Fantasy Novel&lt;/a&gt;&quot;). They have the plot, characterization, and imagery that you would normally associate with a human-written work. Programmers will have to put in more effort for computer-generated novels to be on-par with human-produced literature, but there does not seem to be any inherent limit to algorithmic creativity.&lt;/p&gt;

&lt;p&gt;Of course, one could argue that robots will never be able to replace humans at all. Robots are reliant on &quot;templates&quot; to help them organize their stories properly, and humans are the ones in charge of designing the templates that the robots will end up using to write. In this viewpoint, humans would willingly give up the ability to write since they can find it a much more rewarding task to simply instruct the computer how to write a certain story.&lt;/p&gt;

&lt;p&gt;But I would argue that even if humans would want to outsource all writing to their robotic slaves, humans will still write out some of their ideas out by hand...because of an inherent limitations of bots. Bots lack the &quot;worldview&quot; of humans.&lt;/p&gt;

&lt;p&gt;Humans take for granted their ability to perceive the world. Their five senses gives a continual stream of data that humans are able to quickly process. Bots, on other hand, are only limited to the &quot;raw data&quot; that we give them to process. They will not &quot;see&quot; anything that is not in the dataset. As a result, how the bots understand our world will be very foreign to our own (human) understanding.&lt;/p&gt;

&lt;p&gt;For some people, this is actually a benefit that bots bring to writing. Bots will not have the same biases as human beings. They will therefore discover new insights and meanings that humans may have overlooked. However, bots will instead bring their own unique &#39;biases&#39; and issues into their work, and humans may not tolerate the biases of algorithms as much as they would tolerate the biases of other humans. Humans will, of course, still happily read what the bots have to say. But they also want to read what humans have to say too.&lt;/p&gt;

&lt;p&gt;Humans will likely tolerate the rise of automation in literature, and accept it. Bots may even write the majority of all literature by 2100. But there will still be some marginal demand for human writers, simply because humans can relate more to the &quot;worldview&quot; of other humans. These human writers must learn how to coexist with their robotic brethren though.&lt;/p&gt;

&lt;p&gt;This article was originally published by me on LinkedIn as &lt;a href=&quot;https://www.linkedin.com/pulse/why-robots-fully-replace-human-writers-tariq-ali?trk=pulse_spock-articles&quot; rel=&quot;no-follow&quot;&gt;&quot;Why Robots Will Not (Fully) Replace Human Writers&quot;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Thank you for reading this blog post written by Tariq Ali, an aspiring web developer. If you are interested in reading more blog posts, please go to tra38.github.io/blog today.&lt;/p&gt;</content>
        <pubDate>Thu, 07 Jan 2016 00:00:00 -0600</pubDate>
        <link>tra38.github.io/blog/c20-robojournalism-2.html</link>
        <guid isPermaLink="true">tra38.github.io/blog/c20-robojournalism-2.html</guid>
        
        
      </item>
    
      <item>
        <title>T13 Sql Ordinals And Aliases</title>
        <description>![CDATA[Writing SQL can be a painful experience. But there are two shortcuts that can be used to make your code easier to type. Both shortcuts are cool to understand, but only one is actually advised by the broader "SQL community".
]</description>
        <content>&lt;p&gt;Writing SQL can be a painful experience. But there are two shortcuts that can be used to make your code easier to type. Both shortcuts are cool to understand, but only one is actually advised by the broader &quot;SQL community&quot;.&lt;/p&gt;

&lt;p&gt;I was inspired to write this blog post while completing Codecademy&#39;s tutorial &quot;SQL: Analyzing Business Metrics&quot;. In one exercise in the tutorial, I had to write an SQL query to find out how many people are playing a game per day. This was the query I wrote:&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-psql&quot; data-lang=&quot;psql&quot;&gt;SELECT date(created_at), count(DISTINCT user_id)
from gameplays
GROUP BY date(created_at)
ORDER BY date(created_at);&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;And this is the resulting table...&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;date(created_at)  count(distinct user_id)
2015-08-04          99
2015-08-05          117
2015-08-06          106
...&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;But the SQL query is too verbose and unclear. Could there be a simpler way to refer back to &#39;date(created_at)&#39;? Codecademy suggest using &quot;ordinals&quot;. Ordinals are numbers that are used to refer to the columns you are &#39;selecting&#39; in an SQL query.&lt;/p&gt;

&lt;p&gt;Here&#39;s an easy diagram to determine the ordinal number of a SELECT query...&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-psql&quot; data-lang=&quot;psql&quot;&gt;SELECT date(created_at), count(DISTINCT user_id)
       ^^^^              ^^^
       1                 2&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;By using ordinals, we can simplify our original SQL query as follows:&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-psql&quot; data-lang=&quot;psql&quot;&gt;SELECT date(created_at), count(DISTINCT user_id)
FROM gameplays
GROUP BY 1
ORDER BY 1;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;This approach &lt;em&gt;seems&lt;/em&gt; less verbose, but it is actually much more unclear. At first glance, it is impossible to know what &quot;1&quot; is supposed to mean. And if someone decides to switch the order of the SELECT query (putting &quot;count(DISTINCT user_id)&quot; first), the resulting table will be messed up.&lt;/p&gt;

&lt;p&gt;There has to be a better way.&lt;/p&gt;

&lt;p&gt;And there is. SQL Aliases. Aliases work the same as variables in other programming languages, and to define an alias in SQL, you simply write &quot;AS [alias_name]&quot;. Aliases ensure that your SQL code will be self-documenting, while also ensuring that your code would be less verbose.&lt;/p&gt;

&lt;p&gt;Here&#39;s an example of me defining aliases in an SELECT query, and then using them later on:&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-psql&quot; data-lang=&quot;psql&quot;&gt;SELECT date(created_at) AS time, count(DISTINCT user_id) AS daily_users
FROM gameplays
GROUP BY time
ORDER BY time;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;An interesting side note is that by defining variables in the SELECT query, you also change the name of the columns in the resulting table...&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;time              daily_users
2015-08-04          99
2015-08-05          117
2015-08-06          106
...&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;The main reason Codecademy teaches the use of ordinals is because it was traditionally used during the early days of SQL programming. Thus, knowing ordinals will allow you to understand and debug any legacy SQL queries you encounter.&lt;/p&gt;

&lt;p&gt;However, the broader SQL community strongly discourages the use of ordinals because of the confusion and problems that they may cause. Instead, the SQL community suggest using aliases to make your code clearer and easier to understand. Following these recommendations would make sure that when you do deal with SQL, your pain would be minimal.&lt;/p&gt;
&lt;p&gt;Thank you for reading this blog post written by Tariq Ali, an aspiring web developer. If you are interested in reading more blog posts, please go to tra38.github.io/blog today.&lt;/p&gt;</content>
        <pubDate>Thu, 31 Dec 2015 00:00:00 -0600</pubDate>
        <link>tra38.github.io/blog/t13-sql-ordinals-and-aliases.html</link>
        <guid isPermaLink="true">tra38.github.io/blog/t13-sql-ordinals-and-aliases.html</guid>
        
        
      </item>
    
  </channel>
</rss>