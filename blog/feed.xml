<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tariq Ali&#39;s Blog</title>
    <description>A blog dedicated to showcasing my talents</description>
    <link>tra38.github.io/blog/</link>
    <atom:link href="tra38.github.io/blog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 21 Apr 2016 20:46:18 -0500</pubDate>
    <lastBuildDate>Thu, 21 Apr 2016 20:46:18 -0500</lastBuildDate>
    <generator>Jekyll v3.0.1</generator>
    
      <item>
        <title>T15 Architect</title>
        <description>![CDATA[Can humans build a story-telling machine that can trick other human beings (a la the 'Turing Test')? Dartmouth College's Neukom Institute for Computational Science is hosting a competition ("DigiLit") to see if that can be done. I submitted a program (Architect) to this program. I think it has a good shot of winning (~40%).
]</description>
        <content>&lt;p&gt;Can humans build a story-telling machine that can trick other human beings (a la the &#39;Turing Test&#39;)? Dartmouth College&#39;s Neukom Institute for Computational Science is hosting a competition (&quot;DigiLit&quot;) to see if that can be done. I submitted a program (Architect) to this program. I think it has a good shot of winning (~40%).&lt;/p&gt;

&lt;p&gt;The idea behind &lt;a href=&quot;http://bregman.dartmouth.edu/turingtests/digilit&quot;&gt;DigiLit&lt;/a&gt; is simple. You build a machine that can accept an arbitrary noun prompt (&quot;wedding&quot;, &quot;sorrow&quot;, &quot;car keys&quot;, etc.). The machine then uses this noun prompt to generate a short story that is 7000 words or less. Then this short story is then presented to two panels (each with 3 judges). If you can convince a majority of one panel that your story is human-generated, then you win.&lt;/p&gt;

&lt;p&gt;This does seem like a difficult task. You must:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Have the computer accept arbitrary input&lt;/li&gt;
&lt;li&gt;Generate a story using the input&lt;/li&gt;
&lt;li&gt;Make sure the story is coherent enough to be readable&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Yet, I think I completed this task. After I finished writing my program (&lt;strong&gt;Architect&lt;/strong&gt;, named after the program in the Matrix trilogy), I decided to test it out. I chose a random noun prompt, and wrote a story based on it. I also had my program generate a story as well (using that same noun prompt). I then gave those two stories to 7 people. Story A was written by me, and Story B was written by the program.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;3 people correctly identified Story A as being written by a human.&lt;/li&gt;
&lt;li&gt;3 people wrongly identified Story B as being written by a human.&lt;/li&gt;
&lt;li&gt;1 person admitted he didn&#39;t know.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This means that 43% of the humans were fooled. That&#39;s pretty good. And it&#39;s good enough for me to submit it over to DigiLit.&lt;/p&gt;

&lt;p&gt;That doesn&#39;t mean that victory is assured, after all. 43% isn&#39;t exactly a majority. But consider that I really need to convince 2 out of 6 judges to have a good chance of &#39;convincing&#39; a whole panel of 3 judges (assuming that both judges sit on the same panel). That means I only need to convince 33% of all judges.&lt;/p&gt;

&lt;p&gt;To be 100% certain of convincing a panel though, I would need to convince 3 out of 6 judges (so that no matter how the judges are distributed, at least a majority of those I convinced would sit on a single panel). That does require me to be able to convince 50% of all judges. That doesn&#39;t seem too likely.&lt;/p&gt;

&lt;p&gt;One main problem with my study is that the format of my test and the DigiLit test is slightly different. In my test, each person is reading both stories individually and then making a choice. In DigiLit, the judges sit on a panel, have access to multiple stories (some human-generated and some machine-generated) so they can detect patterns and &#39;tells&#39;, and can talk to one another and share their insights. The mob will likely find more faults in a story than a single human can. So my study may likely overestimate how good my program actually is.&lt;/p&gt;

&lt;p&gt;But enough boring stats. Here&#39;s &lt;a href=&quot;https://github.com/tra38/Architect&quot;&gt;the source code&lt;/a&gt; of Architect and a brief description of how it works:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
  Have the computer accept input:
  &lt;ul&gt;
  &lt;li&gt;Plug the input into a madlib template: {INPUT} {LOCATION_TYPE}. For example:
    &lt;ul&gt;
      &lt;li&gt;Sorrow Academy.&lt;/li&gt;
      &lt;li&gt;Wedding Plaza.&lt;/li&gt;
      &lt;li&gt;Car Keys Muesum&lt;/li&gt;
    &lt;/ul&gt;
  The story will therefore be about a generated LOCATION, and not directly about the noun prompt. Here, I&#39;m assuming that reader will assume that when the program is writing about &quot;Sorrow Academy&quot;, they assume it to be symbolic of the actual noun in question (&#39;sorrow&#39;).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;br&gt;
&lt;li&gt;Generate a story using that input:
  &lt;br&gt;
  &lt;ul&gt;
    &lt;li&gt;Randomly pick an introductory sentence from a corpus of introductory sentences.&lt;/li&gt;
    &lt;li&gt;Load the program with several pre-written passages.&lt;/li&gt;
    &lt;li&gt;Randomly pick a few of those passages. Insert in the LOCATION into the passages (so that the passages would therefore be about the input in question).&lt;/li&gt;
    &lt;li&gt;Between each passage, insert a &quot;transition passage&quot; to link the two passages together (providing an illusion of continuity between the passages). The &quot;transition passage&quot; was also pre-written.&lt;/li&gt;
    &lt;li&gt;Randomly pick an conclusion sentence from a corpus of conclusion sentences.&lt;/li&gt;
  &lt;/ul&gt;
This isn&#39;t a story with any planned plot or coherence. The program has no idea what it&#39;s writing. But since there is an illusion of continuity between passages, humans are able to read a &#39;narrative&#39;.
&lt;/li&gt;
&lt;br&gt;
&lt;li&gt;
  Have the story be thematically coherent enough to be readable.
  &lt;br&gt;
  &lt;ul&gt;
    &lt;li&gt;Each sentence of the story must have certain repeated symbols, themes, and characters within them, so that readers are more likely to assume that there is some purpose behind the words. (This approach was originally used in the &lt;a href=&quot;https://web.archive.org/web/20061112014356/http://www.brown.edu/Courses/FR0133/Fairytale_Generator/gen.html&quot;&gt;Fairy Tale Generator&lt;/a&gt; to create interesting stories based on random paragraphs.)&lt;/li&gt;
    &lt;li&gt;Since I&#39;m too lazy to hand-craft most of these sentences myself, I decided to use a preexisting generator (Abulafia&#39;s &lt;a href=&quot;http://www.random-generator.com/index.php?title=Film_Noir_Monologue&quot;&gt;Film Noir Monologue&lt;/a&gt; generator) to generate most of the paragraphs, introductory sentences, and conclusion sentences. Since this generator already had a prebuilt theme in mind (a cynical detective trying to solve a mysterious case), the resulting output does have some sort of thematic coherence. I then edited most of the paragraphs and sentences to provide even more thematic coherence.&lt;/li&gt;
    &lt;li&gt;The &#39;transition passages&#39;, however, were handwritten by me. Again, their goal is to provide thematic coherence and to &quot;fit&quot; with the rest of the passages.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;This was the computer-generated story that I presented to my readers during the test:&lt;/p&gt;

&lt;blockquote&gt;&lt;blockquote&gt;&lt;p&gt;It was a dark day—maybe normal for this time of year, but today the big city felt even darker and more sinister.&lt;/p&gt;

&lt;p&gt;Miss Kitty Rider prowled through my door like a tigress slinks into a Burmese orphanage—a pinup blonde with legs as far as you could want ’em. No dame her age could afford a coat like that, my money was right where my mind was: the gutter. She was to bad news what apple pie is to America. The dame was all business—before I could even close the office door, she told me she wants Investigator Blake dead. Turns out Blake is masterminding the gang violence, and wants to seize the Envy Foundation for himself by killing off all his rivals. I laughed at her; Blake may be a devil, but he&#39;s a devil I can work with. I wasn&#39;t going to sell him out.&lt;/p&gt;

&lt;p&gt;I asked Miss Kitty Rider if there was another way to end the gang violence, without having to take out Investigator Blake. Miss Kitty Rider responded me that if I wanted to bring an end to the violence, then I needed someone who is outside of the system. She recommended a fresh recruit from the police academy who was &#39;on the ball&#39;. Mr. Simpson. I foolishly took her advice.&lt;/p&gt;

&lt;p&gt;“An inside job…?” Simpson gasped timidly. “Well … No… not an inside job,” I growled. I could barely contain myself with this new guy. “Here’s the deal,” I muttered, “Why don’t I handle this case, while you make like a magnet … and flux off?”&lt;/p&gt;

&lt;p&gt;Before I left, Mr. Simpson offered me the phone number to the office. He claimed that the office had helped him greatly and that it can help me too. Desperate for any clues, I took Mr. Simpson&#39;s advice. I spoke to the receptionist for an hour, and jotted down everything she said.&lt;/p&gt;

&lt;p&gt;I had little to go on this time … but the office did manage to dig something up. G-Men, fighting crime and resisting corruption, deciding that the best way to fight crime and resist corruption is to kill everyone who might be a criminal and might be corrupt. Their current goal is to &#39;purify&#39; the Envy Foundation before moving onto other tourist attractions. I always knew that G-Men were a little &#39;special&#39;.&lt;/p&gt;

&lt;p&gt;I shoulda got out when I had the chance.&lt;/p&gt;&lt;/blockquote&gt;&lt;/blockquote&gt;

&lt;p&gt;Now, this story does have its faults. According to one of my readers, there isn&#39;t really any coherent plot, and no actual attempt to explain what&#39;s going on. And I agree with that reader. The stories that my program spits out are really more like &#39;prose poetry&#39;: the plot is really just an excuse to show off different evocative scenes. But I know that some people may like reading these stories anyway. Evocative scenes are enjoyable in their own right.&lt;/p&gt;

&lt;p&gt;In addition, another reader praised the computer-generated story as having &quot;linear progression&quot;, and a &quot;[natural] level of detail and organization and flow&quot;. This suggest that the existing of some underlining structure may also help to make this story more appealing to read.&lt;/p&gt;

&lt;p&gt;What is undeniable is that my computer has written a story.&lt;/p&gt;

&lt;p&gt;My entry is based on a simple trick: Humans has a tendency to see patterns in everyday life, even when no patterns exist. The very act of me placing words right next to each other imply that the words must have some &lt;em&gt;relationship&lt;/em&gt; with each other. So the humans create this &lt;em&gt;relationship&lt;/em&gt; within their own mind. This tendency for humans to see patterns where none exist has a name: &quot;apophenia&quot;.&lt;/p&gt;

&lt;p&gt;All I have to do is to provide some context and themes to help trick the humans into assuming that there must be meaning in the words that the program generates. Once that happens, the humans will then fill in the details by interpreting the program&#39;s words. The computer can write total nonsense...and that&#39;s okay, because the humans will just happily figure out the meaning of that nonsense. While the humans figure out that meaning, they thereby construct the story out of that nonsense. A story, therefore, appears from a mere collection of words.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Appendix A: Is This Story Of High-Quality?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I don&#39;t know. It appears that if the judges think that a story is human-written, they will generally assume it to be better...&lt;/p&gt;

&lt;p&gt;This sounds silly, but according to my data, the rating of the quality of the story is dependent on whether the judge assume the story is written by a human.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Out of the 3 people who correctly identified Story A as being written by a human, 2 of them gave that story higher marks than Story B, while 1 person gave Story B higher marks.&lt;/li&gt;
&lt;li&gt;Out of the 3 people who wrongly identified Story B as being written by a human, all of them gave Story B higher marks.&lt;/li&gt;
&lt;li&gt;The one person who didn&#39;t know which story was human-written? He gave both stories &lt;em&gt;equal&lt;/em&gt; marks.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This is a pretty odd result and suggests to me that the &#39;origin&#39; of a story impacts how judges view its quality. This makes some sense: humans like reading what other humans have written, and if they assume that something is human-written, they are willing to think of it as good (even if it&#39;s not). But at the same time, I do have a small sample size, and it&#39;s possible that a different group of judges would determine the quality of a story more &quot;objectively&quot;.&lt;/p&gt;

&lt;p&gt;By the way, there have been several peer-reviewed studies about how humans&#39; perceptions of news articles can change based on whether they were told the news article was written by a &#39;human&#39; or by a &#39;robot&#39;, and I do plan on blogging on these articles later. I trust the results of those peer-reviewed studies over that of my unscientific study.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Appendix B: Can This Program &#39;Scale&#39;?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Using a single noun prompt, my story can generate 120 different short stories. Most people are not going to read all 120 stories. Instead, they&#39;ll only read a few computer-generated stories before moving onto the next content to consume. So I&#39;m fine with how the algorithm works currently. There&#39;s no reason to spend time writing more code than is necessary.&lt;/p&gt;

&lt;p&gt;That being said, the algorithm could &quot;scale&quot; upwards. If the program is given more passages, you could easily generate more short stories (or even &lt;em&gt;longer&lt;/em&gt; stories, possibly even novels). Finding those passages (and making sure they are all thematic coherent) may be somewhat difficult to do, and would most likely require using passages from Creative Commons or public domain works.&lt;/p&gt;

&lt;p&gt;Note that the &quot;transition phrases&quot; will have to be made more &#39;generic&#39; and reusable for different passages. Currently, I had to handwrite each transition phrase to handle specific situations (for example, I handwrote a the transition phrase to justify our unnamed detective leaving Mr. Simpson&#39;s place and calling the Office). Handwriting each transition phrase is a very laborious and tedious process. Having &quot;generic&quot; transition phrases, on the other hand, would have enabled me to focus on copying and pasting as many evocative scenes as possible into the generator.&lt;/p&gt;
&lt;p&gt;Thank you for reading this blog post written by Tariq Ali, an aspiring web developer. If you are interested in reading more blog posts, please go to tra38.github.io/blog today.&lt;/p&gt;</content>
        <pubDate>Thu, 21 Apr 2016 00:00:00 -0500</pubDate>
        <link>tra38.github.io/blog/t15-architect.html</link>
        <guid isPermaLink="true">tra38.github.io/blog/t15-architect.html</guid>
        
        
      </item>
    
      <item>
        <title>Ai3</title>
        <description>![CDATA[I was 'inspired' to write this article because I read the botifesto "How To Think About Bots". As I thought the 'botifesto' was too pro-bot, I wanted to write an article that takes the anti-bot approach. However, halfway through writing this blog post, I realized that the botifesto...wasn't written by a bot. In fact, most pro-bot articles have been hand-written by human beings. This is not at all a demonstration of the power of AI; after all, humans have written optimistic proclamations about the future since the dawn of time.
]</description>
        <content>&lt;p&gt;I was &#39;inspired&#39; to write this article because I read the botifesto &lt;a href=&quot;http://motherboard.vice.com/read/how-to-think-about-bots&quot;&gt;&quot;How To Think About Bots&quot;&lt;/a&gt;. As I thought the &#39;botifesto&#39; was too pro-bot, I wanted to write an article that takes the anti-bot approach. However, halfway through writing this blog post, I realized that the botifesto...wasn&#39;t written by a bot. In fact, most pro-bot articles have been hand-written by human beings. This is not at all a demonstration of the power of AI; after all, humans have written optimistic proclamations about the future since the dawn of time.&lt;/p&gt;

&lt;p&gt;If I am to demonstrate that AI is a threat, I have to also demonstrate that AI &lt;em&gt;can be&lt;/em&gt; a threat, and to do that, I have to show what machines are currently capable of doing (in the hopes of provoking a hostile reaction).&lt;/p&gt;

&lt;p&gt;So this blog post has been generated by a robot. I have provided all the content, but an algorithm (&quot;Prolefeed&quot;) is responsible for arranging the content in a manner that will please the reader. Here is the &lt;a href=&quot;https://gist.github.com/tra38/8a6bf3743cd89687151c&quot;&gt;source code&lt;/a&gt;. And as you browse through it, think of what else can be automated away with a little human creativity. And think whether said automation would be a good thing.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Unemployment&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In 2013, Oxford Professors Frey and Obsurne argued that robots will replace 70 million jobs in the next 20 years (or 47% of all jobs in the USA). J.P Gownder, an analyst at the Boston-based tech research firm &quot;Forrester&quot;, makes a more optimistic case for technology in 2015, by claiming that by the year 2025, robots will only cause a net job loss of 9.1 million. (Both studies came from &lt;a href=&quot;http://www.wired.com/2015/08/robots-will-steal-jobs-theyll-give-us-new-ones/&quot;&gt;Wired&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;J.P. Gownder argued his lower estimate for job loss is because technology will create new jobs. In a Forbes article, J.P. Gowdner &lt;a href=&quot;http://www.forbes.com/sites/forrester/2015/08/24/robots-wont-steal-all-the-jobs-but-theyll-transform-the-way-we-work/#1d0356cd29a3&quot;&gt;justified his viewpoint&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We forecast that 16% of jobs will disspear[sic] due to automation technologies between now and 2025, but that jobs equivalent to 9% of today’s jobs will be created. Physical robots require repair and maintenance professionals — one of several job categories that will grow up around a more automated world. That’s a net loss of 7%: far fewer than most forecasts, though still a significant job loss number.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The same &#39;adjustment&#39; for job gains was also done in a 2016 report by &lt;a href=&quot;http://reports.weforum.org/future-of-jobs-2016/employment-trends/&quot;&gt;World Economic Forum at Davos&lt;/a&gt;. &quot;[D]isruptive labour market changes&quot; (which includes not only AI, but other emerging tech such as 3D printing) could destroy 7.1 million jobs by 2020, while also creating 2 million jobs in smaller industry sectors. This means a net total of 5.1 million jobs lost.&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;&lt;td&gt;Study&lt;/td&gt;&lt;td&gt;Net Job Loss/Year&lt;/td&gt;
    &lt;tr&gt;&lt;td&gt;Frey and Obsurne (2013)&lt;/td&gt;&lt;td&gt;3.50 million&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td&gt;J.P. Gownder (2015)&lt;/td&gt;&lt;td&gt;0.91 million&lt;td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td&gt;World Economic Forum (2016)&lt;/td&gt;&lt;td&gt;1.02 million&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;


&lt;p&gt;There are some people who claim that we should not worry about robots because we&#39;ll just create brand new jobs out of thin air. To me, they are behaving like a complusive gambler boasting about how he can earn $2.82 by simply gambling away $10.&lt;/p&gt;

&lt;p&gt;In the long-term, perhaps, technology may finally erase the deficit in jobs and be seen as a net job producer. But that is exactly why we need to worry about this &quot;transition period&quot; to this &#39;long-term&#39;, whenever that may arrive.&lt;/p&gt;

&lt;p&gt;How will this new joblessness come into being? Personally, I do not foresee a bunch of people getting laid off immediately. Instead, companies will gradually reduce their hiring. Why hire a new [PROFESSION_NAME_HERE] when you can just get a robot to do the job for you? Existing employees may be deemed &#39;obsolete&#39;, but will be retrained with new skills that cannot be automated (yet).&lt;/p&gt;

&lt;p&gt;However, an academic paper entitled &lt;a href=&quot;http://miguelmorin.com/docs/Miguel_Morin_Great_Depression.pdf&quot;&gt;&quot;The Labor Market Consequences of Electricity Adoption: Concrete Evidence From The Great Depression&quot;&lt;/a&gt;, by Miguel Morin, does suggest that technological unemployment will indeed take the form of layoffs. During the Great Depression, the cost of electricity decreased for concrete plants. This increased the productivity of workers. Instead of increasing the production of concrete though, the concrete plants simply fired workers instead, thereby cutting their costs. &lt;a href=&quot;http://www.theatlantic.com/magazine/archive/2015/07/world-without-work/395294/&quot;&gt;The Atlantic&lt;/a&gt; also wrote several examples where technological unemployment occurred during times of recessions...when companies need to save money, humans get laid off and the cheaper bots come in instead.&lt;/p&gt;

&lt;p&gt;I hope I do not need to write out why unemployment is not good.&lt;/p&gt;

&lt;p&gt;&lt;img style=&quot;float: left;&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/9/94/Gartner_Hype_Cycle.svg/559px-Gartner_Hype_Cycle.svg.png&quot; &gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The &quot;AI Winter&quot;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Artificial Intelligence is, ultimately, just a technology. And technologies can often times go through a &lt;a href=&quot;https://en.wikipedia.org/wiki/Hype_cycle&quot;&gt;&#39;Hype Cycle&#39;&lt;/a&gt;, as coined by the research firm Gartner.&lt;/p&gt;

&lt;p&gt;At first, people become very interested in a brand new technology (a &quot;Technology Trigger&quot;). Companies start to over-invest in researching this technology, leading to a &quot;Peak of Inflated Expectations&quot; (i.e, a bubble). But the technology turns out to have major limitations. As a result, investment in the technology dries up (&quot;Trough of Disillusionment&quot;). Most companies either begin laying people off or closing down outright.&lt;/p&gt;

&lt;p&gt;Eventually, the survivors soon realize how to use the technology properly (&quot;Slope of Enlightenment&quot;), and we can finally use the technology in our day-to-day life (&quot;Plateau of Productivity&quot;). But as this picture from Wikipedia shows, the visibility of the technology in the Plateau of Productivity is much less than the visibility of that same technology in the Peak of Inflated Expectations. The brand new technology has done great things for us. It&#39;s just not as great as we hoped it to be. And does it justify the extreme waste seen in the &quot;Peak of Inflated Expectations&quot;?&lt;/p&gt;

&lt;p&gt;If this is some hypothetical graph, then it&#39;s not much to be worried about. But I have already lived through two tech bubbles: &lt;a href=&quot;https://en.wikipedia.org/wiki/Dot-com_bubble&quot;&gt;the dot-com bubble of 1997-2000&lt;/a&gt; and the &lt;a href=&quot;http://www.pbs.org/newshour/making-sense/unicorns-and-delusions-in-silicon-valleys-tech-bubble/&quot;&gt;current unicorn bubble&lt;/a&gt; (ending this year, at 2016). A cycle of &quot;irrational exuberance&quot; (&quot;Uber for [Plural Nouns]&quot;) followed by layoffs can be never a good thing. Especially if you have to live with the consequences. Any actual benefit caused by this overinvestment is only incidental.
&lt;br&gt;
&lt;br&gt;
I&#39;m afraid that this hype cycle can only get worse. The major reason the &lt;a href=&quot;https://en.wikipedia.org/wiki/United_States_housing_bubble&quot;&gt;American Real Estate Bubble of 2005-2007&lt;/a&gt; and the current unicorn bubble has grown as big as it did was due to a policy of &#39;low interest rates&#39; pursued by central banks. Investors are &#39;encouraged&#39; not to save money but instead to invest in risky ventures. The current interest in AI  suggest that investors may view this new technology as yet another chance to make money. These investors will probably pour way too much money into AI research (if they haven&#39;t started doing so already). And almost all of it will be exposed as wasteful in the &quot;Trough of Disillusionment&quot; stage.&lt;/p&gt;

&lt;p&gt;So why do I call the &quot;Trough of Disillusionment&quot; an &quot;AI Winter&quot;? Because I didn&#39;t come up with this name. It was invented in &lt;em&gt;1984&lt;/em&gt;. According to &lt;a href=&quot;https://en.wikipedia.org/wiki/AI_winter&quot;&gt;Wikipedia&lt;/a&gt;, the AI field had went through &lt;em&gt;two&lt;/em&gt; major AI winters (1974-1980, 1987-1993) and several &quot;smaller episodes&quot; as well.  Obviously our technology has improved. But human nature has not changed. If an AI bubble inflates, run.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Technological Dependence&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Some people believe that AI could serve as a tool that can perform routine, autonomous tasks. This frees up human to handle more creative tasks. Darrel West, the director for technology innovation at the Brookings Institution, embodies this sentiment of techno-optimism by saying:&lt;/p&gt;

&lt;blockquote&gt;&lt;blockquote&gt;&lt;p&gt;&quot;It&#39;s good that we&#39;re figuring out how to use robots to make our lives easier. There are tasks they can do very well and that free humans for more creative enterprises.&quot;&lt;/p&gt;&lt;/blockquote&gt;&lt;/blockquote&gt;

&lt;p&gt;For example, &lt;a href=&quot;http://articles.philly.com/2015-11-29/news/68626755_1_new-technology-textbook-robots&quot;&gt;robots are very good at writing 9-page textbooks&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now, I understand that some textbooks can be dry and boring. But it is hard to say that they are not &quot;creative enterprises&quot;. Yet, if you click on my above link, you will actually see Darrel West&#39;s quote, down to his very last words: &quot;more creative enterprises&quot;. Some human journalist told Darrel that a university is building a program that can write textbooks, and Darrel&#39;s only response boils down to: &quot;Oh, that&#39;s cool. More time for creativity then.&quot; (He also points to Associated Press&#39; own use of bots to write stories about sports to justify his viewpoint as well.)&lt;/p&gt;

&lt;p&gt;Does Darrel consider the act of writing &lt;em&gt;not creative&lt;/em&gt; then?&lt;/p&gt;

&lt;p&gt;Here&#39;s a dystopian idea. The term &quot;creative enterprise&quot; is a euphemism to refer to &quot;any activity that cannot be routinely automated away yet&quot;. Any task that we declare &#39;a creative expression of the human experience&#39; will be seen as &#39;dull busywork&#39; as soon as we invent a bot.  We delude themselves into thinking we are still the superior race, while slowly replacing all our usual activities with a bunch of silicon-based tools.&lt;/p&gt;

&lt;p&gt;This might be tolerable if your tools work 100% of the time. But &lt;a href=&quot;http://www.joelonsoftware.com/articles/LeakyAbstractions.html&quot;&gt;abstractions are leaky&lt;/a&gt;. If you rely on your tools too much, you open yourself up to terrible consequences if you ever lose access to the tools or your tools wind up malfunctioning. And the worst part is that your tools may fail at the very moment your (human) skills has decayed. After all, you didn&#39;t need to learn &quot;busywork&quot;. You focused all your efforts on mastering &quot;more creative enterprises&quot;.&lt;/p&gt;

&lt;p&gt;This skill decay has already happened to pilots. Thanks to the glory of automation on airplanes, &lt;a href=&quot;https://www.washingtonpost.com/local/trafficandcommuting/does-using-an-autopilot-dull-the-skills-of-us-commercial-pilots/2016/01/13/00e458fe-ba13-11e5-829c-26ffb874a18d_story.html&quot;&gt;the US Department of Transporation believe that many pilots are unable to fly airplanes in times of crises&lt;/a&gt;. When autopilot works, then all is well. When autopilot fails, then there&#39;s a real chance that the less capable human pilots make mistakes that winds up crashing the airplane.&lt;/p&gt;

&lt;p&gt;Far from AI freeing us to pursue worthier endeavours, it can only make us more dependent on technology and more vulnerable to disasters when that technology breaks. The only good news is that we can reduce our dependency. For example, the US Department of Transportation recommends that pilots should periodically fly their airplanes manually to keep their own skills fresh.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Angst&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;It is not enough to build robots to handle the tedious tasks of interviewing human beings and hiring them to do routine tasks, but instead, &lt;a href=&quot;http://www.govexec.com/excellence/promising-practices/2015/11/algorithms-make-better-hiring-decisions-humans/124035/&quot;&gt;&quot;Algorithms Make Better Hiring Decisions Than Humans&quot;&lt;/a&gt;. It is not enough to have a robot be able to cheerfully play board games and find creative strategies, but instead &lt;a href=&quot;http://www.theverge.com/2016/3/15/11213518/alphago-deepmind-go-match-5-result&quot;&gt;&quot;Google&#39;s AlphaGo AI beats Lee Se-dol again to win Go series, 4-1&quot;&lt;/a&gt;. It is not enough to give video game AI the ability to simulate emotional decision-making by keeping track of a bunch of variables and behaving differently based on those variables, but instead &lt;a href=&quot;http://time.com/3674972/mario-lives-artificial-intelligence/&quot;&gt;&quot;Researchers Make Super Mario Self-Aware&quot;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now, some people may argue that these algorithms are not examples of &quot;intelligence&quot;. The obvious conclusion must be that hiring people, beating people at Go, and playing Super Mario must also not be tasks that require intelligence.&lt;/p&gt;

&lt;p&gt;One of the problems with dealing with AI is the inherent vagueness of terms used to distinguish &quot;us&quot; (the humans) from &quot;them&quot; (the robots), leading to long and tedious arguments over whether this specific algorithm is an example of &quot;true AI&quot; without ever actually providing a decent definition of what is &quot;true AI&quot;. What  hurts matters even more is the &lt;a href=&quot;https://en.wikipedia.org/wiki/AI_effect&quot;&gt;AI Effect&lt;/a&gt;, where &lt;a href=&quot;http://www.dansdata.com/gz107.htm&quot;&gt;the goal posts are constantly shifting in response to new advances in technology&lt;/a&gt;. If you design a test to determine what is &quot;true AI&quot;, and then a machine passes the test, a new test will just get created instead.&lt;/p&gt;

&lt;blockquote&gt;&lt;blockquote&gt;&lt;p&gt;Apparently, you see, when they said &quot;a machine will never be able to spot-weld a car together&quot;, they meant to say &quot;a machine will never be &lt;em&gt;aware&lt;/em&gt; that it&#39;s welding a car together&quot;.&lt;/p&gt;&lt;/blockquote&gt;&lt;/blockquote&gt;

&lt;p&gt;Some of the goal post shifting is justified: if we build something, we know how it works, and if we know how it works, we can see that it is artificial. And yet, again, at some point, the goal post shifting starts being seen as utterly ridiculous. Please don&#39;t say the act of writing novels is not a sign of intelligence just because &lt;a href=&quot;https://github.com/dariusk/NaNoGenMo-2015&quot;&gt;NaNoGenMo&lt;/a&gt; exists. (In fact, it is very possible that as AI improves, that we may be forced to confront the possibility that &lt;a href=&quot;https://plus.google.com/100656786406473859284/posts/Yp83aFwFJEr&quot;&gt;intelligence itself may not exist&lt;/a&gt;, which seems like a far worse fate than merely accepting the existence of AI.)&lt;/p&gt;

&lt;p&gt;One way around this problem is to essentially refuse to mention the term AI at all, and instead use a more neutral term, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_learning&quot;&gt;machine learning&lt;/a&gt; or &lt;a href=&quot;https://en.wikipedia.org/wiki/Expert_system&quot;&gt;expert systems&lt;/a&gt;. &quot;Yeah that robot may not meet my arbitrary definition of intelligence, but it &lt;em&gt;is&lt;/em&gt; an expert in one specific domain area, and so I&#39;ll defer to its expertise.&quot; Yet the term of AI still continues to capture our imagination.&lt;/p&gt;

&lt;p&gt;Why?&lt;/p&gt;

&lt;p&gt;I think that our own ego is deeply invested in the idea that we are &#39;special&#39;, and that we are concerned when that &#39;specialness&#39; gets challenged. Michael Kearns, an AI researcher at the University of Pennsylvania, claimed that &quot;[p]eople subconsciously are trying to preserve for themselves some special role in the universe&quot;, &lt;a href=&quot;http://articles.philly.com/2004-01-15/news/25367871_1_new-robot-genes-yeast-cells&quot;&gt;in an article about an AI being built to conduct scientific experiments&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now Kearns doesn&#39;t care about protecting the human ego. But I do. I don&#39;t know what would happen to humanity when its self-image crumbles in the face of advanced machine capabilities. But I don&#39;t think it&#39;s something to look forward to.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Botcrime&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Consider the following quotes from the botifesto &lt;a href=&quot;http://motherboard.vice.com/read/how-to-think-about-bots&quot;&gt;&quot;How To Think About Bots&quot;&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;&lt;blockquote&gt;&lt;p&gt;The work and the legal response raise crucial questions. Who is responsible for the output and actions of bots, both ethically and legally? How does semi-autonomy create ethical constraints that limit the maker of a bot?&lt;/p&gt;

&lt;p&gt;If you make a bot, are you prepared to deal with the fallout when your tool does something that you yourself would not choose to do? How do you stem the spread of misinformation published by a bot? Automation and “big” data certainly afford innovative reporting techniques, but they also highlight a need for revamped journalistic ethics.&lt;/p&gt;

&lt;p&gt;Bots might be effective tools for guiding people toward healthier lifestyles or for spreading information about natural disasters. How can policies allow for civically “good” bots while stopping those that are repressive or manipulative?&lt;/p&gt;&lt;/blockquote&gt;&lt;/blockquote&gt;

&lt;p&gt;And so on and so forth. There are obviously legitimate fears about bots doing evil, either due to its interactions with the outside world or because it has been programmed to do evil by another entity.&lt;/p&gt;

&lt;p&gt;Raising questions about bot regulation is troubling though because they imply that these questions &lt;em&gt;must be answered&lt;/em&gt;. They do not have to be answered now, of course. But they do have to be answered fairly soon.&lt;/p&gt;

&lt;p&gt;Now only must they be answered, in the form of new government regulations, they must also be enforced. A law that is not enforced might as well not exist at all. Considering how successful we are currently in stopping preexisting spambots and social media manipulators, my hopes for effective enforcement of regulations is fairly low.&lt;/p&gt;

&lt;p&gt;What is worse is the fact that people will stand in the way of regulations. The authors (all creators of bots) strongly support regulation...except when said regulation might be used against them.&lt;/p&gt;

&lt;blockquote&gt;&lt;blockquote&gt;&lt;p&gt;Rumination on bots should also work to avoid policies or perspectives that simply blacklist all bots.  These automatons can and might be used for many positive efforts, from serving as a social scaffolding to pushing the bounds of art. We hope to provoke conversation about the design, implementation and regulation of bots in order to preserve these, and other as yet unimagined, possibilities.&lt;/p&gt;&lt;/blockquote&gt;&lt;/blockquote&gt;

&lt;p&gt;Again, a general blacklist of bots is a perfectly horrible idea (mostly because we cannot enforce it). But attempting to sort out &#39;good&#39; bots from &#39;bad&#39; bots seem like a rather dangerous and futile task. I can easily see the emergence of a &quot;pro-bot&quot; lobby that will stand against any but the most token of regulations, using doublespeak to claim that any use of technology has &quot;positive effects&quot;, while excusing away any problems the bots may cause.&lt;/p&gt;

&lt;p&gt;Alternatively, we can also see bot developers pitted against each other, decrying other people&#39;s uses of bots as being &quot;negative&quot; while championing their own use of bots as being &quot;positive&quot;. We need to have a legal system that can help evaluate whether bots are good or bad.&lt;/p&gt;

&lt;p&gt;According to Ryan Calo though, &lt;a href=&quot;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2737598&quot;&gt;the American legal system&#39;s views about robots are outdated and ill-suited to our brave new world&lt;/a&gt;. Judges generally see robots as &quot;programmable machine[s], by definition incapable of spontaneity&quot;, and ignore possible &#39;emergent properties&#39; that can exist when robots interact with society as a whole.&lt;/p&gt;

&lt;p&gt;Ryan Calo support the creation of &quot;a new technology commission, a kind of NASA-for-everything that can act as a repository of knowledge about robots to guide legal actors, including courts&quot;...but this commission seems very similar to that of an interest group, and one that may only have the interests of robot developers at heart, not that of society.&lt;/p&gt;

&lt;p&gt;It will take time to come up with the right balance between bot freedom and bot banning, if there really is any.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Conclusion&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In 2016, &lt;a href=&quot;http://www.evansdata.com/press/viewRelease.php?pressID=231&quot;&gt;Evans Data conducted a survey of 500 software engineers&lt;/a&gt; to find out what they feared the most. The largest plurality (29.1%) said that they feared AI taking their jobs. What&#39;s worse is that over 60% of software engineers thought that AI &quot;would be a disaster&quot;.&lt;/p&gt;

&lt;p&gt;This does not mean that these software engineers are Luddites. &quot;[O]ver three-quarters of the developers thought that robots and artificial intelligence would be a great benefit to mankind&quot;, claimed Janel Garvin (the CEO of Evans Data). &quot;Overlap between two groups was clear which shows the ambivalence that developers feel about the dawn of intelligent machines. There will be wonderful benefits, but there will also be some cataclysmic changes culturally and economically.&quot;&lt;/p&gt;

&lt;p&gt;There has been a lot of coverage about the rise of AI and its &#39;wonderful benefits&#39;. The goal of this post is to illustrate the &#39;cataclysmic changes&#39; and thereby make a implicit argument against AI.&lt;/p&gt;

&lt;p&gt;The dangers of AI are great, and we should not let the potentials of AI blind us to real risks. There are &lt;a href=&quot;tra38.github.io/blog/c11-ai2.html&quot;&gt;some solutions to help manage AI risk&lt;/a&gt; that I proposed in the past, but probably the most practical and sensible solution at the moment is to slow down AI development and think through these risks carefully. By slowing down progress, we can ensure that the changes won&#39;t be so &quot;cataclysmic&quot;, and that humanity can survive intact.&lt;/p&gt;
&lt;p&gt;Thank you for reading this blog post written by Tariq Ali, an aspiring web developer. If you are interested in reading more blog posts, please go to tra38.github.io/blog today.&lt;/p&gt;</content>
        <pubDate>Fri, 11 Mar 2016 00:00:00 -0600</pubDate>
        <link>tra38.github.io/blog/ai3.html</link>
        <guid isPermaLink="true">tra38.github.io/blog/ai3.html</guid>
        
        
      </item>
    
      <item>
        <title>C22 Sartre</title>
        <description>![CDATA[A few days ago, I read a blog post titled "Write code that was easy to delete, not easy to extend". At the top of this blog post was a quote...
]</description>
        <content>&lt;p&gt;A few days ago, I read a blog post titled &lt;a href=&quot;http://programmingisterrible.com/post/139222674273/write-code-that-is-easy-to-delete-not-easy-to&quot;&gt;&quot;Write code that was easy to delete, not easy to extend&quot;&lt;/a&gt;. At the top of this blog post was a quote...&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“Every line of code is written without reason, maintained out of weakness, and deleted by chance.”&lt;/em&gt; Jean-Paul Sartre’s Programming in ANSI C[1]&lt;/p&gt;

&lt;p&gt;I was so fasinciated by this profound quote that I didn&#39;t even bother reading the rest of the blog post and decided to find this obscure book and read it myself.&lt;/p&gt;

&lt;p&gt;I don&#39;t agree with all of what Sartre has written; his ideas about individualism reads a bit too utopian for my tastes. However, his ideas are still interesting to read. Here are a few choice quotes from the book.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;“Ah! How I hate the crimes of the new generation: they are dry and sterile as Assembly.”&lt;/li&gt;
&lt;li&gt;“A programmer is condemned to be free; because once thrown into the codebase, he is responsible for everything he does.”&lt;/li&gt;
&lt;li&gt;“Intellectuals cannot be good revolutionaries; they are just good enough to be programmers.”&lt;/li&gt;
&lt;li&gt;“If you want to deserve Hell, you need only stay in bed. The codebase is iniquity; if you accept it, you are an accomplice, if you change it you are an executioner.”&lt;/li&gt;
&lt;li&gt;“You must be afraid, my son. That is how one becomes an honest programmer.”&lt;/li&gt;
&lt;li&gt;“The more one is absorbed in fighting bad programming, the less one is tempted to write good programs.”&lt;/li&gt;
&lt;li&gt;“Do you think you can program innocently?”&lt;/li&gt;
&lt;li&gt;“As far as programmers go, it is not what they are that interests me, but what they can become.”&lt;/li&gt;
&lt;li&gt;“A programmer cannot will unless he has first understood that he must count on no one but himself; that he is alone, abandoned in the codebase in the midst of his infinite responsibilities, without help, with no other aim than the one he sets himself, with no other destiny than the one he forges for himself in his IDE.”&lt;/li&gt;
&lt;li&gt;“I do not give a damn about the previous programmers. They died for the codebase and the codebase can decide what it wants. I practice a live man’s politics, for the living.”&lt;/li&gt;
&lt;li&gt;“We programmers exist, that is all, and I find it nauseating.”&lt;/li&gt;
&lt;li&gt;“How can I, who was not able to retain my own past knowledge of the codebase, hope to teach another?”&lt;/li&gt;
&lt;li&gt;“I embraced Agile because its cause was just and I will leave Agile when it ceases to be just.”&lt;/li&gt;
&lt;li&gt;“Ah! Do not judge the gurus, young man, for they too have painful secrets.”&lt;/li&gt;
&lt;li&gt;“If a victory is told in detail, one can no longer distinguish it from a defeat.”&lt;/li&gt;
&lt;li&gt;“I respect orders but I respect myself too and I do not obey foolish design patterns made especially to humiliate me.”&lt;/li&gt;
&lt;li&gt;“What do you want to do with the language? Write Fizz-Buzz? What good is it to sharpen a knife every day if you never use it for slicing? Code is never more than a means. There is only one objective: power.”&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;[1]&quot;Jean-Paul Sartre’s Programming in ANSI C&quot; is a fictional book. The philosopher himself is real, as are the quotes, but almost all of them were modified to take into account the subject matter of programming.&lt;/p&gt;
&lt;p&gt;Thank you for reading this blog post written by Tariq Ali, an aspiring web developer. If you are interested in reading more blog posts, please go to tra38.github.io/blog today.&lt;/p&gt;</content>
        <pubDate>Wed, 17 Feb 2016 00:00:00 -0600</pubDate>
        <link>tra38.github.io/blog/c22-sartre.html</link>
        <guid isPermaLink="true">tra38.github.io/blog/c22-sartre.html</guid>
        
        
      </item>
    
      <item>
        <title>T14 Abstract</title>
        <description>![CDATA[Yesterday, while reading an old computer science textbook "Programming and Problem Solving with C++", I saw a chapter on "Abstract Data Types" (ADTs) and was instantly curious. So I read the chapter, hoping to learn about this strange and mysterious things. I was surprised to find out that ADTs are not strange and mysterious at all. In fact, I have been writing them in my programming career.
]</description>
        <content>&lt;p&gt;Yesterday, while reading an old computer science textbook &quot;Programming and Problem Solving with C++&quot;, I saw a chapter on &quot;Abstract Data Types&quot; (ADTs) and was instantly curious. So I read the chapter, hoping to learn about this strange and mysterious things. I was surprised to find out that ADTs are not strange and mysterious at all. In fact, I have been writing them in my programming career.&lt;/p&gt;

&lt;p&gt;An ADT is any entity that contains data and specified behavior (methods/functions). For example, let us define a simple Bank Account ADT. This is a &quot;specification&quot; (a detailed list of what we want our ADT to do):&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;We should store how much money is in the account.&lt;/li&gt;
&lt;li&gt;We can withdraw money from this account.&lt;/li&gt;
&lt;li&gt;We can deposit money from this account.&lt;/li&gt;
&lt;li&gt;We know how much money is in the account.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;And here&#39;s an example of the Bank Account ADT, coded in Ruby...&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
  class BankAccount
    attr_reader :current_funds&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def initialize(amount)
  @current_funds = amount
end

def withdraw(amount)
  @current_funds -= amount
end

def deposit(amount)
  @current_funds += amount
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;  end
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;That&#39;s it.&lt;/p&gt;

&lt;p&gt;Now, you may not like how I wrote this ADT. Perhaps you prefer not using classes and may wish to use a more &quot;functional&quot; approach to programming. Or maybe you just want to write this ADT in a different language.&lt;/p&gt;

&lt;p&gt;That&#39;s perfectly fine. Code the way that you wish. As long as you follow the specification listed above, the ADT is still valid. Your version of BankAccount is just as valid as mine. The implementation of an ADT is seperate from the specification of an ADT. That&#39;s why it is possible to refactor your code...so long as your code behaves the same way before and after the refactor.&lt;/p&gt;

&lt;p&gt;Now, the mind-blowing part is that you can build ADTs...&lt;strong&gt;using&lt;/strong&gt; ADTs. Suppose I want to create a Bank ADT that can store Bank Account ADTs, and I want to know how much money the Bank ADT has.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
  class Bank&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def initialize(array_of_bank_accounts)
  @accounts = array_of_bank_accounts
end

def total_funds
  sum = 0
  @accounts.each do |account|
    sum += account.current_funds
  end
  sum
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;  end
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&quot;Programming and Problem Solving with C++&quot; recommends the use of ADTs because it makes programs simpler and easier to understand, thereby making code easier to write. The book uses an example of a car to illustrate its point:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;&quot;With abstraction, we focus on the &lt;em&gt;what&lt;/em&gt;, not the &lt;em&gt;how&lt;/em&gt;. For example, our understanding of automobiles is largely based on abstraction. Most of us know &lt;em&gt;what&lt;/em&gt; the engine does (it propels the car) but fewer of us know-or wnt to know-precisely &lt;em&gt;how&lt;/em&gt; the engine works internally. Abstraction allows us to discuss, think about, and use automobiles without having to know everything about how they work.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;And programming makes use of abstractions heavily, with some languages being entirely built on other languages. Ruby, for example, is built on C. Abstraction serves as a great way to manage complex code bases...and ADTs are the living paragons of abstraction.&lt;/p&gt;

&lt;p&gt;But &lt;a href=&quot;http://www.joelonsoftware.com/articles/LeakyAbstractions.html&quot;&gt;abstractions are &#39;leaky&#39;&lt;/a&gt;. What if there is some bug in my BankAccount ADT? That bug would harm my Bank ADT as well...and could even harm any ADTs that were built on top of my Bank ADT.&lt;/p&gt;

&lt;p&gt;So while ADTs are useful, you cannot be totally dependent on them. You have to be willing to look at the internal code of an ADT you&#39;re using and be able to debug it.&lt;/p&gt;

&lt;p&gt;It is not enough to know how to program using abstractions. You must also know how to program the abstractions &lt;em&gt;themselves&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Thank you for reading this blog post written by Tariq Ali, an aspiring web developer. If you are interested in reading more blog posts, please go to tra38.github.io/blog today.&lt;/p&gt;</content>
        <pubDate>Fri, 12 Feb 2016 00:00:00 -0600</pubDate>
        <link>tra38.github.io/blog/t14-abstract.html</link>
        <guid isPermaLink="true">tra38.github.io/blog/t14-abstract.html</guid>
        
        
      </item>
    
      <item>
        <title>C21 Robot Rights</title>
        <description>![CDATA[As technology advances, so too does the capabilities of robots. As a result, some philosophers wondered whether robots may one day acquire 'rights' equal to that of their human brethren.
]</description>
        <content>&lt;p&gt;As technology advances, so too does the capabilities of robots. As a result, some philosophers wondered whether robots may one day acquire &#39;rights&#39; equal to that of their human brethren.&lt;/p&gt;

&lt;p&gt;In 2015, Time Maganize asked the question: &quot;Will Robots Need Rights?&quot; It also posted the answers of three people:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://time.com/4026684/david-gelernter-will-robots-need-rights/&quot;&gt;David Gelernter&lt;/a&gt; claim that robots are unlikely to gain rights because they lack consciousness. However, people should still treat robots humanely.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://time.com/4023496/ray-kurzweil-will-robots-need-rights/&quot;&gt;Ray Kurzweil&lt;/a&gt; states that consciousness is not scientifically testable, and thus would have to defined through philosophical arguments. Ray argued that robots will have rights once they are able to convince us that they deserves rights.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;(http://time.com/4023497/susan-n-herman-will-robots-need-rights/&quot;&gt;Susan N. Herman&lt;/a&gt;, President of the ACLU, suggest that the ACLU might defend robot rights if robots share the same sensibilities as human beings, but would first have to argue deeply over what types of rights robots should get.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;All three answers show a sympathetic outlook towards robots, treating the concept of &quot;rights&quot; as something to seriously consider as we deal with potential alien-sque entities. And while I support respecting robots as &quot;potential alien-sque entities&quot; instead of treating them as &quot;just tools&quot; to be used and abused, I believe that the case for robotic rights has been vastly overstated (and you don&#39;t even need to bring in the nebulous idea of consciousness into the discussion).&lt;/p&gt;

&lt;p&gt;In this blog post, I will lay out a hardline case against robotic rights. It&#39;s not a case I fully belive in myself, but it&#39;s a case that I believe that most people will end up using. Note that my argument is only against robot rights; it is silent on the question on whether humans have rights.&lt;/p&gt;

&lt;h3&gt;Premise&lt;/h3&gt;

&lt;p&gt;I rest my case on the premise that rights can only belong to entities that are able to exercise some control over their own actions and motives. I call these entities &quot;autonomous actors&quot;, to avoid any confusion with the term &lt;a href=&quot;https://en.wikipedia.org/wiki/Autonomous_agent&quot;&gt;&quot;autonomous agent&quot;&lt;/a&gt;.&lt;/p&gt;

&lt;h3&gt;The Lovelace Test&lt;/h3&gt;

&lt;p&gt;The original Lovelace Test, outlined in &lt;a href=&quot;http://kryten.mm.rpi.edu/lovelace.pdf&quot;&gt;&quot;Creativity, the Turing Test, and the (Better) Lovelace Test&quot;&lt;/a&gt;, was meant as a way to determine whether programs are intelligent. A program is intelligent if and only if it did all these things:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The program must be able to design something &#39;original&#39; and &#39;creative&#39; (such as a story, music, idea, or even another computer program).&lt;/li&gt;
&lt;li&gt;The program itself is a result of processes that can be &#39;reproduced&#39;. (In other words, it does not rely on some bug in the &#39;hardware&#39; that the program is running on.)&lt;/li&gt;
&lt;li&gt;The program&#39;s creative output must not be a result of bugs within the program itself.&lt;/li&gt;
&lt;li&gt;The programmer must not know how the program actually works.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Criteria #4 is the kicker though. It is very possible for programs to design &#39;original&#39; and &#39;creative&#39; work. After all, &lt;a href=&quot;http://tra38.github.io/blog/c20-robojournalism-2.html&quot;&gt;robots are already writing fiction and nonfiction stories&lt;/a&gt;. But the programmers themselves &lt;em&gt;know&lt;/em&gt; how the robots are able to produce creative outputs. Therefore, all of these programs fail the original Lovelace Test.&lt;/p&gt;

&lt;p&gt;Even an algorithm that uses &quot;machine learning&quot; is unable to pass Criteria #4, as it can be argued that the programmers are able to control the robot by controlling the dataset the robot uses to understand the world. Thus, you can explain the robot&#39;s creative output by deducing the dataset that it used.&lt;/p&gt;

&lt;p&gt;In fact, the whole point of this original Lovelace Test is to argue against the idea that robots could ever be intelligent, by arguing that robots ultimately need programmers. Mark O. Riedl &lt;a href=&quot;http://arxiv.org/pdf/1410.6142v3.pdf&quot;&gt;wrote&lt;/a&gt; that &quot;any [programmer] with resources to build [the program] in the first place ... also has the ability to explain [how the program generates the creative output]&quot; [1].&lt;/p&gt;

&lt;p&gt;I disagree with the original Lovelace Test, because it claims that intelligence is a trait that either exists or doesn&#39;t exist. I prefer to think of intelligence either in terms of a continuum or through the idea of multiple intelligences.&lt;/p&gt;

&lt;p&gt;But I think the original version of the Lovelace Test is useful when thinking about robotic rights. Robots do what programmers tell them to do. They do not have any independent &#39;will&#39; of their own. Robots are essentially puppets. It&#39;s hard to consider them autonomous actors, because there is always someone behind them (either a human or a dataset) pulling the strings. And we can see those strings.&lt;/p&gt;

&lt;h3&gt;The Parable of the Paperclip Maximizer&lt;/h3&gt;

&lt;p&gt;That doesn&#39;t mean programmers has &lt;em&gt;total control&lt;/em&gt; over these robots. Sloppy code, bad planning and codebase complexity can lead to unexpected outcomes.&lt;/p&gt;

&lt;p&gt;Given enough time and patience, a programmer should be able to figure out why his code lead to the outcome that it did. If a problem is big enough though, practically speaking, many programmers will not have the time or patience. Shortcuts are taken. Rationalization sets in. Ignorance is accepted as best practice.&lt;/p&gt;

&lt;p&gt;For example, I can easily imagine a lazy programmer write the following code before going to bed...&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Produce as many paperclips as physically possible.&lt;/li&gt;
&lt;li&gt;Increase your processing power so that you can more effectively produce paperclips.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;...and thereby destroy humanity because his &lt;a href=&quot;https://wiki.lesswrong.com/wiki/Paperclip_maximizer&quot;&gt;&#39;Paperclip Maximizer&#39;&lt;/a&gt; has decided to turn the entire solar system into paperclips and computer mainframes.&lt;/p&gt;

&lt;p&gt;But this is not an &quot;artificial intelligence&quot; problem. It&#39;s a &quot;human stupidity&quot; problem. The programmer wanted to produce paperclips and did not think through the consequences of his code. The &#39;Paperclip Maximizer&#39; simply followed orders.&lt;/p&gt;

&lt;p&gt;The programmer, if he did think through his code carefully, would have likely noticed problems. But, of course, he had other priorities. He had to go to sleep so that he can be well-rested the next day to watch Netflix movies and contribute to open source projects. And his code was so elegant, and it passed all his automated tests. He had nothing to worry about.&lt;/p&gt;

&lt;p&gt;So he goes to sleep and never wakes up.&lt;/p&gt;

&lt;h3&gt;Robots Follow Orders, Only We May Not Know What Those Orders Mean&lt;/h3&gt;

&lt;p&gt;A programmer may not know exactly why a program is doing what it is doing...but he has the &lt;em&gt;theoretical capability&lt;/em&gt; to find out for himself (since, you know, the programmer wrote the program). And he should at least attempt to do that, so that you can reduce the chances of scenarios such as the above parable.&lt;/p&gt;

&lt;p&gt;But what if a programmer is unable (or unwilling) to do this? Does the robot deserves rights then?&lt;/p&gt;

&lt;p&gt;No. The programmer had the capability to understand what a robot is doing. He just decided not to use it. But the fact that the programmer could have found out suggest that the robot is not an autonomous actor.&lt;/p&gt;

&lt;p&gt;The robot simply follow orders...only in this specific case, they are orders that we do not quite fully understand ourselves.&lt;/p&gt;

&lt;p&gt;For debugging purposes, we should hire another programmer who will be more willing to figure out why the robot is acting the way it is. After all, if one human has the theoretical capability to find out what a robot is doing...then it is likely that another human will eventually gain that same theoretical capability.&lt;/p&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;If we want robots to actually think for themselves (instead of just being puppets of datasets and human programmers), we have to turn robots into autonomous actors. As the original Lovelace Test suggest, this is an impossible goal. If we are able to write a program, then we should be able to also know how that program works. There is no autonomy to be found anywhere.&lt;/p&gt;

&lt;p&gt;If robots can never be free, then they can never deserve rights.&lt;/p&gt;

&lt;h2&gt;Footnotes&lt;/h2&gt;

&lt;p&gt;[1] Incidentally, Mark O. Riedl proposed a less strict version of the Lovelace Test, the &lt;a href=&quot;http://arxiv.org/pdf/1410.6142v3.pdf&quot;&gt;Lovelace 2.0 Test&lt;/a&gt;, as a test that can actually be beaten. Instead of mandating that the programmer remain ignorant of the inner workings of his program, the program&#39;s creative output must meet certain constraints as determined by an independent judge.&lt;/p&gt;
&lt;p&gt;Thank you for reading this blog post written by Tariq Ali, an aspiring web developer. If you are interested in reading more blog posts, please go to tra38.github.io/blog today.&lt;/p&gt;</content>
        <pubDate>Tue, 02 Feb 2016 00:00:00 -0600</pubDate>
        <link>tra38.github.io/blog/c21-robot-rights.html</link>
        <guid isPermaLink="true">tra38.github.io/blog/c21-robot-rights.html</guid>
        
        
      </item>
    
      <item>
        <title>C20 Robojournalism 2</title>
        <description>![CDATA[Though we are able to teach robots how to write as well as a human, we may have difficulty teaching them how to view the world as a human.
]</description>
        <content>&lt;p&gt;Though we are able to teach robots how to write as well as a human, we may have difficulty teaching them how to view the world as a human.&lt;/p&gt;

&lt;p&gt;Computers are learning how to write. It&#39;s not considered weird or bizarre anymore to see companies like &lt;a href=&quot;http://narrativescience.com&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;Narrative Science&lt;/a&gt; and &lt;a href=&quot;https://automatedinsights.com&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;Automated Insights&lt;/a&gt; be able to generate news reports covering topics as broad as sports and finance.  Automated Insights have also made their &quot;Wordsmith&quot; platform publicly available, meaning that anyone have the potential to command their own robotic writers, without any programming knowledge.&lt;/p&gt;

&lt;p&gt;Human journalists have been able to accept their robo-journalistic brethren under the mistaken impression that robo-journalists will be regulated to writing &quot;quantitative&quot; articles while humans will have more time to write &quot;qualitative&quot; analyses. But you can indeed turn human experiences into quantitative data that a robot can then write. For example, &quot;sentiment analysis&quot; algorithms are able to determine whether a certain article is happy or sad, based on analyzing what words were used. The output would be a qualitative judgment, based solely on quantitative data. Narrative Science has already explored the possibility of moving onto &quot;qualitative&quot; analyses by creating &quot;Quill Connect&quot;, a program that is able to write &lt;a href=&quot;https://quillconnect.narrativescience.com&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;qualitative analyses of Twitter profiles&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Algorithms are not limited to writing nonfiction. Every November (starting from 2013), programmers participate in a competition called &lt;a href=&quot;https://github.com/dariusk/NaNoGenMo-2015&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;National Novel Generation Month&lt;/a&gt;; the goal is to write a fictional novel of 50,000 words or more. Some of these generated novels are generally dull, but readable (examples: &quot;&lt;a href=&quot;https://github.com/dariusk/NaNoGenMo-2015/issues/142&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;Around the World in X Wikipedia Articles&lt;/a&gt;&quot;, &quot;&lt;a href=&quot;https://github.com/dariusk/NaNoGenMo-2015/issues/11&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;A Time of Destiny&lt;/a&gt;&quot;, &quot;&lt;a href=&quot;https://github.com/dariusk/NaNoGenMo-2015/issues/40&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;Simulationist Fantasy Novel&lt;/a&gt;&quot;). They have the plot, characterization, and imagery that you would normally associate with a human-written work. Programmers will have to put in more effort for computer-generated novels to be on-par with human-produced literature, but there does not seem to be any inherent limit to algorithmic creativity.&lt;/p&gt;

&lt;p&gt;Of course, one could argue that robots will never be able to replace humans at all. Robots are reliant on &quot;templates&quot; to help them organize their stories properly, and humans are the ones in charge of designing the templates that the robots will end up using to write. In this viewpoint, humans would willingly give up the ability to write since they can find it a much more rewarding task to simply instruct the computer how to write a certain story.&lt;/p&gt;

&lt;p&gt;But I would argue that even if humans would want to outsource all writing to their robotic slaves, humans will still write out some of their ideas out by hand...because of an inherent limitations of bots. Bots lack the &quot;worldview&quot; of humans.&lt;/p&gt;

&lt;p&gt;Humans take for granted their ability to perceive the world. Their five senses gives a continual stream of data that humans are able to quickly process. Bots, on other hand, are only limited to the &quot;raw data&quot; that we give them to process. They will not &quot;see&quot; anything that is not in the dataset. As a result, how the bots understand our world will be very foreign to our own (human) understanding.&lt;/p&gt;

&lt;p&gt;For some people, this is actually a benefit that bots bring to writing. Bots will not have the same biases as human beings. They will therefore discover new insights and meanings that humans may have overlooked. However, bots will instead bring their own unique &#39;biases&#39; and issues into their work, and humans may not tolerate the biases of algorithms as much as they would tolerate the biases of other humans. Humans will, of course, still happily read what the bots have to say. But they also want to read what humans have to say too.&lt;/p&gt;

&lt;p&gt;Humans will likely tolerate the rise of automation in literature, and accept it. Bots may even write the majority of all literature by 2100. But there will still be some marginal demand for human writers, simply because humans can relate more to the &quot;worldview&quot; of other humans. These human writers must learn how to coexist with their robotic brethren though.&lt;/p&gt;

&lt;p&gt;This article was originally published by me on LinkedIn as &lt;a href=&quot;https://www.linkedin.com/pulse/why-robots-fully-replace-human-writers-tariq-ali?trk=pulse_spock-articles&quot; rel=&quot;no-follow&quot;&gt;&quot;Why Robots Will Not (Fully) Replace Human Writers&quot;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Thank you for reading this blog post written by Tariq Ali, an aspiring web developer. If you are interested in reading more blog posts, please go to tra38.github.io/blog today.&lt;/p&gt;</content>
        <pubDate>Thu, 07 Jan 2016 00:00:00 -0600</pubDate>
        <link>tra38.github.io/blog/c20-robojournalism-2.html</link>
        <guid isPermaLink="true">tra38.github.io/blog/c20-robojournalism-2.html</guid>
        
        
      </item>
    
      <item>
        <title>T13 Sql Ordinals And Aliases</title>
        <description>![CDATA[Writing SQL can be a painful experience. But there are two shortcuts that can be used to make your code easier to type. Both shortcuts are cool to understand, but only one is actually advised by the broader "SQL community".
]</description>
        <content>&lt;p&gt;Writing SQL can be a painful experience. But there are two shortcuts that can be used to make your code easier to type. Both shortcuts are cool to understand, but only one is actually advised by the broader &quot;SQL community&quot;.&lt;/p&gt;

&lt;p&gt;I was inspired to write this blog post while completing Codecademy&#39;s tutorial &quot;SQL: Analyzing Business Metrics&quot;. In one exercise in the tutorial, I had to write an SQL query to find out how many people are playing a game per day. This was the query I wrote:&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-psql&quot; data-lang=&quot;psql&quot;&gt;SELECT date(created_at), count(DISTINCT user_id)
from gameplays
GROUP BY date(created_at)
ORDER BY date(created_at);&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;And this is the resulting table...&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;date(created_at)  count(distinct user_id)
2015-08-04          99
2015-08-05          117
2015-08-06          106
...&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;But the SQL query is too verbose and unclear. Could there be a simpler way to refer back to &#39;date(created_at)&#39;? Codecademy suggest using &quot;ordinals&quot;. Ordinals are numbers that are used to refer to the columns you are &#39;selecting&#39; in an SQL query.&lt;/p&gt;

&lt;p&gt;Here&#39;s an easy diagram to determine the ordinal number of a SELECT query...&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-psql&quot; data-lang=&quot;psql&quot;&gt;SELECT date(created_at), count(DISTINCT user_id)
       ^^^^              ^^^
       1                 2&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;By using ordinals, we can simplify our original SQL query as follows:&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-psql&quot; data-lang=&quot;psql&quot;&gt;SELECT date(created_at), count(DISTINCT user_id)
FROM gameplays
GROUP BY 1
ORDER BY 1;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;This approach &lt;em&gt;seems&lt;/em&gt; less verbose, but it is actually much more unclear. At first glance, it is impossible to know what &quot;1&quot; is supposed to mean. And if someone decides to switch the order of the SELECT query (putting &quot;count(DISTINCT user_id)&quot; first), the resulting table will be messed up.&lt;/p&gt;

&lt;p&gt;There has to be a better way.&lt;/p&gt;

&lt;p&gt;And there is. SQL Aliases. Aliases work the same as variables in other programming languages, and to define an alias in SQL, you simply write &quot;AS [alias_name]&quot;. Aliases ensure that your SQL code will be self-documenting, while also ensuring that your code would be less verbose.&lt;/p&gt;

&lt;p&gt;Here&#39;s an example of me defining aliases in an SELECT query, and then using them later on:&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-psql&quot; data-lang=&quot;psql&quot;&gt;SELECT date(created_at) AS time, count(DISTINCT user_id) AS daily_users
FROM gameplays
GROUP BY time
ORDER BY time;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;An interesting side note is that by defining variables in the SELECT query, you also change the name of the columns in the resulting table...&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;time              daily_users
2015-08-04          99
2015-08-05          117
2015-08-06          106
...&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;The main reason Codecademy teaches the use of ordinals is because it was traditionally used during the early days of SQL programming. Thus, knowing ordinals will allow you to understand and debug any legacy SQL queries you encounter.&lt;/p&gt;

&lt;p&gt;However, the broader SQL community strongly discourages the use of ordinals because of the confusion and problems that they may cause. Instead, the SQL community suggest using aliases to make your code clearer and easier to understand. Following these recommendations would make sure that when you do deal with SQL, your pain would be minimal.&lt;/p&gt;
&lt;p&gt;Thank you for reading this blog post written by Tariq Ali, an aspiring web developer. If you are interested in reading more blog posts, please go to tra38.github.io/blog today.&lt;/p&gt;</content>
        <pubDate>Thu, 31 Dec 2015 00:00:00 -0600</pubDate>
        <link>tra38.github.io/blog/t13-sql-ordinals-and-aliases.html</link>
        <guid isPermaLink="true">tra38.github.io/blog/t13-sql-ordinals-and-aliases.html</guid>
        
        
      </item>
    
      <item>
        <title>E6 Fads 2</title>
        <description>![CDATA[In the past, I wrote about Yali Pali's paper about the lifecycle of intellectual fads. I am curious about fads because the programming field seems prone to them, and it's important to avoid fads at all costs. Today, I learned two new ways to explain fads, after reading When a Fad Ends: An Agent-Based Model of Imitative Behavior by Margo Bergman.
]</description>
        <content>&lt;p&gt;In the past, I wrote about &lt;a href=&quot;http://tra38.github.io/blog/e2-fads.html&quot;&gt;Yali Pali&#39;s paper about the lifecycle of intellectual fads&lt;/a&gt;. I am curious about fads because the programming field seems prone to them, and it&#39;s important to avoid fads at all costs. Today, I learned two new ways to explain fads, after reading &lt;a href=&quot;http://uh.edu/margo/paper.pdf&quot;&gt;When a Fad Ends: An Agent-Based Model of Imitative Behavior&lt;/a&gt; by Margo Bergman.&lt;/p&gt;

&lt;p&gt;Pali focused his studies on academica and claimed that &quot;intellectual fads&quot; followed this pattern:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A new field is developed.&lt;/li&gt;
&lt;li&gt;People see that the field has no experts yet (since it is a new field).&lt;/li&gt;
&lt;li&gt;People &lt;strong&gt;rush&lt;/strong&gt; to be a part of the hot new thing, with the goal of becoming prestigious experts.&lt;/li&gt;
&lt;li&gt;A few people ends up becoming experts.&lt;/li&gt;
&lt;li&gt;Everyone else gets bored and leave. The fad ends.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;This approach seems to make sense for explaining fads in programming, since it implies why certain languages, frameworks, and even methodologies may rise and fall in popularity. But what if programming is not structured like academia? What if languages/frameworks/methodologies are not specialized fields of interest but instead tools that can be switched in and out? Then it may be important to view fads in programming as being about &#39;products&#39;, not about &#39;intellectual pursuits&#39;. Bergman&#39;s paper only discusses product fads (and I have taken the liberty of using term &quot;product&quot; with &quot;tool&quot; interchangeably).&lt;/p&gt;

&lt;p&gt;When Bergman wrote her paper, she argued against &quot;information cascades&quot;, a theory developed in economics in the 1990s that also purports to provide a lifecycle for fads. An &quot;information cascade&quot; has these stages:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;People are given a choice to accept or reject a tool. Since they lack complete knowledge about whether that tool is good to use, they rely on other signals from previous &quot;adopters&quot; of the tool.&lt;/li&gt;
&lt;li&gt;The signals from previous &quot;adopters&quot; show that the tool is indeed good. Therefore, people end up adapting the tool.&lt;/li&gt;
&lt;li&gt;The tool actually turns out to be terrible.&lt;/li&gt;
&lt;li&gt;When people learn that the tool is terrible, they stop using the tool. The fad ends.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Bergman does not like this theory though because it implies that fads are always around &#39;bad&#39; tools. For example, Hula Hoops, miniskirts, and Rubik&#39;s Cubes are not &quot;bad&quot; products by any means, yet they were all examples of fads.&lt;/p&gt;

&lt;p&gt;Bergman instead argued that people can be divided into two groups:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Fad Setters&lt;/em&gt; are people who are always looking out for the next best thing. According to Bergman, they &quot;specialize in the discovery of new products&quot; and &quot;are known to be the people who others wish to emulate&quot;. Fad Setters do not care about what other Fad Setters do and are independent.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Fad Followers&lt;/em&gt; are people who want to use the next best thing, but do not know how to find it. So they instead rely on the preferences of Fad Setters. Whatever a Fad Setter wants, the Fad Followers will want too.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Fad Followers do not need to know the exact identity of a Fad Setter to decide whether to buy a product. Instead they look at the behavior of other people, some of whom may potentially have access to the knowledge of a Fad Setter. They can then copy the behavior of those other people, and thereby indirectly emulate the Fad Setter.&lt;/p&gt;

&lt;p&gt;Bergman&#39;s fad theory has these stages:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A Fad Setter finds a product, finding it novel and new. He starts using the product.&lt;/li&gt;
&lt;li&gt;Fad Followers end up emulating the Fad Setter by using that product too.&lt;/li&gt;
&lt;li&gt;When too many Fad Followers use the product, the Fad Setter gets disgusted. The product is no longer novel or new enough for him to use. The Fad Setter leaves the product.&lt;/li&gt;
&lt;li&gt;Fad Followers also leave the product, thereby emulating the Fad Setter.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Bergman also discussed ways to prevent fads from occuring.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;First, there may be multiple Fad Setters using different products and competing with each other. No one product would become popular enough to inspire &quot;disgust&quot; in the Fad Setters (as all the Fad Followers would end up following different Fad Setters). These products thereby continue to stay popular, and avoid having to face the downturn that a fad normally faces.&lt;/li&gt;
&lt;li&gt;Second, a Fad Setter may get disgusted easily and leave a product as soon as Fad Followers starts getting interested. This stops a fad from rising.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I tend to not like Bergman&#39;s theory all that much, because it seems to imply that most people are lemmings, who only follow Fad Setters without any regard to whether the product is actually good. It also implies that Fad Setters are themselves not interested in whether the product is actually good either, and only choose products based on whether other people are &lt;em&gt;not&lt;/em&gt; using them. Yet whether I like it has no bearing on whether it is true or not, and it does seem plausible that programmers are tempted to &quot;follow the crowd&quot; and listen to gurus. I&#39;m not sure whether the gurus themselves are as flaky as the term &quot;Fad Setters&quot; imply.&lt;/p&gt;

&lt;p&gt;Surely, more work has to be done to determine whether Pali&#39;s theory, &quot;Information Cascade&quot; theory, or Bergman&#39;s theory is the correct way to explain the presences of fads in programming. Or maybe all three approaches are correct. I just don&#39;t know yet. I do know that the &#39;life cycle&#39; of a fad is an important topic that I have to keep in mind when evaluating the &quot;next hot thing&quot; in programming. As I wrote in a previous blog post:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;&quot;As beginning programmers, we do not have the time or the money to be chasing fads. We want to specialize in the fields that (a) we like and/or (b) will be popular for a long time. Understanding these fads is important, simply so that we can spot them and avoid them.&quot;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Thank you for reading this blog post written by Tariq Ali, an aspiring web developer. If you are interested in reading more blog posts, please go to tra38.github.io/blog today.&lt;/p&gt;</content>
        <pubDate>Sat, 14 Nov 2015 00:00:00 -0600</pubDate>
        <link>tra38.github.io/blog/e6-fads-2.html</link>
        <guid isPermaLink="true">tra38.github.io/blog/e6-fads-2.html</guid>
        
        
      </item>
    
      <item>
        <title>T12 Dumb Ai</title>
        <description>![CDATA[For a recent coding challenge, I had to implement an AI for a Tic-Tac-Toe game. This AI should not be able to lose a match. A quick Google search reveals that the most efficent way is to use a minimax algorithm to determine what moves are likely to cause the AI to either tie or win against another player. However, nobody I found seemed to analyze what decisions the AI actually makes.
]</description>
        <content>&lt;p&gt;For a recent coding challenge, I had to implement an AI for a Tic-Tac-Toe game. This AI should not be able to lose a match. A quick Google search reveals that the most efficent way is to use &lt;a href=&quot;http://neverstopbuilding.com/minimax&quot;&gt;a minimax algorithm&lt;/a&gt; to determine what moves are likely to cause the AI to either tie or win against another player. However, nobody I found seemed to analyze what decisions the AI actually makes.&lt;/p&gt;

&lt;p&gt;The minimax algorimth works by having the computer play Tic-Tac-Toe against itself, figuring out the best moves to use against itself...and then figuring out the best counters to those moves, and so on and so forth. It will then rank all the possible moves it can take:&lt;/p&gt;

&lt;table&gt;
&lt;tr&gt;&lt;td&gt;-10&lt;/td&gt;&lt;td&gt;- The AI will lose if it chooses this move. Avoid it.&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; 0&lt;/td&gt;&lt;td&gt;- It&#39;s a tie.&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;- The AI will win.&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;


&lt;p&gt;The AI will pick the move that has the highest score.&lt;/p&gt;

&lt;p&gt;Therefore, the computer will play perfectly and never lose. It may never win either though, if the human it is playing against is also playing perfectly.&lt;/p&gt;

&lt;p&gt;But there is an extreme amount of trust in how the Minimax Algorithm works. There is a lot of literature examining how to program the algorithm, and very little literature on what choices the algorithm ends up making. I assume that most people assume that since the algorithm will choose the best possible move for Tic-Tac-Toe, examining what moves it actually picks would be a waste of time.&lt;/p&gt;

&lt;p&gt;But the problem is that the computer has to play through every possible game of Tic-Tac-Toe before it can make its choice. This is not an instantaneous process. For the worst case scenario (where the AI goes first, and thus must play through all Tic-Tac-Toe games), I had to wait 114 seconds for a response from the AI. This is absurd.&lt;/p&gt;

&lt;p&gt;There are ways to reduce the number of games a computer has to play through (since many Tic-Tac-Toe games are really just duplicates of each other, with the boards being rotated)...but there had to be a better way to make the game go faster.&lt;/p&gt;

&lt;p&gt;I decided to look &quot;under the hood&quot; of the AI, because I was curious in seeing how the AI thought for itself. I soon realize that if I know that the AI is thinking, I could &#39;hardcode&#39; in the AI&#39;s choices. The AI will just end up doing its move rather than spending 114 seconds learning to do that same move.&lt;/p&gt;

&lt;p&gt;In the following terminal outputs, O is human and X is the computer. I also output a Hash showing the computer&#39;s thought process (&quot;MOVE&quot;=&gt;SCORE).&lt;/p&gt;

&lt;p&gt;So here&#39;s three starting scenarios:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;O goes first and pick a corner space.&lt;/p&gt;

&lt;p&gt;|&lt;em&gt;0&lt;/em&gt;|&lt;em&gt;1&lt;/em&gt;|&lt;em&gt;2&lt;/em&gt;|&lt;/p&gt;

&lt;p&gt;|&lt;em&gt;3&lt;/em&gt;|&lt;em&gt;4&lt;/em&gt;|&lt;em&gt;5&lt;/em&gt;|&lt;/p&gt;

&lt;p&gt;|&lt;em&gt;6&lt;/em&gt;|&lt;em&gt;7&lt;/em&gt;|&lt;em&gt;O&lt;/em&gt;|&lt;/p&gt;

&lt;p&gt;It is now X&#39;s Turn.&lt;/p&gt;

&lt;p&gt;{&quot;0&quot;=&gt;-10, &quot;1&quot;=&gt;-10, &quot;2&quot;=&gt;-10, &quot;3&quot;=&gt;-10, &quot;4&quot;=&gt;0, &quot;5&quot;=&gt;-10, &quot;6&quot;=&gt;-10, &quot;7&quot;=&gt;-10}&lt;/p&gt;

&lt;p&gt;X has picked Spot 4.
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The computer will choose the space that has the highest score. In this case, &lt;em&gt;every space&lt;/em&gt; other than 4 has a score of -10. The only space that allows the computer to tie the human player is 4, the spot right in the middle. So X picks that.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
O goes first and pick a edge space.&lt;/p&gt;

&lt;p&gt;|&lt;em&gt;0&lt;/em&gt;|&lt;em&gt;1&lt;/em&gt;|&lt;em&gt;2&lt;/em&gt;|&lt;/p&gt;

&lt;p&gt;|&lt;em&gt;3&lt;/em&gt;|&lt;em&gt;4&lt;/em&gt;|&lt;em&gt;5&lt;/em&gt;|&lt;/p&gt;

&lt;p&gt;|&lt;em&gt;6&lt;/em&gt;|&lt;em&gt;O&lt;/em&gt;|&lt;em&gt;8&lt;/em&gt;|&lt;/p&gt;

&lt;p&gt;It is now X&#39;s Turn.&lt;/p&gt;

&lt;p&gt;{&quot;0&quot;=&gt;-10, &quot;1&quot;=&gt;0, &quot;2&quot;=&gt;-10, &quot;3&quot;=&gt;-10, &quot;4&quot;=&gt;0, &quot;5&quot;=&gt;-10, &quot;6&quot;=&gt;0, &quot;8&quot;=&gt;0}&lt;/p&gt;

&lt;p&gt;X has picked Spot 1.
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;In this case, now, there are four moves that may lead to a tie: 1, 4, 6, and 8. Any other move will lead to the computer&#39;s loss.&lt;/p&gt;

&lt;p&gt;But it does not matter how the computer gets to that tie. A tie is a tie. So the computer can easily pick any move that leads to a tie, and the outcome will always be the same. In this case, the computer just choses Spot 1.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
What happens if I let the computer go first?&lt;/p&gt;

&lt;p&gt;|&lt;em&gt;0&lt;/em&gt;|&lt;em&gt;1&lt;/em&gt;|&lt;em&gt;2&lt;/em&gt;|&lt;/p&gt;

&lt;p&gt;|&lt;em&gt;3&lt;/em&gt;|&lt;em&gt;4&lt;/em&gt;|&lt;em&gt;5&lt;/em&gt;|&lt;/p&gt;

&lt;p&gt;|&lt;em&gt;6&lt;/em&gt;|&lt;em&gt;7&lt;/em&gt;|&lt;em&gt;8&lt;/em&gt;|&lt;/p&gt;

&lt;p&gt;{&quot;0&quot;=&gt;0, &quot;1&quot;=&gt;0, &quot;2&quot;=&gt;0, &quot;3&quot;=&gt;0, &quot;4&quot;=&gt;0, &quot;5&quot;=&gt;0, &quot;6&quot;=&gt;0, &quot;7&quot;=&gt;0, &quot;8&quot;=&gt;0}&lt;/p&gt;

&lt;p&gt;X has picked Spot 0.
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;None&lt;/em&gt; of the spaces are any more advantageous than the others. No matter what move the computer picks, a tie will result. So the computer could pick literally any move and be ensured that the other player won&#39;t win. In this case, the computer choses Spot 0.&lt;/p&gt;

&lt;p&gt;I now know that the computer sees the space &quot;4&quot; as an excellent move to make, no matter whether O goes first or second. (Other moves can be just as excellent as &quot;4&quot;, but &quot;4&quot; will always be excellent.)&lt;/p&gt;

&lt;p&gt;I then programmed the computer to always pick &quot;4&quot; as its first move. The minimax algorithm would still be used after the first move though. After the first few moves, the number of possible Tic-Tac-Toe games decreases, and so the minimax algorithm is speedy enough for the user to not be annoyed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Result&lt;/strong&gt;: The AI is still unbeatable, but now the game is faster to play! It takes less than a second for the computer to make its first move, and its second move against the player is still just as fast.&lt;/p&gt;

&lt;p&gt;It is cool to have the AI think how to beat its opponent. But just because something is cool does not mean that it is necessary or worthwhile in the real world. Thinking takes time, and a user is not going to be happy waiting for 114 seconds while the computer figure out what it should do next. Hardcoding in the AI choice ensure that the game runs faster. And since the hardcoded choice is actually based on the results of the minimax algorithm, we are not risking the chance of the AI losing.&lt;/p&gt;
&lt;p&gt;Thank you for reading this blog post written by Tariq Ali, an aspiring web developer. If you are interested in reading more blog posts, please go to tra38.github.io/blog today.&lt;/p&gt;</content>
        <pubDate>Sun, 27 Sep 2015 00:00:00 -0500</pubDate>
        <link>tra38.github.io/blog/t12-dumb-ai.html</link>
        <guid isPermaLink="true">tra38.github.io/blog/t12-dumb-ai.html</guid>
        
        
      </item>
    
      <item>
        <title>C19 Robojournalism</title>
        <description>![CDATA[Automated Insights and Narrative Science are two companies that specialize in producing software that can write news articles. But how do these news articles stack up to the human-written competition? A German researcher attempts to find out.
]</description>
        <content>&lt;p&gt;Automated Insights and Narrative Science are two companies that specialize in producing software that can write news articles. But how do these news articles stack up to the human-written competition? A German researcher attempts to find out.&lt;/p&gt;

&lt;p&gt;In the peer-reviewed article &lt;a href=&quot;http://kau.diva-portal.org/smash/get/diva2:699641/FULLTEXT01.pdf&quot;&gt;&quot;Enter the robot journalist: users’ perceptions of automated content&quot;&lt;/a&gt;, researchers randomly gave German undergraduate students one of two articles. One article was written by a human, and another was written by a program called &quot;Statsheet&quot; (which was created by Automated Insights). The German students were to read the article, rate it, and then say whether the article was written by a human being or a bot.&lt;/p&gt;

&lt;p&gt;The researchers asked two questions:&lt;/p&gt;

&lt;p&gt;1) Can the research students consistently identify whether an article is written by a human or a bot? - The answer is a resounding &quot;NO&quot;. While a majority of students correctly identified the bot-written article as being written by a bot, a majority of students also identified the &lt;strong&gt;human&lt;/strong&gt;-written article as being written by a bot. There is no statistically significant difference between how the students &#39;evaluated&#39; whether the article was written by a human or a bot.&lt;/p&gt;

&lt;p&gt;2) Are bots able to write prose of equal quality and crediblity as that of humans? - The answer is &quot;Yes, with one exception&quot;. There was no statistically significant difference between the quality of the journalism of the human and the bot...&lt;em&gt;except&lt;/em&gt; for the fact that the human&#39;s article was seen as more pleasant to read.&lt;/p&gt;

&lt;p&gt;(As a tangentical side note, the students rated the bot&#39;s article was being more informative, accurate, trustworthy, and objective...though the bot&#39;s article was also rated as more boring to read too. The students also rated the human&#39;s article as being well-written and coherent.&lt;/p&gt;

&lt;p&gt;The reason this note is tangentical is that these results are not &quot;statistically significant&quot;; if you were to run this same experiment with a different population, you would likely get different results.)&lt;/p&gt;

&lt;p&gt;Conclusion: The &quot;robojournalists&quot; are able to produce journalism that is able to equal that of the human competition. There are limits to the robojournalists though. They are unable to write prose that is &#39;pleasent to read&#39;. This may be a sign of limits to robojournalists&#39; creativity, which would give humans the edge. For now.&lt;/p&gt;
&lt;p&gt;Thank you for reading this blog post written by Tariq Ali, an aspiring web developer. If you are interested in reading more blog posts, please go to tra38.github.io/blog today.&lt;/p&gt;</content>
        <pubDate>Mon, 07 Sep 2015 00:00:00 -0500</pubDate>
        <link>tra38.github.io/blog/c19-robojournalism.html</link>
        <guid isPermaLink="true">tra38.github.io/blog/c19-robojournalism.html</guid>
        
        
      </item>
    
  </channel>
</rss>